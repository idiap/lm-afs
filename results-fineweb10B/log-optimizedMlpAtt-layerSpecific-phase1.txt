Hyperparameters:
  runName = "optimizedMlpAtt-layerSpecific-phase1"
  seed = 1234
  plotMinAcc = 0.0
  plotMaxAcc = 0.4
  plotMinLoss = 3.0
  plotMaxLoss = 15.0
  shuffleTrData = 1
  tokenDocSep = 50256
  nLayers = 4
  nHeads = 4
  dimEmb = 256
  tieEmbeddings = True
  lr = 0.0006
  freezeEmbeddings = 0
  batchSize = 1
  seqLength = 4096
  contextSize = 1024
  nSteps = 20000
  nStepsWarmup = 2000
  nStepsCooldown = 10000
  lrSchedule = "trap"
  wtDecay = 0.0
  nModels = 20
  staggeredStarts = 2
  sameDataAcrossModels = False
  sameInitAcrossModels = False
  adamB1 = 0.8
  adamB2 = 0.95
  gradientClipping = 0
  afType = ['spline', 'spline']
  afLayerSpecific = 1
  afRange = 15
  afNAnchors = 64
  afInit = 0.01
  afLr = 0.1
  afAdamB1 = 0.99
  afAdamB2 = 0.999
  afFileToLoad = ""
  dirResults = "./results-fineweb10B"
  filesTokensTr = "./data-fineweb10B/fineweb_train_*.bin"
  filesTokensVa = "./data-fineweb10B/fineweb_val_*.bin"
  nTokensVa = 10485760
  valEvery = 500
  saveModelEvery = -1
  saveAfEvery = 0
  plotEvery = 200
  plottingLevel = 2
  dtype = torch.float32
  flexBlockSize = 32
====================================================================================================
pytorch 2.10.0.dev20251001+cu126, CUDA 12.6
Fri Feb 13 11:44:36 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 566.07                 Driver Version: 566.07         CUDA Version: 12.7     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 4090 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |
| N/A   53C    P3             24W /  111W |     236MiB /  16376MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A      9548      C   ...nvs\modded-nanogpt-py312\python.exe      N/A      |
+-----------------------------------------------------------------------------------------+

Tr data: 899,999,680 tokens, 9 files (./data-fineweb10B/fineweb_train_*.bin)
Va data: 100,000,000 tokens, 1 files (./data-fineweb10B/fineweb_val_*.bin)
Number of validation batches: 2,560 (10,485,760 tokens, 4,096 tokens/sequence)
====================================================================================================
Projected number of training tokens:
  20,000 steps * 1 batchSize * 4,096 seqLength
  = 81,920,000 tokens
  = 0.09 epochs
Vocabulary size: 50,304
====================================================================================================
Number of model parameters (frozen): 0
Number of model parameters (trainable): 16,026,646
  _orig_mod.skipWeights                              2
  _orig_mod.transformer.wte.weight                   12,877,824
  _orig_mod.transformer.h.0.blockLambdas             2
  _orig_mod.transformer.h.0.attn.lambdas             1
  _orig_mod.transformer.h.0.attn.projAtt.weight      1
  _orig_mod.transformer.h.0.attn.projAtt.bias        1
  _orig_mod.transformer.h.0.attn.af.afVals           64
  _orig_mod.transformer.h.0.attn.projQ.weight        65,536
  _orig_mod.transformer.h.0.attn.projK.weight        65,536
  _orig_mod.transformer.h.0.attn.projV.weight        65,536
  _orig_mod.transformer.h.0.attn.projOut.weight      65,536
  _orig_mod.transformer.h.0.mlp.fc.weight            262,144
  _orig_mod.transformer.h.0.mlp.proj.weight          262,144
  _orig_mod.transformer.h.0.mlp.af.afVals            64
  _orig_mod.transformer.h.0.normAtt.weight           256
  _orig_mod.transformer.h.0.normMlp.weight           256
  _orig_mod.transformer.h.1.blockLambdas             2
  _orig_mod.transformer.h.1.attn.lambdas             1
  _orig_mod.transformer.h.1.attn.projAtt.weight      1
  _orig_mod.transformer.h.1.attn.projAtt.bias        1
  _orig_mod.transformer.h.1.attn.af.afVals           64
  _orig_mod.transformer.h.1.attn.projQ.weight        65,536
  _orig_mod.transformer.h.1.attn.projK.weight        65,536
  _orig_mod.transformer.h.1.attn.projV.weight        65,536
  _orig_mod.transformer.h.1.attn.projOut.weight      65,536
  _orig_mod.transformer.h.1.mlp.fc.weight            262,144
  _orig_mod.transformer.h.1.mlp.proj.weight          262,144
  _orig_mod.transformer.h.1.mlp.af.afVals            64
  _orig_mod.transformer.h.1.normAtt.weight           256
  _orig_mod.transformer.h.1.normMlp.weight           256
  _orig_mod.transformer.h.2.blockLambdas             2
  _orig_mod.transformer.h.2.attn.lambdas             1
  _orig_mod.transformer.h.2.attn.projAtt.weight      1
  _orig_mod.transformer.h.2.attn.projAtt.bias        1
  _orig_mod.transformer.h.2.attn.af.afVals           64
  _orig_mod.transformer.h.2.attn.projQ.weight        65,536
  _orig_mod.transformer.h.2.attn.projK.weight        65,536
  _orig_mod.transformer.h.2.attn.projV.weight        65,536
  _orig_mod.transformer.h.2.attn.projOut.weight      65,536
  _orig_mod.transformer.h.2.mlp.fc.weight            262,144
  _orig_mod.transformer.h.2.mlp.proj.weight          262,144
  _orig_mod.transformer.h.2.mlp.af.afVals            64
  _orig_mod.transformer.h.2.normAtt.weight           256
  _orig_mod.transformer.h.2.normMlp.weight           256
  _orig_mod.transformer.h.3.blockLambdas             2
  _orig_mod.transformer.h.3.attn.lambdas             1
  _orig_mod.transformer.h.3.attn.projAtt.weight      1
  _orig_mod.transformer.h.3.attn.projAtt.bias        1
  _orig_mod.transformer.h.3.attn.af.afVals           64
  _orig_mod.transformer.h.3.attn.projQ.weight        65,536
  _orig_mod.transformer.h.3.attn.projK.weight        65,536
  _orig_mod.transformer.h.3.attn.projV.weight        65,536
  _orig_mod.transformer.h.3.attn.projOut.weight      65,536
  _orig_mod.transformer.h.3.mlp.fc.weight            262,144
  _orig_mod.transformer.h.3.mlp.proj.weight          262,144
  _orig_mod.transformer.h.3.mlp.af.afVals            64
  _orig_mod.transformer.h.3.normAtt.weight           256
  _orig_mod.transformer.h.3.normMlp.weight           256
  _orig_mod.normWte.weight                           256
  _orig_mod.normWte.bias                             256
5.1 tokens/parameter
====================================================================================================
Optimizer #0/20: Adam
  Param group 0: lr=0.0006, weight_decay=0.0
  models.0._orig_mod.skipWeights
  models.0._orig_mod.transformer.wte.weight
  models.0._orig_mod.transformer.h.0.blockLambdas
  models.0._orig_mod.transformer.h.0.attn.lambdas
  models.0._orig_mod.transformer.h.0.attn.projAtt.weight
  models.0._orig_mod.transformer.h.0.attn.projAtt.bias
  models.0._orig_mod.transformer.h.0.attn.projQ.weight
  models.0._orig_mod.transformer.h.0.attn.projK.weight
  models.0._orig_mod.transformer.h.0.attn.projV.weight
  models.0._orig_mod.transformer.h.0.attn.projOut.weight
  models.0._orig_mod.transformer.h.0.mlp.fc.weight
  models.0._orig_mod.transformer.h.0.mlp.proj.weight
  models.0._orig_mod.transformer.h.0.normAtt.weight
  models.0._orig_mod.transformer.h.0.normMlp.weight
  models.0._orig_mod.transformer.h.1.blockLambdas
  models.0._orig_mod.transformer.h.1.attn.lambdas
  models.0._orig_mod.transformer.h.1.attn.projAtt.weight
  models.0._orig_mod.transformer.h.1.attn.projAtt.bias
  models.0._orig_mod.transformer.h.1.attn.projQ.weight
  models.0._orig_mod.transformer.h.1.attn.projK.weight
  models.0._orig_mod.transformer.h.1.attn.projV.weight
  models.0._orig_mod.transformer.h.1.attn.projOut.weight
  models.0._orig_mod.transformer.h.1.mlp.fc.weight
  models.0._orig_mod.transformer.h.1.mlp.proj.weight
  models.0._orig_mod.transformer.h.1.normAtt.weight
  models.0._orig_mod.transformer.h.1.normMlp.weight
  models.0._orig_mod.transformer.h.2.blockLambdas
  models.0._orig_mod.transformer.h.2.attn.lambdas
  models.0._orig_mod.transformer.h.2.attn.projAtt.weight
  models.0._orig_mod.transformer.h.2.attn.projAtt.bias
  models.0._orig_mod.transformer.h.2.attn.projQ.weight
  models.0._orig_mod.transformer.h.2.attn.projK.weight
  models.0._orig_mod.transformer.h.2.attn.projV.weight
  models.0._orig_mod.transformer.h.2.attn.projOut.weight
  models.0._orig_mod.transformer.h.2.mlp.fc.weight
  models.0._orig_mod.transformer.h.2.mlp.proj.weight
  models.0._orig_mod.transformer.h.2.normAtt.weight
  models.0._orig_mod.transformer.h.2.normMlp.weight
  models.0._orig_mod.transformer.h.3.blockLambdas
  models.0._orig_mod.transformer.h.3.attn.lambdas
  models.0._orig_mod.transformer.h.3.attn.projAtt.weight
  models.0._orig_mod.transformer.h.3.attn.projAtt.bias
  models.0._orig_mod.transformer.h.3.attn.projQ.weight
  models.0._orig_mod.transformer.h.3.attn.projK.weight
  models.0._orig_mod.transformer.h.3.attn.projV.weight
  models.0._orig_mod.transformer.h.3.attn.projOut.weight
  models.0._orig_mod.transformer.h.3.mlp.fc.weight
  models.0._orig_mod.transformer.h.3.mlp.proj.weight
  models.0._orig_mod.transformer.h.3.normAtt.weight
  models.0._orig_mod.transformer.h.3.normMlp.weight
  models.0._orig_mod.normWte.weight
  models.0._orig_mod.normWte.bias
Optimizer #1/20: Adam
  Param group 0: lr=0.0006, weight_decay=0.0
  models.1._orig_mod.skipWeights
  models.1._orig_mod.transformer.wte.weight
  models.1._orig_mod.transformer.h.0.blockLambdas
  models.1._orig_mod.transformer.h.0.attn.lambdas
  models.1._orig_mod.transformer.h.0.attn.projAtt.weight
  models.1._orig_mod.transformer.h.0.attn.projAtt.bias
  models.1._orig_mod.transformer.h.0.attn.projQ.weight
  models.1._orig_mod.transformer.h.0.attn.projK.weight
  models.1._orig_mod.transformer.h.0.attn.projV.weight
  models.1._orig_mod.transformer.h.0.attn.projOut.weight
  models.1._orig_mod.transformer.h.0.mlp.fc.weight
  models.1._orig_mod.transformer.h.0.mlp.proj.weight
  models.1._orig_mod.transformer.h.0.normAtt.weight
  models.1._orig_mod.transformer.h.0.normMlp.weight
  models.1._orig_mod.transformer.h.1.blockLambdas
  models.1._orig_mod.transformer.h.1.attn.lambdas
  models.1._orig_mod.transformer.h.1.attn.projAtt.weight
  models.1._orig_mod.transformer.h.1.attn.projAtt.bias
  models.1._orig_mod.transformer.h.1.attn.projQ.weight
  models.1._orig_mod.transformer.h.1.attn.projK.weight
  models.1._orig_mod.transformer.h.1.attn.projV.weight
  models.1._orig_mod.transformer.h.1.attn.projOut.weight
  models.1._orig_mod.transformer.h.1.mlp.fc.weight
  models.1._orig_mod.transformer.h.1.mlp.proj.weight
  models.1._orig_mod.transformer.h.1.normAtt.weight
  models.1._orig_mod.transformer.h.1.normMlp.weight
  models.1._orig_mod.transformer.h.2.blockLambdas
  models.1._orig_mod.transformer.h.2.attn.lambdas
  models.1._orig_mod.transformer.h.2.attn.projAtt.weight
  models.1._orig_mod.transformer.h.2.attn.projAtt.bias
  models.1._orig_mod.transformer.h.2.attn.projQ.weight
  models.1._orig_mod.transformer.h.2.attn.projK.weight
  models.1._orig_mod.transformer.h.2.attn.projV.weight
  models.1._orig_mod.transformer.h.2.attn.projOut.weight
  models.1._orig_mod.transformer.h.2.mlp.fc.weight
  models.1._orig_mod.transformer.h.2.mlp.proj.weight
  models.1._orig_mod.transformer.h.2.normAtt.weight
  models.1._orig_mod.transformer.h.2.normMlp.weight
  models.1._orig_mod.transformer.h.3.blockLambdas
  models.1._orig_mod.transformer.h.3.attn.lambdas
  models.1._orig_mod.transformer.h.3.attn.projAtt.weight
  models.1._orig_mod.transformer.h.3.attn.projAtt.bias
  models.1._orig_mod.transformer.h.3.attn.projQ.weight
  models.1._orig_mod.transformer.h.3.attn.projK.weight
  models.1._orig_mod.transformer.h.3.attn.projV.weight
  models.1._orig_mod.transformer.h.3.attn.projOut.weight
  models.1._orig_mod.transformer.h.3.mlp.fc.weight
  models.1._orig_mod.transformer.h.3.mlp.proj.weight
  models.1._orig_mod.transformer.h.3.normAtt.weight
  models.1._orig_mod.transformer.h.3.normMlp.weight
  models.1._orig_mod.normWte.weight
  models.1._orig_mod.normWte.bias
Optimizer #2/20: Adam
  Param group 0: lr=0.0006, weight_decay=0.0
  models.2._orig_mod.skipWeights
  models.2._orig_mod.transformer.wte.weight
  models.2._orig_mod.transformer.h.0.blockLambdas
  models.2._orig_mod.transformer.h.0.attn.lambdas
  models.2._orig_mod.transformer.h.0.attn.projAtt.weight
  models.2._orig_mod.transformer.h.0.attn.projAtt.bias
  models.2._orig_mod.transformer.h.0.attn.projQ.weight
  models.2._orig_mod.transformer.h.0.attn.projK.weight
  models.2._orig_mod.transformer.h.0.attn.projV.weight
  models.2._orig_mod.transformer.h.0.attn.projOut.weight
  models.2._orig_mod.transformer.h.0.mlp.fc.weight
  models.2._orig_mod.transformer.h.0.mlp.proj.weight
  models.2._orig_mod.transformer.h.0.normAtt.weight
  models.2._orig_mod.transformer.h.0.normMlp.weight
  models.2._orig_mod.transformer.h.1.blockLambdas
  models.2._orig_mod.transformer.h.1.attn.lambdas
  models.2._orig_mod.transformer.h.1.attn.projAtt.weight
  models.2._orig_mod.transformer.h.1.attn.projAtt.bias
  models.2._orig_mod.transformer.h.1.attn.projQ.weight
  models.2._orig_mod.transformer.h.1.attn.projK.weight
  models.2._orig_mod.transformer.h.1.attn.projV.weight
  models.2._orig_mod.transformer.h.1.attn.projOut.weight
  models.2._orig_mod.transformer.h.1.mlp.fc.weight
  models.2._orig_mod.transformer.h.1.mlp.proj.weight
  models.2._orig_mod.transformer.h.1.normAtt.weight
  models.2._orig_mod.transformer.h.1.normMlp.weight
  models.2._orig_mod.transformer.h.2.blockLambdas
  models.2._orig_mod.transformer.h.2.attn.lambdas
  models.2._orig_mod.transformer.h.2.attn.projAtt.weight
  models.2._orig_mod.transformer.h.2.attn.projAtt.bias
  models.2._orig_mod.transformer.h.2.attn.projQ.weight
  models.2._orig_mod.transformer.h.2.attn.projK.weight
  models.2._orig_mod.transformer.h.2.attn.projV.weight
  models.2._orig_mod.transformer.h.2.attn.projOut.weight
  models.2._orig_mod.transformer.h.2.mlp.fc.weight
  models.2._orig_mod.transformer.h.2.mlp.proj.weight
  models.2._orig_mod.transformer.h.2.normAtt.weight
  models.2._orig_mod.transformer.h.2.normMlp.weight
  models.2._orig_mod.transformer.h.3.blockLambdas
  models.2._orig_mod.transformer.h.3.attn.lambdas
  models.2._orig_mod.transformer.h.3.attn.projAtt.weight
  models.2._orig_mod.transformer.h.3.attn.projAtt.bias
  models.2._orig_mod.transformer.h.3.attn.projQ.weight
  models.2._orig_mod.transformer.h.3.attn.projK.weight
  models.2._orig_mod.transformer.h.3.attn.projV.weight
  models.2._orig_mod.transformer.h.3.attn.projOut.weight
  models.2._orig_mod.transformer.h.3.mlp.fc.weight
  models.2._orig_mod.transformer.h.3.mlp.proj.weight
  models.2._orig_mod.transformer.h.3.normAtt.weight
  models.2._orig_mod.transformer.h.3.normMlp.weight
  models.2._orig_mod.normWte.weight
  models.2._orig_mod.normWte.bias
Optimizer #3/20: Adam
  Param group 0: lr=0.0006, weight_decay=0.0
  models.3._orig_mod.skipWeights
  models.3._orig_mod.transformer.wte.weight
  models.3._orig_mod.transformer.h.0.blockLambdas
  models.3._orig_mod.transformer.h.0.attn.lambdas
  models.3._orig_mod.transformer.h.0.attn.projAtt.weight
  models.3._orig_mod.transformer.h.0.attn.projAtt.bias
  models.3._orig_mod.transformer.h.0.attn.projQ.weight
  models.3._orig_mod.transformer.h.0.attn.projK.weight
  models.3._orig_mod.transformer.h.0.attn.projV.weight
  models.3._orig_mod.transformer.h.0.attn.projOut.weight
  models.3._orig_mod.transformer.h.0.mlp.fc.weight
  models.3._orig_mod.transformer.h.0.mlp.proj.weight
  models.3._orig_mod.transformer.h.0.normAtt.weight
  models.3._orig_mod.transformer.h.0.normMlp.weight
  models.3._orig_mod.transformer.h.1.blockLambdas
  models.3._orig_mod.transformer.h.1.attn.lambdas
  models.3._orig_mod.transformer.h.1.attn.projAtt.weight
  models.3._orig_mod.transformer.h.1.attn.projAtt.bias
  models.3._orig_mod.transformer.h.1.attn.projQ.weight
  models.3._orig_mod.transformer.h.1.attn.projK.weight
  models.3._orig_mod.transformer.h.1.attn.projV.weight
  models.3._orig_mod.transformer.h.1.attn.projOut.weight
  models.3._orig_mod.transformer.h.1.mlp.fc.weight
  models.3._orig_mod.transformer.h.1.mlp.proj.weight
  models.3._orig_mod.transformer.h.1.normAtt.weight
  models.3._orig_mod.transformer.h.1.normMlp.weight
  models.3._orig_mod.transformer.h.2.blockLambdas
  models.3._orig_mod.transformer.h.2.attn.lambdas
  models.3._orig_mod.transformer.h.2.attn.projAtt.weight
  models.3._orig_mod.transformer.h.2.attn.projAtt.bias
  models.3._orig_mod.transformer.h.2.attn.projQ.weight
  models.3._orig_mod.transformer.h.2.attn.projK.weight
  models.3._orig_mod.transformer.h.2.attn.projV.weight
  models.3._orig_mod.transformer.h.2.attn.projOut.weight
  models.3._orig_mod.transformer.h.2.mlp.fc.weight
  models.3._orig_mod.transformer.h.2.mlp.proj.weight
  models.3._orig_mod.transformer.h.2.normAtt.weight
  models.3._orig_mod.transformer.h.2.normMlp.weight
  models.3._orig_mod.transformer.h.3.blockLambdas
  models.3._orig_mod.transformer.h.3.attn.lambdas
  models.3._orig_mod.transformer.h.3.attn.projAtt.weight
  models.3._orig_mod.transformer.h.3.attn.projAtt.bias
  models.3._orig_mod.transformer.h.3.attn.projQ.weight
  models.3._orig_mod.transformer.h.3.attn.projK.weight
  models.3._orig_mod.transformer.h.3.attn.projV.weight
  models.3._orig_mod.transformer.h.3.attn.projOut.weight
  models.3._orig_mod.transformer.h.3.mlp.fc.weight
  models.3._orig_mod.transformer.h.3.mlp.proj.weight
  models.3._orig_mod.transformer.h.3.normAtt.weight
  models.3._orig_mod.transformer.h.3.normMlp.weight
  models.3._orig_mod.normWte.weight
  models.3._orig_mod.normWte.bias
Optimizer #4/20: Adam
  Param group 0: lr=0.0006, weight_decay=0.0
  models.4._orig_mod.skipWeights
  models.4._orig_mod.transformer.wte.weight
  models.4._orig_mod.transformer.h.0.blockLambdas
  models.4._orig_mod.transformer.h.0.attn.lambdas
  models.4._orig_mod.transformer.h.0.attn.projAtt.weight
  models.4._orig_mod.transformer.h.0.attn.projAtt.bias
  models.4._orig_mod.transformer.h.0.attn.projQ.weight
  models.4._orig_mod.transformer.h.0.attn.projK.weight
  models.4._orig_mod.transformer.h.0.attn.projV.weight
  models.4._orig_mod.transformer.h.0.attn.projOut.weight
  models.4._orig_mod.transformer.h.0.mlp.fc.weight
  models.4._orig_mod.transformer.h.0.mlp.proj.weight
  models.4._orig_mod.transformer.h.0.normAtt.weight
  models.4._orig_mod.transformer.h.0.normMlp.weight
  models.4._orig_mod.transformer.h.1.blockLambdas
  models.4._orig_mod.transformer.h.1.attn.lambdas
  models.4._orig_mod.transformer.h.1.attn.projAtt.weight
  models.4._orig_mod.transformer.h.1.attn.projAtt.bias
  models.4._orig_mod.transformer.h.1.attn.projQ.weight
  models.4._orig_mod.transformer.h.1.attn.projK.weight
  models.4._orig_mod.transformer.h.1.attn.projV.weight
  models.4._orig_mod.transformer.h.1.attn.projOut.weight
  models.4._orig_mod.transformer.h.1.mlp.fc.weight
  models.4._orig_mod.transformer.h.1.mlp.proj.weight
  models.4._orig_mod.transformer.h.1.normAtt.weight
  models.4._orig_mod.transformer.h.1.normMlp.weight
  models.4._orig_mod.transformer.h.2.blockLambdas
  models.4._orig_mod.transformer.h.2.attn.lambdas
  models.4._orig_mod.transformer.h.2.attn.projAtt.weight
  models.4._orig_mod.transformer.h.2.attn.projAtt.bias
  models.4._orig_mod.transformer.h.2.attn.projQ.weight
  models.4._orig_mod.transformer.h.2.attn.projK.weight
  models.4._orig_mod.transformer.h.2.attn.projV.weight
  models.4._orig_mod.transformer.h.2.attn.projOut.weight
  models.4._orig_mod.transformer.h.2.mlp.fc.weight
  models.4._orig_mod.transformer.h.2.mlp.proj.weight
  models.4._orig_mod.transformer.h.2.normAtt.weight
  models.4._orig_mod.transformer.h.2.normMlp.weight
  models.4._orig_mod.transformer.h.3.blockLambdas
  models.4._orig_mod.transformer.h.3.attn.lambdas
  models.4._orig_mod.transformer.h.3.attn.projAtt.weight
  models.4._orig_mod.transformer.h.3.attn.projAtt.bias
  models.4._orig_mod.transformer.h.3.attn.projQ.weight
  models.4._orig_mod.transformer.h.3.attn.projK.weight
  models.4._orig_mod.transformer.h.3.attn.projV.weight
  models.4._orig_mod.transformer.h.3.attn.projOut.weight
  models.4._orig_mod.transformer.h.3.mlp.fc.weight
  models.4._orig_mod.transformer.h.3.mlp.proj.weight
  models.4._orig_mod.transformer.h.3.normAtt.weight
  models.4._orig_mod.transformer.h.3.normMlp.weight
  models.4._orig_mod.normWte.weight
  models.4._orig_mod.normWte.bias
Optimizer #5/20: Adam
  Param group 0: lr=0.0006, weight_decay=0.0
  models.5._orig_mod.skipWeights
  models.5._orig_mod.transformer.wte.weight
  models.5._orig_mod.transformer.h.0.blockLambdas
  models.5._orig_mod.transformer.h.0.attn.lambdas
  models.5._orig_mod.transformer.h.0.attn.projAtt.weight
  models.5._orig_mod.transformer.h.0.attn.projAtt.bias
  models.5._orig_mod.transformer.h.0.attn.projQ.weight
  models.5._orig_mod.transformer.h.0.attn.projK.weight
  models.5._orig_mod.transformer.h.0.attn.projV.weight
  models.5._orig_mod.transformer.h.0.attn.projOut.weight
  models.5._orig_mod.transformer.h.0.mlp.fc.weight
  models.5._orig_mod.transformer.h.0.mlp.proj.weight
  models.5._orig_mod.transformer.h.0.normAtt.weight
  models.5._orig_mod.transformer.h.0.normMlp.weight
  models.5._orig_mod.transformer.h.1.blockLambdas
  models.5._orig_mod.transformer.h.1.attn.lambdas
  models.5._orig_mod.transformer.h.1.attn.projAtt.weight
  models.5._orig_mod.transformer.h.1.attn.projAtt.bias
  models.5._orig_mod.transformer.h.1.attn.projQ.weight
  models.5._orig_mod.transformer.h.1.attn.projK.weight
  models.5._orig_mod.transformer.h.1.attn.projV.weight
  models.5._orig_mod.transformer.h.1.attn.projOut.weight
  models.5._orig_mod.transformer.h.1.mlp.fc.weight
  models.5._orig_mod.transformer.h.1.mlp.proj.weight
  models.5._orig_mod.transformer.h.1.normAtt.weight
  models.5._orig_mod.transformer.h.1.normMlp.weight
  models.5._orig_mod.transformer.h.2.blockLambdas
  models.5._orig_mod.transformer.h.2.attn.lambdas
  models.5._orig_mod.transformer.h.2.attn.projAtt.weight
  models.5._orig_mod.transformer.h.2.attn.projAtt.bias
  models.5._orig_mod.transformer.h.2.attn.projQ.weight
  models.5._orig_mod.transformer.h.2.attn.projK.weight
  models.5._orig_mod.transformer.h.2.attn.projV.weight
  models.5._orig_mod.transformer.h.2.attn.projOut.weight
  models.5._orig_mod.transformer.h.2.mlp.fc.weight
  models.5._orig_mod.transformer.h.2.mlp.proj.weight
  models.5._orig_mod.transformer.h.2.normAtt.weight
  models.5._orig_mod.transformer.h.2.normMlp.weight
  models.5._orig_mod.transformer.h.3.blockLambdas
  models.5._orig_mod.transformer.h.3.attn.lambdas
  models.5._orig_mod.transformer.h.3.attn.projAtt.weight
  models.5._orig_mod.transformer.h.3.attn.projAtt.bias
  models.5._orig_mod.transformer.h.3.attn.projQ.weight
  models.5._orig_mod.transformer.h.3.attn.projK.weight
  models.5._orig_mod.transformer.h.3.attn.projV.weight
  models.5._orig_mod.transformer.h.3.attn.projOut.weight
  models.5._orig_mod.transformer.h.3.mlp.fc.weight
  models.5._orig_mod.transformer.h.3.mlp.proj.weight
  models.5._orig_mod.transformer.h.3.normAtt.weight
  models.5._orig_mod.transformer.h.3.normMlp.weight
  models.5._orig_mod.normWte.weight
  models.5._orig_mod.normWte.bias
Optimizer #6/20: Adam
  Param group 0: lr=0.0006, weight_decay=0.0
  models.6._orig_mod.skipWeights
  models.6._orig_mod.transformer.wte.weight
  models.6._orig_mod.transformer.h.0.blockLambdas
  models.6._orig_mod.transformer.h.0.attn.lambdas
  models.6._orig_mod.transformer.h.0.attn.projAtt.weight
  models.6._orig_mod.transformer.h.0.attn.projAtt.bias
  models.6._orig_mod.transformer.h.0.attn.projQ.weight
  models.6._orig_mod.transformer.h.0.attn.projK.weight
  models.6._orig_mod.transformer.h.0.attn.projV.weight
  models.6._orig_mod.transformer.h.0.attn.projOut.weight
  models.6._orig_mod.transformer.h.0.mlp.fc.weight
  models.6._orig_mod.transformer.h.0.mlp.proj.weight
  models.6._orig_mod.transformer.h.0.normAtt.weight
  models.6._orig_mod.transformer.h.0.normMlp.weight
  models.6._orig_mod.transformer.h.1.blockLambdas
  models.6._orig_mod.transformer.h.1.attn.lambdas
  models.6._orig_mod.transformer.h.1.attn.projAtt.weight
  models.6._orig_mod.transformer.h.1.attn.projAtt.bias
  models.6._orig_mod.transformer.h.1.attn.projQ.weight
  models.6._orig_mod.transformer.h.1.attn.projK.weight
  models.6._orig_mod.transformer.h.1.attn.projV.weight
  models.6._orig_mod.transformer.h.1.attn.projOut.weight
  models.6._orig_mod.transformer.h.1.mlp.fc.weight
  models.6._orig_mod.transformer.h.1.mlp.proj.weight
  models.6._orig_mod.transformer.h.1.normAtt.weight
  models.6._orig_mod.transformer.h.1.normMlp.weight
  models.6._orig_mod.transformer.h.2.blockLambdas
  models.6._orig_mod.transformer.h.2.attn.lambdas
  models.6._orig_mod.transformer.h.2.attn.projAtt.weight
  models.6._orig_mod.transformer.h.2.attn.projAtt.bias
  models.6._orig_mod.transformer.h.2.attn.projQ.weight
  models.6._orig_mod.transformer.h.2.attn.projK.weight
  models.6._orig_mod.transformer.h.2.attn.projV.weight
  models.6._orig_mod.transformer.h.2.attn.projOut.weight
  models.6._orig_mod.transformer.h.2.mlp.fc.weight
  models.6._orig_mod.transformer.h.2.mlp.proj.weight
  models.6._orig_mod.transformer.h.2.normAtt.weight
  models.6._orig_mod.transformer.h.2.normMlp.weight
  models.6._orig_mod.transformer.h.3.blockLambdas
  models.6._orig_mod.transformer.h.3.attn.lambdas
  models.6._orig_mod.transformer.h.3.attn.projAtt.weight
  models.6._orig_mod.transformer.h.3.attn.projAtt.bias
  models.6._orig_mod.transformer.h.3.attn.projQ.weight
  models.6._orig_mod.transformer.h.3.attn.projK.weight
  models.6._orig_mod.transformer.h.3.attn.projV.weight
  models.6._orig_mod.transformer.h.3.attn.projOut.weight
  models.6._orig_mod.transformer.h.3.mlp.fc.weight
  models.6._orig_mod.transformer.h.3.mlp.proj.weight
  models.6._orig_mod.transformer.h.3.normAtt.weight
  models.6._orig_mod.transformer.h.3.normMlp.weight
  models.6._orig_mod.normWte.weight
  models.6._orig_mod.normWte.bias
Optimizer #7/20: Adam
  Param group 0: lr=0.0006, weight_decay=0.0
  models.7._orig_mod.skipWeights
  models.7._orig_mod.transformer.wte.weight
  models.7._orig_mod.transformer.h.0.blockLambdas
  models.7._orig_mod.transformer.h.0.attn.lambdas
  models.7._orig_mod.transformer.h.0.attn.projAtt.weight
  models.7._orig_mod.transformer.h.0.attn.projAtt.bias
  models.7._orig_mod.transformer.h.0.attn.projQ.weight
  models.7._orig_mod.transformer.h.0.attn.projK.weight
  models.7._orig_mod.transformer.h.0.attn.projV.weight
  models.7._orig_mod.transformer.h.0.attn.projOut.weight
  models.7._orig_mod.transformer.h.0.mlp.fc.weight
  models.7._orig_mod.transformer.h.0.mlp.proj.weight
  models.7._orig_mod.transformer.h.0.normAtt.weight
  models.7._orig_mod.transformer.h.0.normMlp.weight
  models.7._orig_mod.transformer.h.1.blockLambdas
  models.7._orig_mod.transformer.h.1.attn.lambdas
  models.7._orig_mod.transformer.h.1.attn.projAtt.weight
  models.7._orig_mod.transformer.h.1.attn.projAtt.bias
  models.7._orig_mod.transformer.h.1.attn.projQ.weight
  models.7._orig_mod.transformer.h.1.attn.projK.weight
  models.7._orig_mod.transformer.h.1.attn.projV.weight
  models.7._orig_mod.transformer.h.1.attn.projOut.weight
  models.7._orig_mod.transformer.h.1.mlp.fc.weight
  models.7._orig_mod.transformer.h.1.mlp.proj.weight
  models.7._orig_mod.transformer.h.1.normAtt.weight
  models.7._orig_mod.transformer.h.1.normMlp.weight
  models.7._orig_mod.transformer.h.2.blockLambdas
  models.7._orig_mod.transformer.h.2.attn.lambdas
  models.7._orig_mod.transformer.h.2.attn.projAtt.weight
  models.7._orig_mod.transformer.h.2.attn.projAtt.bias
  models.7._orig_mod.transformer.h.2.attn.projQ.weight
  models.7._orig_mod.transformer.h.2.attn.projK.weight
  models.7._orig_mod.transformer.h.2.attn.projV.weight
  models.7._orig_mod.transformer.h.2.attn.projOut.weight
  models.7._orig_mod.transformer.h.2.mlp.fc.weight
  models.7._orig_mod.transformer.h.2.mlp.proj.weight
  models.7._orig_mod.transformer.h.2.normAtt.weight
  models.7._orig_mod.transformer.h.2.normMlp.weight
  models.7._orig_mod.transformer.h.3.blockLambdas
  models.7._orig_mod.transformer.h.3.attn.lambdas
  models.7._orig_mod.transformer.h.3.attn.projAtt.weight
  models.7._orig_mod.transformer.h.3.attn.projAtt.bias
  models.7._orig_mod.transformer.h.3.attn.projQ.weight
  models.7._orig_mod.transformer.h.3.attn.projK.weight
  models.7._orig_mod.transformer.h.3.attn.projV.weight
  models.7._orig_mod.transformer.h.3.attn.projOut.weight
  models.7._orig_mod.transformer.h.3.mlp.fc.weight
  models.7._orig_mod.transformer.h.3.mlp.proj.weight
  models.7._orig_mod.transformer.h.3.normAtt.weight
  models.7._orig_mod.transformer.h.3.normMlp.weight
  models.7._orig_mod.normWte.weight
  models.7._orig_mod.normWte.bias
Optimizer #8/20: Adam
  Param group 0: lr=0.0006, weight_decay=0.0
  models.8._orig_mod.skipWeights
  models.8._orig_mod.transformer.wte.weight
  models.8._orig_mod.transformer.h.0.blockLambdas
  models.8._orig_mod.transformer.h.0.attn.lambdas
  models.8._orig_mod.transformer.h.0.attn.projAtt.weight
  models.8._orig_mod.transformer.h.0.attn.projAtt.bias
  models.8._orig_mod.transformer.h.0.attn.projQ.weight
  models.8._orig_mod.transformer.h.0.attn.projK.weight
  models.8._orig_mod.transformer.h.0.attn.projV.weight
  models.8._orig_mod.transformer.h.0.attn.projOut.weight
  models.8._orig_mod.transformer.h.0.mlp.fc.weight
  models.8._orig_mod.transformer.h.0.mlp.proj.weight
  models.8._orig_mod.transformer.h.0.normAtt.weight
  models.8._orig_mod.transformer.h.0.normMlp.weight
  models.8._orig_mod.transformer.h.1.blockLambdas
  models.8._orig_mod.transformer.h.1.attn.lambdas
  models.8._orig_mod.transformer.h.1.attn.projAtt.weight
  models.8._orig_mod.transformer.h.1.attn.projAtt.bias
  models.8._orig_mod.transformer.h.1.attn.projQ.weight
  models.8._orig_mod.transformer.h.1.attn.projK.weight
  models.8._orig_mod.transformer.h.1.attn.projV.weight
  models.8._orig_mod.transformer.h.1.attn.projOut.weight
  models.8._orig_mod.transformer.h.1.mlp.fc.weight
  models.8._orig_mod.transformer.h.1.mlp.proj.weight
  models.8._orig_mod.transformer.h.1.normAtt.weight
  models.8._orig_mod.transformer.h.1.normMlp.weight
  models.8._orig_mod.transformer.h.2.blockLambdas
  models.8._orig_mod.transformer.h.2.attn.lambdas
  models.8._orig_mod.transformer.h.2.attn.projAtt.weight
  models.8._orig_mod.transformer.h.2.attn.projAtt.bias
  models.8._orig_mod.transformer.h.2.attn.projQ.weight
  models.8._orig_mod.transformer.h.2.attn.projK.weight
  models.8._orig_mod.transformer.h.2.attn.projV.weight
  models.8._orig_mod.transformer.h.2.attn.projOut.weight
  models.8._orig_mod.transformer.h.2.mlp.fc.weight
  models.8._orig_mod.transformer.h.2.mlp.proj.weight
  models.8._orig_mod.transformer.h.2.normAtt.weight
  models.8._orig_mod.transformer.h.2.normMlp.weight
  models.8._orig_mod.transformer.h.3.blockLambdas
  models.8._orig_mod.transformer.h.3.attn.lambdas
  models.8._orig_mod.transformer.h.3.attn.projAtt.weight
  models.8._orig_mod.transformer.h.3.attn.projAtt.bias
  models.8._orig_mod.transformer.h.3.attn.projQ.weight
  models.8._orig_mod.transformer.h.3.attn.projK.weight
  models.8._orig_mod.transformer.h.3.attn.projV.weight
  models.8._orig_mod.transformer.h.3.attn.projOut.weight
  models.8._orig_mod.transformer.h.3.mlp.fc.weight
  models.8._orig_mod.transformer.h.3.mlp.proj.weight
  models.8._orig_mod.transformer.h.3.normAtt.weight
  models.8._orig_mod.transformer.h.3.normMlp.weight
  models.8._orig_mod.normWte.weight
  models.8._orig_mod.normWte.bias
Optimizer #9/20: Adam
  Param group 0: lr=0.0006, weight_decay=0.0
  models.9._orig_mod.skipWeights
  models.9._orig_mod.transformer.wte.weight
  models.9._orig_mod.transformer.h.0.blockLambdas
  models.9._orig_mod.transformer.h.0.attn.lambdas
  models.9._orig_mod.transformer.h.0.attn.projAtt.weight
  models.9._orig_mod.transformer.h.0.attn.projAtt.bias
  models.9._orig_mod.transformer.h.0.attn.projQ.weight
  models.9._orig_mod.transformer.h.0.attn.projK.weight
  models.9._orig_mod.transformer.h.0.attn.projV.weight
  models.9._orig_mod.transformer.h.0.attn.projOut.weight
  models.9._orig_mod.transformer.h.0.mlp.fc.weight
  models.9._orig_mod.transformer.h.0.mlp.proj.weight
  models.9._orig_mod.transformer.h.0.normAtt.weight
  models.9._orig_mod.transformer.h.0.normMlp.weight
  models.9._orig_mod.transformer.h.1.blockLambdas
  models.9._orig_mod.transformer.h.1.attn.lambdas
  models.9._orig_mod.transformer.h.1.attn.projAtt.weight
  models.9._orig_mod.transformer.h.1.attn.projAtt.bias
  models.9._orig_mod.transformer.h.1.attn.projQ.weight
  models.9._orig_mod.transformer.h.1.attn.projK.weight
  models.9._orig_mod.transformer.h.1.attn.projV.weight
  models.9._orig_mod.transformer.h.1.attn.projOut.weight
  models.9._orig_mod.transformer.h.1.mlp.fc.weight
  models.9._orig_mod.transformer.h.1.mlp.proj.weight
  models.9._orig_mod.transformer.h.1.normAtt.weight
  models.9._orig_mod.transformer.h.1.normMlp.weight
  models.9._orig_mod.transformer.h.2.blockLambdas
  models.9._orig_mod.transformer.h.2.attn.lambdas
  models.9._orig_mod.transformer.h.2.attn.projAtt.weight
  models.9._orig_mod.transformer.h.2.attn.projAtt.bias
  models.9._orig_mod.transformer.h.2.attn.projQ.weight
  models.9._orig_mod.transformer.h.2.attn.projK.weight
  models.9._orig_mod.transformer.h.2.attn.projV.weight
  models.9._orig_mod.transformer.h.2.attn.projOut.weight
  models.9._orig_mod.transformer.h.2.mlp.fc.weight
  models.9._orig_mod.transformer.h.2.mlp.proj.weight
  models.9._orig_mod.transformer.h.2.normAtt.weight
  models.9._orig_mod.transformer.h.2.normMlp.weight
  models.9._orig_mod.transformer.h.3.blockLambdas
  models.9._orig_mod.transformer.h.3.attn.lambdas
  models.9._orig_mod.transformer.h.3.attn.projAtt.weight
  models.9._orig_mod.transformer.h.3.attn.projAtt.bias
  models.9._orig_mod.transformer.h.3.attn.projQ.weight
  models.9._orig_mod.transformer.h.3.attn.projK.weight
  models.9._orig_mod.transformer.h.3.attn.projV.weight
  models.9._orig_mod.transformer.h.3.attn.projOut.weight
  models.9._orig_mod.transformer.h.3.mlp.fc.weight
  models.9._orig_mod.transformer.h.3.mlp.proj.weight
  models.9._orig_mod.transformer.h.3.normAtt.weight
  models.9._orig_mod.transformer.h.3.normMlp.weight
  models.9._orig_mod.normWte.weight
  models.9._orig_mod.normWte.bias
Optimizer #10/20: Adam
  Param group 0: lr=0.0006, weight_decay=0.0
  models.10._orig_mod.skipWeights
  models.10._orig_mod.transformer.wte.weight
  models.10._orig_mod.transformer.h.0.blockLambdas
  models.10._orig_mod.transformer.h.0.attn.lambdas
  models.10._orig_mod.transformer.h.0.attn.projAtt.weight
  models.10._orig_mod.transformer.h.0.attn.projAtt.bias
  models.10._orig_mod.transformer.h.0.attn.projQ.weight
  models.10._orig_mod.transformer.h.0.attn.projK.weight
  models.10._orig_mod.transformer.h.0.attn.projV.weight
  models.10._orig_mod.transformer.h.0.attn.projOut.weight
  models.10._orig_mod.transformer.h.0.mlp.fc.weight
  models.10._orig_mod.transformer.h.0.mlp.proj.weight
  models.10._orig_mod.transformer.h.0.normAtt.weight
  models.10._orig_mod.transformer.h.0.normMlp.weight
  models.10._orig_mod.transformer.h.1.blockLambdas
  models.10._orig_mod.transformer.h.1.attn.lambdas
  models.10._orig_mod.transformer.h.1.attn.projAtt.weight
  models.10._orig_mod.transformer.h.1.attn.projAtt.bias
  models.10._orig_mod.transformer.h.1.attn.projQ.weight
  models.10._orig_mod.transformer.h.1.attn.projK.weight
  models.10._orig_mod.transformer.h.1.attn.projV.weight
  models.10._orig_mod.transformer.h.1.attn.projOut.weight
  models.10._orig_mod.transformer.h.1.mlp.fc.weight
  models.10._orig_mod.transformer.h.1.mlp.proj.weight
  models.10._orig_mod.transformer.h.1.normAtt.weight
  models.10._orig_mod.transformer.h.1.normMlp.weight
  models.10._orig_mod.transformer.h.2.blockLambdas
  models.10._orig_mod.transformer.h.2.attn.lambdas
  models.10._orig_mod.transformer.h.2.attn.projAtt.weight
  models.10._orig_mod.transformer.h.2.attn.projAtt.bias
  models.10._orig_mod.transformer.h.2.attn.projQ.weight
  models.10._orig_mod.transformer.h.2.attn.projK.weight
  models.10._orig_mod.transformer.h.2.attn.projV.weight
  models.10._orig_mod.transformer.h.2.attn.projOut.weight
  models.10._orig_mod.transformer.h.2.mlp.fc.weight
  models.10._orig_mod.transformer.h.2.mlp.proj.weight
  models.10._orig_mod.transformer.h.2.normAtt.weight
  models.10._orig_mod.transformer.h.2.normMlp.weight
  models.10._orig_mod.transformer.h.3.blockLambdas
  models.10._orig_mod.transformer.h.3.attn.lambdas
  models.10._orig_mod.transformer.h.3.attn.projAtt.weight
  models.10._orig_mod.transformer.h.3.attn.projAtt.bias
  models.10._orig_mod.transformer.h.3.attn.projQ.weight
  models.10._orig_mod.transformer.h.3.attn.projK.weight
  models.10._orig_mod.transformer.h.3.attn.projV.weight
  models.10._orig_mod.transformer.h.3.attn.projOut.weight
  models.10._orig_mod.transformer.h.3.mlp.fc.weight
  models.10._orig_mod.transformer.h.3.mlp.proj.weight
  models.10._orig_mod.transformer.h.3.normAtt.weight
  models.10._orig_mod.transformer.h.3.normMlp.weight
  models.10._orig_mod.normWte.weight
  models.10._orig_mod.normWte.bias
Optimizer #11/20: Adam
  Param group 0: lr=0.0006, weight_decay=0.0
  models.11._orig_mod.skipWeights
  models.11._orig_mod.transformer.wte.weight
  models.11._orig_mod.transformer.h.0.blockLambdas
  models.11._orig_mod.transformer.h.0.attn.lambdas
  models.11._orig_mod.transformer.h.0.attn.projAtt.weight
  models.11._orig_mod.transformer.h.0.attn.projAtt.bias
  models.11._orig_mod.transformer.h.0.attn.projQ.weight
  models.11._orig_mod.transformer.h.0.attn.projK.weight
  models.11._orig_mod.transformer.h.0.attn.projV.weight
  models.11._orig_mod.transformer.h.0.attn.projOut.weight
  models.11._orig_mod.transformer.h.0.mlp.fc.weight
  models.11._orig_mod.transformer.h.0.mlp.proj.weight
  models.11._orig_mod.transformer.h.0.normAtt.weight
  models.11._orig_mod.transformer.h.0.normMlp.weight
  models.11._orig_mod.transformer.h.1.blockLambdas
  models.11._orig_mod.transformer.h.1.attn.lambdas
  models.11._orig_mod.transformer.h.1.attn.projAtt.weight
  models.11._orig_mod.transformer.h.1.attn.projAtt.bias
  models.11._orig_mod.transformer.h.1.attn.projQ.weight
  models.11._orig_mod.transformer.h.1.attn.projK.weight
  models.11._orig_mod.transformer.h.1.attn.projV.weight
  models.11._orig_mod.transformer.h.1.attn.projOut.weight
  models.11._orig_mod.transformer.h.1.mlp.fc.weight
  models.11._orig_mod.transformer.h.1.mlp.proj.weight
  models.11._orig_mod.transformer.h.1.normAtt.weight
  models.11._orig_mod.transformer.h.1.normMlp.weight
  models.11._orig_mod.transformer.h.2.blockLambdas
  models.11._orig_mod.transformer.h.2.attn.lambdas
  models.11._orig_mod.transformer.h.2.attn.projAtt.weight
  models.11._orig_mod.transformer.h.2.attn.projAtt.bias
  models.11._orig_mod.transformer.h.2.attn.projQ.weight
  models.11._orig_mod.transformer.h.2.attn.projK.weight
  models.11._orig_mod.transformer.h.2.attn.projV.weight
  models.11._orig_mod.transformer.h.2.attn.projOut.weight
  models.11._orig_mod.transformer.h.2.mlp.fc.weight
  models.11._orig_mod.transformer.h.2.mlp.proj.weight
  models.11._orig_mod.transformer.h.2.normAtt.weight
  models.11._orig_mod.transformer.h.2.normMlp.weight
  models.11._orig_mod.transformer.h.3.blockLambdas
  models.11._orig_mod.transformer.h.3.attn.lambdas
  models.11._orig_mod.transformer.h.3.attn.projAtt.weight
  models.11._orig_mod.transformer.h.3.attn.projAtt.bias
  models.11._orig_mod.transformer.h.3.attn.projQ.weight
  models.11._orig_mod.transformer.h.3.attn.projK.weight
  models.11._orig_mod.transformer.h.3.attn.projV.weight
  models.11._orig_mod.transformer.h.3.attn.projOut.weight
  models.11._orig_mod.transformer.h.3.mlp.fc.weight
  models.11._orig_mod.transformer.h.3.mlp.proj.weight
  models.11._orig_mod.transformer.h.3.normAtt.weight
  models.11._orig_mod.transformer.h.3.normMlp.weight
  models.11._orig_mod.normWte.weight
  models.11._orig_mod.normWte.bias
Optimizer #12/20: Adam
  Param group 0: lr=0.0006, weight_decay=0.0
  models.12._orig_mod.skipWeights
  models.12._orig_mod.transformer.wte.weight
  models.12._orig_mod.transformer.h.0.blockLambdas
  models.12._orig_mod.transformer.h.0.attn.lambdas
  models.12._orig_mod.transformer.h.0.attn.projAtt.weight
  models.12._orig_mod.transformer.h.0.attn.projAtt.bias
  models.12._orig_mod.transformer.h.0.attn.projQ.weight
  models.12._orig_mod.transformer.h.0.attn.projK.weight
  models.12._orig_mod.transformer.h.0.attn.projV.weight
  models.12._orig_mod.transformer.h.0.attn.projOut.weight
  models.12._orig_mod.transformer.h.0.mlp.fc.weight
  models.12._orig_mod.transformer.h.0.mlp.proj.weight
  models.12._orig_mod.transformer.h.0.normAtt.weight
  models.12._orig_mod.transformer.h.0.normMlp.weight
  models.12._orig_mod.transformer.h.1.blockLambdas
  models.12._orig_mod.transformer.h.1.attn.lambdas
  models.12._orig_mod.transformer.h.1.attn.projAtt.weight
  models.12._orig_mod.transformer.h.1.attn.projAtt.bias
  models.12._orig_mod.transformer.h.1.attn.projQ.weight
  models.12._orig_mod.transformer.h.1.attn.projK.weight
  models.12._orig_mod.transformer.h.1.attn.projV.weight
  models.12._orig_mod.transformer.h.1.attn.projOut.weight
  models.12._orig_mod.transformer.h.1.mlp.fc.weight
  models.12._orig_mod.transformer.h.1.mlp.proj.weight
  models.12._orig_mod.transformer.h.1.normAtt.weight
  models.12._orig_mod.transformer.h.1.normMlp.weight
  models.12._orig_mod.transformer.h.2.blockLambdas
  models.12._orig_mod.transformer.h.2.attn.lambdas
  models.12._orig_mod.transformer.h.2.attn.projAtt.weight
  models.12._orig_mod.transformer.h.2.attn.projAtt.bias
  models.12._orig_mod.transformer.h.2.attn.projQ.weight
  models.12._orig_mod.transformer.h.2.attn.projK.weight
  models.12._orig_mod.transformer.h.2.attn.projV.weight
  models.12._orig_mod.transformer.h.2.attn.projOut.weight
  models.12._orig_mod.transformer.h.2.mlp.fc.weight
  models.12._orig_mod.transformer.h.2.mlp.proj.weight
  models.12._orig_mod.transformer.h.2.normAtt.weight
  models.12._orig_mod.transformer.h.2.normMlp.weight
  models.12._orig_mod.transformer.h.3.blockLambdas
  models.12._orig_mod.transformer.h.3.attn.lambdas
  models.12._orig_mod.transformer.h.3.attn.projAtt.weight
  models.12._orig_mod.transformer.h.3.attn.projAtt.bias
  models.12._orig_mod.transformer.h.3.attn.projQ.weight
  models.12._orig_mod.transformer.h.3.attn.projK.weight
  models.12._orig_mod.transformer.h.3.attn.projV.weight
  models.12._orig_mod.transformer.h.3.attn.projOut.weight
  models.12._orig_mod.transformer.h.3.mlp.fc.weight
  models.12._orig_mod.transformer.h.3.mlp.proj.weight
  models.12._orig_mod.transformer.h.3.normAtt.weight
  models.12._orig_mod.transformer.h.3.normMlp.weight
  models.12._orig_mod.normWte.weight
  models.12._orig_mod.normWte.bias
Optimizer #13/20: Adam
  Param group 0: lr=0.0006, weight_decay=0.0
  models.13._orig_mod.skipWeights
  models.13._orig_mod.transformer.wte.weight
  models.13._orig_mod.transformer.h.0.blockLambdas
  models.13._orig_mod.transformer.h.0.attn.lambdas
  models.13._orig_mod.transformer.h.0.attn.projAtt.weight
  models.13._orig_mod.transformer.h.0.attn.projAtt.bias
  models.13._orig_mod.transformer.h.0.attn.projQ.weight
  models.13._orig_mod.transformer.h.0.attn.projK.weight
  models.13._orig_mod.transformer.h.0.attn.projV.weight
  models.13._orig_mod.transformer.h.0.attn.projOut.weight
  models.13._orig_mod.transformer.h.0.mlp.fc.weight
  models.13._orig_mod.transformer.h.0.mlp.proj.weight
  models.13._orig_mod.transformer.h.0.normAtt.weight
  models.13._orig_mod.transformer.h.0.normMlp.weight
  models.13._orig_mod.transformer.h.1.blockLambdas
  models.13._orig_mod.transformer.h.1.attn.lambdas
  models.13._orig_mod.transformer.h.1.attn.projAtt.weight
  models.13._orig_mod.transformer.h.1.attn.projAtt.bias
  models.13._orig_mod.transformer.h.1.attn.projQ.weight
  models.13._orig_mod.transformer.h.1.attn.projK.weight
  models.13._orig_mod.transformer.h.1.attn.projV.weight
  models.13._orig_mod.transformer.h.1.attn.projOut.weight
  models.13._orig_mod.transformer.h.1.mlp.fc.weight
  models.13._orig_mod.transformer.h.1.mlp.proj.weight
  models.13._orig_mod.transformer.h.1.normAtt.weight
  models.13._orig_mod.transformer.h.1.normMlp.weight
  models.13._orig_mod.transformer.h.2.blockLambdas
  models.13._orig_mod.transformer.h.2.attn.lambdas
  models.13._orig_mod.transformer.h.2.attn.projAtt.weight
  models.13._orig_mod.transformer.h.2.attn.projAtt.bias
  models.13._orig_mod.transformer.h.2.attn.projQ.weight
  models.13._orig_mod.transformer.h.2.attn.projK.weight
  models.13._orig_mod.transformer.h.2.attn.projV.weight
  models.13._orig_mod.transformer.h.2.attn.projOut.weight
  models.13._orig_mod.transformer.h.2.mlp.fc.weight
  models.13._orig_mod.transformer.h.2.mlp.proj.weight
  models.13._orig_mod.transformer.h.2.normAtt.weight
  models.13._orig_mod.transformer.h.2.normMlp.weight
  models.13._orig_mod.transformer.h.3.blockLambdas
  models.13._orig_mod.transformer.h.3.attn.lambdas
  models.13._orig_mod.transformer.h.3.attn.projAtt.weight
  models.13._orig_mod.transformer.h.3.attn.projAtt.bias
  models.13._orig_mod.transformer.h.3.attn.projQ.weight
  models.13._orig_mod.transformer.h.3.attn.projK.weight
  models.13._orig_mod.transformer.h.3.attn.projV.weight
  models.13._orig_mod.transformer.h.3.attn.projOut.weight
  models.13._orig_mod.transformer.h.3.mlp.fc.weight
  models.13._orig_mod.transformer.h.3.mlp.proj.weight
  models.13._orig_mod.transformer.h.3.normAtt.weight
  models.13._orig_mod.transformer.h.3.normMlp.weight
  models.13._orig_mod.normWte.weight
  models.13._orig_mod.normWte.bias
Optimizer #14/20: Adam
  Param group 0: lr=0.0006, weight_decay=0.0
  models.14._orig_mod.skipWeights
  models.14._orig_mod.transformer.wte.weight
  models.14._orig_mod.transformer.h.0.blockLambdas
  models.14._orig_mod.transformer.h.0.attn.lambdas
  models.14._orig_mod.transformer.h.0.attn.projAtt.weight
  models.14._orig_mod.transformer.h.0.attn.projAtt.bias
  models.14._orig_mod.transformer.h.0.attn.projQ.weight
  models.14._orig_mod.transformer.h.0.attn.projK.weight
  models.14._orig_mod.transformer.h.0.attn.projV.weight
  models.14._orig_mod.transformer.h.0.attn.projOut.weight
  models.14._orig_mod.transformer.h.0.mlp.fc.weight
  models.14._orig_mod.transformer.h.0.mlp.proj.weight
  models.14._orig_mod.transformer.h.0.normAtt.weight
  models.14._orig_mod.transformer.h.0.normMlp.weight
  models.14._orig_mod.transformer.h.1.blockLambdas
  models.14._orig_mod.transformer.h.1.attn.lambdas
  models.14._orig_mod.transformer.h.1.attn.projAtt.weight
  models.14._orig_mod.transformer.h.1.attn.projAtt.bias
  models.14._orig_mod.transformer.h.1.attn.projQ.weight
  models.14._orig_mod.transformer.h.1.attn.projK.weight
  models.14._orig_mod.transformer.h.1.attn.projV.weight
  models.14._orig_mod.transformer.h.1.attn.projOut.weight
  models.14._orig_mod.transformer.h.1.mlp.fc.weight
  models.14._orig_mod.transformer.h.1.mlp.proj.weight
  models.14._orig_mod.transformer.h.1.normAtt.weight
  models.14._orig_mod.transformer.h.1.normMlp.weight
  models.14._orig_mod.transformer.h.2.blockLambdas
  models.14._orig_mod.transformer.h.2.attn.lambdas
  models.14._orig_mod.transformer.h.2.attn.projAtt.weight
  models.14._orig_mod.transformer.h.2.attn.projAtt.bias
  models.14._orig_mod.transformer.h.2.attn.projQ.weight
  models.14._orig_mod.transformer.h.2.attn.projK.weight
  models.14._orig_mod.transformer.h.2.attn.projV.weight
  models.14._orig_mod.transformer.h.2.attn.projOut.weight
  models.14._orig_mod.transformer.h.2.mlp.fc.weight
  models.14._orig_mod.transformer.h.2.mlp.proj.weight
  models.14._orig_mod.transformer.h.2.normAtt.weight
  models.14._orig_mod.transformer.h.2.normMlp.weight
  models.14._orig_mod.transformer.h.3.blockLambdas
  models.14._orig_mod.transformer.h.3.attn.lambdas
  models.14._orig_mod.transformer.h.3.attn.projAtt.weight
  models.14._orig_mod.transformer.h.3.attn.projAtt.bias
  models.14._orig_mod.transformer.h.3.attn.projQ.weight
  models.14._orig_mod.transformer.h.3.attn.projK.weight
  models.14._orig_mod.transformer.h.3.attn.projV.weight
  models.14._orig_mod.transformer.h.3.attn.projOut.weight
  models.14._orig_mod.transformer.h.3.mlp.fc.weight
  models.14._orig_mod.transformer.h.3.mlp.proj.weight
  models.14._orig_mod.transformer.h.3.normAtt.weight
  models.14._orig_mod.transformer.h.3.normMlp.weight
  models.14._orig_mod.normWte.weight
  models.14._orig_mod.normWte.bias
Optimizer #15/20: Adam
  Param group 0: lr=0.0006, weight_decay=0.0
  models.15._orig_mod.skipWeights
  models.15._orig_mod.transformer.wte.weight
  models.15._orig_mod.transformer.h.0.blockLambdas
  models.15._orig_mod.transformer.h.0.attn.lambdas
  models.15._orig_mod.transformer.h.0.attn.projAtt.weight
  models.15._orig_mod.transformer.h.0.attn.projAtt.bias
  models.15._orig_mod.transformer.h.0.attn.projQ.weight
  models.15._orig_mod.transformer.h.0.attn.projK.weight
  models.15._orig_mod.transformer.h.0.attn.projV.weight
  models.15._orig_mod.transformer.h.0.attn.projOut.weight
  models.15._orig_mod.transformer.h.0.mlp.fc.weight
  models.15._orig_mod.transformer.h.0.mlp.proj.weight
  models.15._orig_mod.transformer.h.0.normAtt.weight
  models.15._orig_mod.transformer.h.0.normMlp.weight
  models.15._orig_mod.transformer.h.1.blockLambdas
  models.15._orig_mod.transformer.h.1.attn.lambdas
  models.15._orig_mod.transformer.h.1.attn.projAtt.weight
  models.15._orig_mod.transformer.h.1.attn.projAtt.bias
  models.15._orig_mod.transformer.h.1.attn.projQ.weight
  models.15._orig_mod.transformer.h.1.attn.projK.weight
  models.15._orig_mod.transformer.h.1.attn.projV.weight
  models.15._orig_mod.transformer.h.1.attn.projOut.weight
  models.15._orig_mod.transformer.h.1.mlp.fc.weight
  models.15._orig_mod.transformer.h.1.mlp.proj.weight
  models.15._orig_mod.transformer.h.1.normAtt.weight
  models.15._orig_mod.transformer.h.1.normMlp.weight
  models.15._orig_mod.transformer.h.2.blockLambdas
  models.15._orig_mod.transformer.h.2.attn.lambdas
  models.15._orig_mod.transformer.h.2.attn.projAtt.weight
  models.15._orig_mod.transformer.h.2.attn.projAtt.bias
  models.15._orig_mod.transformer.h.2.attn.projQ.weight
  models.15._orig_mod.transformer.h.2.attn.projK.weight
  models.15._orig_mod.transformer.h.2.attn.projV.weight
  models.15._orig_mod.transformer.h.2.attn.projOut.weight
  models.15._orig_mod.transformer.h.2.mlp.fc.weight
  models.15._orig_mod.transformer.h.2.mlp.proj.weight
  models.15._orig_mod.transformer.h.2.normAtt.weight
  models.15._orig_mod.transformer.h.2.normMlp.weight
  models.15._orig_mod.transformer.h.3.blockLambdas
  models.15._orig_mod.transformer.h.3.attn.lambdas
  models.15._orig_mod.transformer.h.3.attn.projAtt.weight
  models.15._orig_mod.transformer.h.3.attn.projAtt.bias
  models.15._orig_mod.transformer.h.3.attn.projQ.weight
  models.15._orig_mod.transformer.h.3.attn.projK.weight
  models.15._orig_mod.transformer.h.3.attn.projV.weight
  models.15._orig_mod.transformer.h.3.attn.projOut.weight
  models.15._orig_mod.transformer.h.3.mlp.fc.weight
  models.15._orig_mod.transformer.h.3.mlp.proj.weight
  models.15._orig_mod.transformer.h.3.normAtt.weight
  models.15._orig_mod.transformer.h.3.normMlp.weight
  models.15._orig_mod.normWte.weight
  models.15._orig_mod.normWte.bias
Optimizer #16/20: Adam
  Param group 0: lr=0.0006, weight_decay=0.0
  models.16._orig_mod.skipWeights
  models.16._orig_mod.transformer.wte.weight
  models.16._orig_mod.transformer.h.0.blockLambdas
  models.16._orig_mod.transformer.h.0.attn.lambdas
  models.16._orig_mod.transformer.h.0.attn.projAtt.weight
  models.16._orig_mod.transformer.h.0.attn.projAtt.bias
  models.16._orig_mod.transformer.h.0.attn.projQ.weight
  models.16._orig_mod.transformer.h.0.attn.projK.weight
  models.16._orig_mod.transformer.h.0.attn.projV.weight
  models.16._orig_mod.transformer.h.0.attn.projOut.weight
  models.16._orig_mod.transformer.h.0.mlp.fc.weight
  models.16._orig_mod.transformer.h.0.mlp.proj.weight
  models.16._orig_mod.transformer.h.0.normAtt.weight
  models.16._orig_mod.transformer.h.0.normMlp.weight
  models.16._orig_mod.transformer.h.1.blockLambdas
  models.16._orig_mod.transformer.h.1.attn.lambdas
  models.16._orig_mod.transformer.h.1.attn.projAtt.weight
  models.16._orig_mod.transformer.h.1.attn.projAtt.bias
  models.16._orig_mod.transformer.h.1.attn.projQ.weight
  models.16._orig_mod.transformer.h.1.attn.projK.weight
  models.16._orig_mod.transformer.h.1.attn.projV.weight
  models.16._orig_mod.transformer.h.1.attn.projOut.weight
  models.16._orig_mod.transformer.h.1.mlp.fc.weight
  models.16._orig_mod.transformer.h.1.mlp.proj.weight
  models.16._orig_mod.transformer.h.1.normAtt.weight
  models.16._orig_mod.transformer.h.1.normMlp.weight
  models.16._orig_mod.transformer.h.2.blockLambdas
  models.16._orig_mod.transformer.h.2.attn.lambdas
  models.16._orig_mod.transformer.h.2.attn.projAtt.weight
  models.16._orig_mod.transformer.h.2.attn.projAtt.bias
  models.16._orig_mod.transformer.h.2.attn.projQ.weight
  models.16._orig_mod.transformer.h.2.attn.projK.weight
  models.16._orig_mod.transformer.h.2.attn.projV.weight
  models.16._orig_mod.transformer.h.2.attn.projOut.weight
  models.16._orig_mod.transformer.h.2.mlp.fc.weight
  models.16._orig_mod.transformer.h.2.mlp.proj.weight
  models.16._orig_mod.transformer.h.2.normAtt.weight
  models.16._orig_mod.transformer.h.2.normMlp.weight
  models.16._orig_mod.transformer.h.3.blockLambdas
  models.16._orig_mod.transformer.h.3.attn.lambdas
  models.16._orig_mod.transformer.h.3.attn.projAtt.weight
  models.16._orig_mod.transformer.h.3.attn.projAtt.bias
  models.16._orig_mod.transformer.h.3.attn.projQ.weight
  models.16._orig_mod.transformer.h.3.attn.projK.weight
  models.16._orig_mod.transformer.h.3.attn.projV.weight
  models.16._orig_mod.transformer.h.3.attn.projOut.weight
  models.16._orig_mod.transformer.h.3.mlp.fc.weight
  models.16._orig_mod.transformer.h.3.mlp.proj.weight
  models.16._orig_mod.transformer.h.3.normAtt.weight
  models.16._orig_mod.transformer.h.3.normMlp.weight
  models.16._orig_mod.normWte.weight
  models.16._orig_mod.normWte.bias
Optimizer #17/20: Adam
  Param group 0: lr=0.0006, weight_decay=0.0
  models.17._orig_mod.skipWeights
  models.17._orig_mod.transformer.wte.weight
  models.17._orig_mod.transformer.h.0.blockLambdas
  models.17._orig_mod.transformer.h.0.attn.lambdas
  models.17._orig_mod.transformer.h.0.attn.projAtt.weight
  models.17._orig_mod.transformer.h.0.attn.projAtt.bias
  models.17._orig_mod.transformer.h.0.attn.projQ.weight
  models.17._orig_mod.transformer.h.0.attn.projK.weight
  models.17._orig_mod.transformer.h.0.attn.projV.weight
  models.17._orig_mod.transformer.h.0.attn.projOut.weight
  models.17._orig_mod.transformer.h.0.mlp.fc.weight
  models.17._orig_mod.transformer.h.0.mlp.proj.weight
  models.17._orig_mod.transformer.h.0.normAtt.weight
  models.17._orig_mod.transformer.h.0.normMlp.weight
  models.17._orig_mod.transformer.h.1.blockLambdas
  models.17._orig_mod.transformer.h.1.attn.lambdas
  models.17._orig_mod.transformer.h.1.attn.projAtt.weight
  models.17._orig_mod.transformer.h.1.attn.projAtt.bias
  models.17._orig_mod.transformer.h.1.attn.projQ.weight
  models.17._orig_mod.transformer.h.1.attn.projK.weight
  models.17._orig_mod.transformer.h.1.attn.projV.weight
  models.17._orig_mod.transformer.h.1.attn.projOut.weight
  models.17._orig_mod.transformer.h.1.mlp.fc.weight
  models.17._orig_mod.transformer.h.1.mlp.proj.weight
  models.17._orig_mod.transformer.h.1.normAtt.weight
  models.17._orig_mod.transformer.h.1.normMlp.weight
  models.17._orig_mod.transformer.h.2.blockLambdas
  models.17._orig_mod.transformer.h.2.attn.lambdas
  models.17._orig_mod.transformer.h.2.attn.projAtt.weight
  models.17._orig_mod.transformer.h.2.attn.projAtt.bias
  models.17._orig_mod.transformer.h.2.attn.projQ.weight
  models.17._orig_mod.transformer.h.2.attn.projK.weight
  models.17._orig_mod.transformer.h.2.attn.projV.weight
  models.17._orig_mod.transformer.h.2.attn.projOut.weight
  models.17._orig_mod.transformer.h.2.mlp.fc.weight
  models.17._orig_mod.transformer.h.2.mlp.proj.weight
  models.17._orig_mod.transformer.h.2.normAtt.weight
  models.17._orig_mod.transformer.h.2.normMlp.weight
  models.17._orig_mod.transformer.h.3.blockLambdas
  models.17._orig_mod.transformer.h.3.attn.lambdas
  models.17._orig_mod.transformer.h.3.attn.projAtt.weight
  models.17._orig_mod.transformer.h.3.attn.projAtt.bias
  models.17._orig_mod.transformer.h.3.attn.projQ.weight
  models.17._orig_mod.transformer.h.3.attn.projK.weight
  models.17._orig_mod.transformer.h.3.attn.projV.weight
  models.17._orig_mod.transformer.h.3.attn.projOut.weight
  models.17._orig_mod.transformer.h.3.mlp.fc.weight
  models.17._orig_mod.transformer.h.3.mlp.proj.weight
  models.17._orig_mod.transformer.h.3.normAtt.weight
  models.17._orig_mod.transformer.h.3.normMlp.weight
  models.17._orig_mod.normWte.weight
  models.17._orig_mod.normWte.bias
Optimizer #18/20: Adam
  Param group 0: lr=0.0006, weight_decay=0.0
  models.18._orig_mod.skipWeights
  models.18._orig_mod.transformer.wte.weight
  models.18._orig_mod.transformer.h.0.blockLambdas
  models.18._orig_mod.transformer.h.0.attn.lambdas
  models.18._orig_mod.transformer.h.0.attn.projAtt.weight
  models.18._orig_mod.transformer.h.0.attn.projAtt.bias
  models.18._orig_mod.transformer.h.0.attn.projQ.weight
  models.18._orig_mod.transformer.h.0.attn.projK.weight
  models.18._orig_mod.transformer.h.0.attn.projV.weight
  models.18._orig_mod.transformer.h.0.attn.projOut.weight
  models.18._orig_mod.transformer.h.0.mlp.fc.weight
  models.18._orig_mod.transformer.h.0.mlp.proj.weight
  models.18._orig_mod.transformer.h.0.normAtt.weight
  models.18._orig_mod.transformer.h.0.normMlp.weight
  models.18._orig_mod.transformer.h.1.blockLambdas
  models.18._orig_mod.transformer.h.1.attn.lambdas
  models.18._orig_mod.transformer.h.1.attn.projAtt.weight
  models.18._orig_mod.transformer.h.1.attn.projAtt.bias
  models.18._orig_mod.transformer.h.1.attn.projQ.weight
  models.18._orig_mod.transformer.h.1.attn.projK.weight
  models.18._orig_mod.transformer.h.1.attn.projV.weight
  models.18._orig_mod.transformer.h.1.attn.projOut.weight
  models.18._orig_mod.transformer.h.1.mlp.fc.weight
  models.18._orig_mod.transformer.h.1.mlp.proj.weight
  models.18._orig_mod.transformer.h.1.normAtt.weight
  models.18._orig_mod.transformer.h.1.normMlp.weight
  models.18._orig_mod.transformer.h.2.blockLambdas
  models.18._orig_mod.transformer.h.2.attn.lambdas
  models.18._orig_mod.transformer.h.2.attn.projAtt.weight
  models.18._orig_mod.transformer.h.2.attn.projAtt.bias
  models.18._orig_mod.transformer.h.2.attn.projQ.weight
  models.18._orig_mod.transformer.h.2.attn.projK.weight
  models.18._orig_mod.transformer.h.2.attn.projV.weight
  models.18._orig_mod.transformer.h.2.attn.projOut.weight
  models.18._orig_mod.transformer.h.2.mlp.fc.weight
  models.18._orig_mod.transformer.h.2.mlp.proj.weight
  models.18._orig_mod.transformer.h.2.normAtt.weight
  models.18._orig_mod.transformer.h.2.normMlp.weight
  models.18._orig_mod.transformer.h.3.blockLambdas
  models.18._orig_mod.transformer.h.3.attn.lambdas
  models.18._orig_mod.transformer.h.3.attn.projAtt.weight
  models.18._orig_mod.transformer.h.3.attn.projAtt.bias
  models.18._orig_mod.transformer.h.3.attn.projQ.weight
  models.18._orig_mod.transformer.h.3.attn.projK.weight
  models.18._orig_mod.transformer.h.3.attn.projV.weight
  models.18._orig_mod.transformer.h.3.attn.projOut.weight
  models.18._orig_mod.transformer.h.3.mlp.fc.weight
  models.18._orig_mod.transformer.h.3.mlp.proj.weight
  models.18._orig_mod.transformer.h.3.normAtt.weight
  models.18._orig_mod.transformer.h.3.normMlp.weight
  models.18._orig_mod.normWte.weight
  models.18._orig_mod.normWte.bias
Optimizer #19/20: Adam
  Param group 0: lr=0.0006, weight_decay=0.0
  models.19._orig_mod.skipWeights
  models.19._orig_mod.transformer.wte.weight
  models.19._orig_mod.transformer.h.0.blockLambdas
  models.19._orig_mod.transformer.h.0.attn.lambdas
  models.19._orig_mod.transformer.h.0.attn.projAtt.weight
  models.19._orig_mod.transformer.h.0.attn.projAtt.bias
  models.19._orig_mod.transformer.h.0.attn.projQ.weight
  models.19._orig_mod.transformer.h.0.attn.projK.weight
  models.19._orig_mod.transformer.h.0.attn.projV.weight
  models.19._orig_mod.transformer.h.0.attn.projOut.weight
  models.19._orig_mod.transformer.h.0.mlp.fc.weight
  models.19._orig_mod.transformer.h.0.mlp.proj.weight
  models.19._orig_mod.transformer.h.0.normAtt.weight
  models.19._orig_mod.transformer.h.0.normMlp.weight
  models.19._orig_mod.transformer.h.1.blockLambdas
  models.19._orig_mod.transformer.h.1.attn.lambdas
  models.19._orig_mod.transformer.h.1.attn.projAtt.weight
  models.19._orig_mod.transformer.h.1.attn.projAtt.bias
  models.19._orig_mod.transformer.h.1.attn.projQ.weight
  models.19._orig_mod.transformer.h.1.attn.projK.weight
  models.19._orig_mod.transformer.h.1.attn.projV.weight
  models.19._orig_mod.transformer.h.1.attn.projOut.weight
  models.19._orig_mod.transformer.h.1.mlp.fc.weight
  models.19._orig_mod.transformer.h.1.mlp.proj.weight
  models.19._orig_mod.transformer.h.1.normAtt.weight
  models.19._orig_mod.transformer.h.1.normMlp.weight
  models.19._orig_mod.transformer.h.2.blockLambdas
  models.19._orig_mod.transformer.h.2.attn.lambdas
  models.19._orig_mod.transformer.h.2.attn.projAtt.weight
  models.19._orig_mod.transformer.h.2.attn.projAtt.bias
  models.19._orig_mod.transformer.h.2.attn.projQ.weight
  models.19._orig_mod.transformer.h.2.attn.projK.weight
  models.19._orig_mod.transformer.h.2.attn.projV.weight
  models.19._orig_mod.transformer.h.2.attn.projOut.weight
  models.19._orig_mod.transformer.h.2.mlp.fc.weight
  models.19._orig_mod.transformer.h.2.mlp.proj.weight
  models.19._orig_mod.transformer.h.2.normAtt.weight
  models.19._orig_mod.transformer.h.2.normMlp.weight
  models.19._orig_mod.transformer.h.3.blockLambdas
  models.19._orig_mod.transformer.h.3.attn.lambdas
  models.19._orig_mod.transformer.h.3.attn.projAtt.weight
  models.19._orig_mod.transformer.h.3.attn.projAtt.bias
  models.19._orig_mod.transformer.h.3.attn.projQ.weight
  models.19._orig_mod.transformer.h.3.attn.projK.weight
  models.19._orig_mod.transformer.h.3.attn.projV.weight
  models.19._orig_mod.transformer.h.3.attn.projOut.weight
  models.19._orig_mod.transformer.h.3.mlp.fc.weight
  models.19._orig_mod.transformer.h.3.mlp.proj.weight
  models.19._orig_mod.transformer.h.3.normAtt.weight
  models.19._orig_mod.transformer.h.3.normMlp.weight
  models.19._orig_mod.normWte.weight
  models.19._orig_mod.normWte.bias
Optimizer #20/20: Adam
  Param group 0: lr=0.1, weight_decay=0
  models.0._orig_mod.transformer.h.0.attn.af.afVals
  models.0._orig_mod.transformer.h.0.mlp.af.afVals
  models.0._orig_mod.transformer.h.1.attn.af.afVals
  models.0._orig_mod.transformer.h.1.mlp.af.afVals
  models.0._orig_mod.transformer.h.2.attn.af.afVals
  models.0._orig_mod.transformer.h.2.mlp.af.afVals
  models.0._orig_mod.transformer.h.3.attn.af.afVals
  models.0._orig_mod.transformer.h.3.mlp.af.afVals
====================================================================================================
Step 1/20000   lossVa 15.1172   accVa 0.0
Step 1/20000   lossTr 15.1678   accTr 0.0   15s = 15.48s/step
Step 1/20000   Activation magnitudes (2): 0.80 0.26 0.80 0.26 0.80 0.25 0.80 0.26
Step 1/20000   Activation magnitudes (3): 4.56 1.66 4.56 1.56 4.56 1.62 4.56 1.75
Step 6/20000   Peak CUDA memory usage: 2.839 GB
Step 6/20000   lossTr 15.1249   accTr 0.0   26s = 2.05s/step
Step 6/20000   Activation magnitudes (2): 0.80 0.26 0.80 0.26 0.80 0.25 0.80 0.26
Step 6/20000   Activation magnitudes (3): 4.70 1.63 4.70 1.66 4.70 1.62 4.70 1.64
Step 11/20000   Peak CUDA memory usage: 2.797 GB
Step 201/20000   lossTr 13.0572   accTr 0.7   38s = 0.06s/step
Step 201/20000   Activation magnitudes (2): 0.80 0.30 0.80 0.24 0.82 0.21 0.84 0.23
Step 201/20000   Activation magnitudes (3): 4.78 1.70 4.86 1.55 4.37 1.65 4.14 1.69
Step 401/20000   lossTr 7.4561   accTr 9.0   50s = 0.06s/step
Step 401/20000   Activation magnitudes (2): 0.80 0.29 0.80 0.25 0.80 0.27 0.81 0.26
Step 401/20000   Activation magnitudes (3): 4.86 1.62 4.49 1.78 4.75 1.77 4.23 1.91
Step 501/20000   lossVa 6.8864   accVa 12.2
Step 601/20000   lossTr 6.8970   accTr 12.1   62s = 0.06s/step
Step 601/20000   Activation magnitudes (2): 0.81 0.30 0.80 0.26 0.80 0.29 0.80 0.29
Step 601/20000   Activation magnitudes (3): 4.71 1.78 4.95 1.69 4.81 1.73 4.28 1.87
Step 801/20000   lossTr 6.7009   accTr 13.5   74s = 0.06s/step
Step 801/20000   Activation magnitudes (2): 0.83 0.30 0.80 0.28 0.80 0.31 0.79 0.30
Step 801/20000   Activation magnitudes (3): 4.87 1.76 4.97 2.23 4.35 2.19 4.51 2.13
Step 1001/20000   lossVa 6.4677   accVa 15.4
Step 1001/20000   lossTr 6.5233   accTr 14.7   86s = 0.06s/step
Step 1001/20000   Activation magnitudes (2): 0.85 0.30 0.80 0.30 0.79 0.36 0.78 0.32
Step 1001/20000   Activation magnitudes (3): 4.81 2.12 4.95 2.38 4.62 2.32 5.26 2.14
Step 1201/20000   lossTr 6.4333   accTr 15.3   111s = 0.12s/step
Step 1201/20000   Activation magnitudes (2): 0.86 0.31 0.82 0.33 0.80 0.38 0.79 0.33
Step 1201/20000   Activation magnitudes (3): 5.21 2.16 4.48 2.45 5.04 2.66 5.97 2.33
Step 1401/20000   lossTr 6.4234   accTr 15.4   135s = 0.12s/step
Step 1401/20000   Activation magnitudes (2): 0.84 0.34 0.82 0.37 0.82 0.42 0.80 0.36
Step 1401/20000   Activation magnitudes (3): 4.71 2.13 4.64 2.32 5.77 2.73 6.58 2.65
Step 1501/20000   lossVa 6.3156   accVa 16.0
Step 1601/20000   lossTr 6.3154   accTr 15.9   160s = 0.12s/step
Step 1601/20000   Activation magnitudes (2): 0.81 0.36 0.82 0.40 0.81 0.43 0.79 0.35
Step 1601/20000   Activation magnitudes (3): 5.06 2.22 5.86 2.61 6.86 2.96 7.34 2.71
Step 1801/20000   lossTr 6.2162   accTr 16.4   186s = 0.13s/step
Step 1801/20000   Activation magnitudes (2): 0.78 0.37 0.81 0.42 0.82 0.45 0.79 0.37
Step 1801/20000   Activation magnitudes (3): 4.56 2.23 6.07 2.94 7.13 2.96 7.90 2.83
Step 2001/20000   lossVa 6.1490   accVa 17.0
Step 2001/20000   lossTr 6.1448   accTr 16.8   211s = 0.12s/step
Step 2001/20000   Activation magnitudes (2): 0.73 0.39 0.80 0.45 0.81 0.47 0.77 0.38
Step 2001/20000   Activation magnitudes (3): 4.79 2.24 6.07 2.97 7.69 2.96 8.32 2.72
Step 2201/20000   lossTr 6.0827   accTr 17.0   247s = 0.18s/step
Step 2201/20000   Activation magnitudes (2): 0.71 0.42 0.84 0.52 0.84 0.51 0.80 0.40
Step 2201/20000   Activation magnitudes (3): 4.71 2.55 6.18 3.37 8.03 3.28 9.48 3.06
Step 2401/20000   lossTr 6.1465   accTr 16.9   283s = 0.18s/step
Step 2401/20000   Activation magnitudes (2): 0.70 0.44 0.81 0.55 0.84 0.54 0.79 0.42
Step 2401/20000   Activation magnitudes (3): 4.85 2.55 6.72 3.39 9.22 3.16 10.47 2.91
Step 2501/20000   lossVa 6.0516   accVa 17.4
Step 2601/20000   lossTr 6.0552   accTr 17.4   320s = 0.18s/step
Step 2601/20000   Activation magnitudes (2): 0.68 0.45 0.81 0.56 0.83 0.56 0.77 0.42
Step 2601/20000   Activation magnitudes (3): 4.77 2.78 7.52 3.52 9.51 3.21 11.00 3.05
Step 2801/20000   lossTr 6.0059   accTr 17.6   355s = 0.18s/step
Step 2801/20000   Activation magnitudes (2): 0.67 0.44 0.81 0.56 0.83 0.57 0.74 0.41
Step 2801/20000   Activation magnitudes (3): 5.00 2.90 7.78 3.73 9.35 3.46 10.89 3.18
Step 3001/20000   lossVa 5.8539   accVa 18.6
Step 3001/20000   lossTr 5.9121   accTr 18.2   391s = 0.18s/step
Step 3001/20000   Activation magnitudes (2): 0.66 0.44 0.80 0.56 0.81 0.58 0.72 0.40
Step 3001/20000   Activation magnitudes (3): 5.16 2.94 7.82 4.07 9.28 3.88 12.11 3.10
Step 3201/20000   lossTr 5.8636   accTr 18.5   438s = 0.24s/step
Step 3201/20000   Activation magnitudes (2): 0.67 0.47 0.83 0.61 0.84 0.61 0.74 0.42
Step 3201/20000   Activation magnitudes (3): 5.29 3.24 8.32 3.98 9.14 3.81 12.23 3.48
Step 3401/20000   lossTr 5.8272   accTr 18.7   485s = 0.24s/step
Step 3401/20000   Activation magnitudes (2): 0.68 0.47 0.82 0.62 0.82 0.61 0.71 0.41
Step 3401/20000   Activation magnitudes (3): 5.30 3.31 8.55 4.04 9.66 3.73 12.21 3.40
Step 3501/20000   lossVa 5.7757   accVa 19.3
Step 3601/20000   lossTr 5.8281   accTr 19.1   532s = 0.23s/step
Step 3601/20000   Activation magnitudes (2): 0.68 0.47 0.83 0.62 0.81 0.63 0.71 0.41
Step 3601/20000   Activation magnitudes (3): 5.35 3.32 8.22 4.44 9.15 4.14 12.23 3.18
Step 3801/20000   lossTr 5.7003   accTr 19.7   578s = 0.23s/step
Step 3801/20000   Activation magnitudes (2): 0.68 0.48 0.83 0.61 0.81 0.66 0.69 0.42
Step 3801/20000   Activation magnitudes (3): 5.29 3.21 8.75 4.57 9.52 4.40 12.68 3.69
Step 4001/20000   lossVa 5.6200   accVa 20.7
Step 4001/20000   lossTr 5.6229   accTr 20.6   624s = 0.23s/step
Step 4001/20000   Activation magnitudes (2): 0.68 0.48 0.83 0.61 0.81 0.65 0.68 0.42
Step 4001/20000   Activation magnitudes (3): 5.29 3.63 8.64 4.46 9.85 4.55 13.20 3.37
Step 4201/20000   lossTr 5.6160   accTr 20.5   682s = 0.29s/step
Step 4201/20000   Activation magnitudes (2): 0.70 0.50 0.85 0.65 0.82 0.68 0.69 0.44
Step 4201/20000   Activation magnitudes (3): 5.47 3.26 8.74 4.73 10.14 5.18 13.41 3.72
Step 4401/20000   lossTr 5.5821   accTr 20.9   739s = 0.29s/step
Step 4401/20000   Activation magnitudes (2): 0.70 0.49 0.85 0.66 0.82 0.67 0.70 0.43
Step 4401/20000   Activation magnitudes (3): 5.89 4.10 8.51 4.82 9.70 4.78 13.13 3.86
Step 4501/20000   lossVa 5.5596   accVa 21.2
Step 4601/20000   lossTr 5.5644   accTr 21.2   797s = 0.29s/step
Step 4601/20000   Activation magnitudes (2): 0.70 0.50 0.85 0.67 0.81 0.69 0.68 0.44
Step 4601/20000   Activation magnitudes (3): 6.31 3.60 9.14 5.04 9.87 5.07 13.28 3.51
Step 4801/20000   lossTr 5.5335   accTr 21.5   855s = 0.29s/step
Step 4801/20000   Activation magnitudes (2): 0.69 0.51 0.85 0.67 0.81 0.70 0.67 0.45
Step 4801/20000   Activation magnitudes (3): 6.67 3.60 9.41 5.08 9.98 4.99 12.99 4.79
Step 5001/20000   lossVa 5.4384   accVa 22.2
Step 5001/20000   lossTr 5.5065   accTr 21.6   912s = 0.29s/step
Step 5001/20000   Activation magnitudes (2): 0.68 0.51 0.85 0.68 0.81 0.74 0.67 0.45
Step 5001/20000   Activation magnitudes (3): 7.02 3.74 9.06 5.40 10.02 5.46 13.22 4.00
Step 5201/20000   lossTr 5.4429   accTr 22.0   982s = 0.35s/step
Step 5201/20000   Activation magnitudes (2): 0.69 0.53 0.88 0.71 0.81 0.74 0.68 0.45
Step 5201/20000   Activation magnitudes (3): 7.30 4.16 9.40 5.46 10.37 5.60 13.20 3.85
Step 5401/20000   lossTr 5.4486   accTr 22.1   1051s = 0.35s/step
Step 5401/20000   Activation magnitudes (2): 0.69 0.54 0.86 0.72 0.81 0.76 0.68 0.47
Step 5401/20000   Activation magnitudes (3): 7.34 3.88 9.15 5.87 10.10 5.79 13.18 4.21
Step 5501/20000   lossVa 5.4117   accVa 22.4
Step 5601/20000   lossTr 5.3993   accTr 22.5   1121s = 0.35s/step
Step 5601/20000   Activation magnitudes (2): 0.68 0.53 0.86 0.72 0.81 0.76 0.67 0.46
Step 5601/20000   Activation magnitudes (3): 7.54 3.97 8.95 5.98 10.19 5.71 13.18 4.10
Step 5801/20000   lossTr 5.3899   accTr 22.7   1197s = 0.38s/step
Step 5801/20000   Activation magnitudes (2): 0.67 0.53 0.86 0.72 0.81 0.76 0.67 0.47
Step 5801/20000   Activation magnitudes (3): 7.46 3.72 8.86 6.38 10.03 6.00 12.88 4.72
Step 6001/20000   lossVa 5.3641   accVa 22.8
Step 6001/20000   lossTr 5.3910   accTr 22.6   1274s = 0.39s/step
Step 6001/20000   Activation magnitudes (2): 0.67 0.54 0.85 0.73 0.81 0.80 0.66 0.46
Step 6001/20000   Activation magnitudes (3): 7.48 4.00 8.75 6.82 9.58 6.68 13.15 4.36
Step 6201/20000   lossTr 5.3769   accTr 22.7   1361s = 0.43s/step
Step 6201/20000   Activation magnitudes (2): 0.67 0.55 0.88 0.76 0.81 0.80 0.68 0.47
Step 6201/20000   Activation magnitudes (3): 7.62 4.03 8.54 6.62 9.30 6.69 12.50 4.29
Step 6401/20000   lossTr 5.3552   accTr 22.7   1445s = 0.42s/step
Step 6401/20000   Activation magnitudes (2): 0.68 0.57 0.87 0.77 0.81 0.79 0.67 0.49
Step 6401/20000   Activation magnitudes (3): 7.60 3.83 8.37 6.25 9.64 6.67 12.82 4.39
Step 6501/20000   lossVa 5.3453   accVa 22.8
Step 6601/20000   lossTr 5.3727   accTr 22.9   1525s = 0.40s/step
Step 6601/20000   Activation magnitudes (2): 0.67 0.57 0.86 0.77 0.81 0.83 0.66 0.49
Step 6601/20000   Activation magnitudes (3): 7.64 3.85 8.38 6.82 9.65 7.24 12.95 4.32
Step 6801/20000   lossTr 5.3218   accTr 23.0   1604s = 0.39s/step
Step 6801/20000   Activation magnitudes (2): 0.66 0.57 0.86 0.78 0.81 0.84 0.68 0.49
Step 6801/20000   Activation magnitudes (3): 7.71 4.03 8.45 6.81 9.15 7.12 12.45 5.41
Step 7001/20000   lossVa 5.3079   accVa 23.3
Step 7001/20000   lossTr 5.3029   accTr 23.2   1683s = 0.40s/step
Step 7001/20000   Activation magnitudes (2): 0.65 0.57 0.85 0.79 0.81 0.88 0.66 0.48
Step 7001/20000   Activation magnitudes (3): 7.65 4.03 8.69 6.82 9.55 7.64 13.02 4.61
Step 7201/20000   lossTr 5.3069   accTr 23.1   1779s = 0.48s/step
Step 7201/20000   Activation magnitudes (2): 0.66 0.59 0.87 0.83 0.81 0.86 0.67 0.49
Step 7201/20000   Activation magnitudes (3): 7.74 4.75 10.19 7.52 10.65 7.50 12.83 4.53
Step 7401/20000   lossTr 5.2813   accTr 23.5   1876s = 0.48s/step
Step 7401/20000   Activation magnitudes (2): 0.65 0.60 0.86 0.83 0.81 0.88 0.68 0.50
Step 7401/20000   Activation magnitudes (3): 7.81 4.15 9.19 7.45 10.07 8.01 12.89 4.71
Step 7501/20000   lossVa 5.2768   accVa 23.3
Step 7601/20000   lossTr 5.2919   accTr 23.5   1970s = 0.47s/step
Step 7601/20000   Activation magnitudes (2): 0.66 0.60 0.86 0.83 0.82 0.90 0.68 0.52
Step 7601/20000   Activation magnitudes (3): 5.39 4.51 8.76 7.13 10.28 7.28 13.06 5.27
Step 7801/20000   lossTr 5.2581   accTr 23.6   2065s = 0.47s/step
Step 7801/20000   Activation magnitudes (2): 0.65 0.61 0.85 0.84 0.81 0.90 0.67 0.52
Step 7801/20000   Activation magnitudes (3): 7.79 4.44 9.89 7.32 10.76 8.42 13.44 5.44
Step 8001/20000   lossVa 5.2490   accVa 23.8
Step 8001/20000   lossTr 5.2698   accTr 23.4   2158s = 0.46s/step
Step 8001/20000   Activation magnitudes (2): 0.64 0.61 0.85 0.85 0.81 0.94 0.67 0.51
Step 8001/20000   Activation magnitudes (3): 7.78 4.63 10.13 8.09 10.78 8.11 13.75 4.91
Step 8201/20000   lossTr 5.2882   accTr 23.5   2265s = 0.54s/step
Step 8201/20000   Activation magnitudes (2): 0.65 0.63 0.87 0.87 0.82 0.92 0.68 0.51
Step 8201/20000   Activation magnitudes (3): 7.84 4.53 10.66 7.71 11.22 7.78 13.84 5.35
Step 8401/20000   lossTr 5.2844   accTr 23.3   2369s = 0.52s/step
Step 8401/20000   Activation magnitudes (2): 0.65 0.62 0.86 0.88 0.82 0.93 0.69 0.53
Step 8401/20000   Activation magnitudes (3): 7.78 4.37 10.41 7.26 11.15 8.46 13.97 6.35
Step 8501/20000   lossVa 5.2500   accVa 23.7
Step 8601/20000   lossTr 5.2706   accTr 23.6   2476s = 0.54s/step
Step 8601/20000   Activation magnitudes (2): 0.64 0.61 0.87 0.88 0.82 0.93 0.70 0.52
Step 8601/20000   Activation magnitudes (3): 7.82 4.68 10.46 8.24 10.79 8.20 13.80 5.56
Step 8801/20000   lossTr 5.2625   accTr 23.6   2579s = 0.52s/step
Step 8801/20000   Activation magnitudes (2): 0.64 0.62 0.86 0.88 0.81 0.96 0.68 0.51
Step 8801/20000   Activation magnitudes (3): 5.10 4.39 9.63 7.69 10.81 8.08 13.71 5.45
Step 9001/20000   lossVa 5.2192   accVa 24.1
Step 9001/20000   lossTr 5.2341   accTr 23.9   2684s = 0.52s/step
Step 9001/20000   Activation magnitudes (2): 0.64 0.63 0.86 0.90 0.82 1.01 0.67 0.53
Step 9001/20000   Activation magnitudes (3): 7.81 4.62 9.95 7.95 10.76 8.88 13.31 6.49
Step 9201/20000   lossTr 5.2379   accTr 23.7   2799s = 0.58s/step
Step 9201/20000   Activation magnitudes (2): 0.64 0.65 0.87 0.93 0.82 0.99 0.68 0.54
Step 9201/20000   Activation magnitudes (3): 7.87 4.96 10.15 8.38 10.63 8.77 12.99 6.35
Step 9401/20000   lossTr 5.2673   accTr 23.5   2916s = 0.58s/step
Step 9401/20000   Activation magnitudes (2): 0.64 0.65 0.86 0.92 0.82 1.02 0.69 0.56
Step 9401/20000   Activation magnitudes (3): 7.90 4.37 9.56 8.49 10.49 8.39 12.46 6.37
Step 9501/20000   lossVa 5.2005   accVa 24.1
Step 9601/20000   lossTr 5.2247   accTr 23.8   3031s = 0.58s/step
Step 9601/20000   Activation magnitudes (2): 0.63 0.65 0.86 0.95 0.82 1.00 0.69 0.54
Step 9601/20000   Activation magnitudes (3): 7.70 5.05 9.20 9.16 9.84 8.88 11.92 7.02
Step 9801/20000   lossTr 5.2168   accTr 23.9   3148s = 0.58s/step
Step 9801/20000   Activation magnitudes (2): 0.63 0.66 0.86 0.96 0.82 1.03 0.68 0.55
Step 9801/20000   Activation magnitudes (3): 7.61 4.72 9.04 8.81 9.33 9.26 12.41 7.11
Step 10001/20000   lossVa 5.1927   accVa 24.2
Step 10001/20000   lossTr 5.2428   accTr 23.9   3262s = 0.57s/step
Step 10001/20000   Activation magnitudes (2): 0.62 0.66 0.85 0.97 0.82 1.08 0.68 0.55
Step 10001/20000   Activation magnitudes (3): 7.48 5.10 8.31 9.43 8.82 10.19 11.80 6.24
Step 10201/20000   lossTr 5.2230   accTr 23.9   3387s = 0.62s/step
Step 10201/20000   Activation magnitudes (2): 0.62 0.67 0.87 1.01 0.82 1.06 0.70 0.56
Step 10201/20000   Activation magnitudes (3): 7.58 5.93 8.67 9.27 9.22 9.85 12.39 6.09
Step 10401/20000   lossTr 5.2016   accTr 24.2   3511s = 0.62s/step
Step 10401/20000   Activation magnitudes (2): 0.63 0.68 0.87 1.00 0.83 1.06 0.70 0.57
Step 10401/20000   Activation magnitudes (3): 7.57 5.16 8.37 8.58 8.54 9.85 11.82 5.99
Step 10501/20000   lossVa 5.1862   accVa 24.2
Step 10601/20000   lossTr 5.1563   accTr 24.3   3635s = 0.62s/step
Step 10601/20000   Activation magnitudes (2): 0.62 0.68 0.86 1.02 0.82 1.11 0.69 0.58
Step 10601/20000   Activation magnitudes (3): 7.55 5.66 8.00 9.90 8.74 9.60 12.36 7.30
Step 10801/20000   lossTr 5.1471   accTr 24.3   3758s = 0.61s/step
Step 10801/20000   Activation magnitudes (2): 0.62 0.69 0.86 1.03 0.83 1.12 0.69 0.57
Step 10801/20000   Activation magnitudes (3): 7.60 5.30 8.03 9.68 8.76 9.70 11.63 7.77
Step 11001/20000   lossVa 5.1503   accVa 24.5
Step 11001/20000   lossTr 5.1662   accTr 24.2   3880s = 0.61s/step
Step 11001/20000   Activation magnitudes (2): 0.62 0.69 0.86 1.04 0.83 1.17 0.69 0.58
Step 11001/20000   Activation magnitudes (3): 7.52 5.33 8.14 9.42 9.08 10.70 11.58 7.20
Step 11201/20000   lossTr 5.1724   accTr 24.4   4017s = 0.68s/step
Step 11201/20000   Activation magnitudes (2): 0.63 0.70 0.88 1.08 0.83 1.14 0.70 0.59
Step 11201/20000   Activation magnitudes (3): 7.57 4.98 8.57 8.97 8.92 10.84 12.03 6.55
Step 11401/20000   lossTr 5.1212   accTr 24.6   4151s = 0.67s/step
Step 11401/20000   Activation magnitudes (2): 0.63 0.70 0.87 1.09 0.82 1.17 0.69 0.58
Step 11401/20000   Activation magnitudes (3): 7.58 5.35 8.07 9.21 9.03 10.20 12.14 6.60
Step 11501/20000   lossVa 5.1490   accVa 24.6
Step 11601/20000   lossTr 5.1259   accTr 24.6   4286s = 0.68s/step
Step 11601/20000   Activation magnitudes (2): 0.62 0.70 0.87 1.09 0.83 1.20 0.70 0.60
Step 11601/20000   Activation magnitudes (3): 7.61 5.00 8.34 9.97 8.88 10.80 12.14 6.97
Step 11801/20000   lossTr 5.1377   accTr 24.7   4420s = 0.67s/step
Step 11801/20000   Activation magnitudes (2): 0.63 0.71 0.87 1.09 0.83 1.19 0.71 0.60
Step 11801/20000   Activation magnitudes (3): 7.64 5.20 8.13 10.03 8.79 11.10 12.43 6.70
Step 12001/20000   lossVa 5.1210   accVa 24.7
Step 12001/20000   lossTr 5.1399   accTr 24.5   4553s = 0.67s/step
Step 12001/20000   Activation magnitudes (2): 0.63 0.70 0.87 1.09 0.83 1.24 0.70 0.59
Step 12001/20000   Activation magnitudes (3): 7.67 5.25 8.35 9.63 9.06 10.36 11.76 7.06
Step 12201/20000   lossTr 5.1178   accTr 24.8   4697s = 0.72s/step
Step 12201/20000   Activation magnitudes (2): 0.63 0.72 0.89 1.12 0.84 1.22 0.74 0.61
Step 12201/20000   Activation magnitudes (3): 7.71 5.19 8.69 9.74 8.71 11.05 12.71 7.46
Step 12401/20000   lossTr 5.0974   accTr 24.6   4843s = 0.73s/step
Step 12401/20000   Activation magnitudes (2): 0.64 0.73 0.88 1.14 0.83 1.23 0.71 0.61
Step 12401/20000   Activation magnitudes (3): 7.73 5.36 8.59 10.01 8.82 10.87 12.38 7.65
Step 12501/20000   lossVa 5.1158   accVa 24.8
Step 12601/20000   lossTr 5.1465   accTr 24.4   4987s = 0.72s/step
Step 12601/20000   Activation magnitudes (2): 0.64 0.73 0.87 1.08 0.84 1.27 0.69 0.60
Step 12601/20000   Activation magnitudes (3): 4.60 5.11 8.07 10.73 8.63 11.12 12.27 7.80
Step 12801/20000   lossTr 5.1391   accTr 24.5   5130s = 0.72s/step
Step 12801/20000   Activation magnitudes (2): 0.64 0.72 0.88 1.15 0.83 1.27 0.73 0.63
Step 12801/20000   Activation magnitudes (3): 7.82 5.17 8.01 10.70 8.74 11.46 12.68 6.55
Step 13001/20000   lossVa 5.0933   accVa 25.0
Step 13001/20000   lossTr 5.1484   accTr 24.6   5270s = 0.70s/step
Step 13001/20000   Activation magnitudes (2): 0.64 0.72 0.88 1.13 0.84 1.31 0.72 0.62
Step 13001/20000   Activation magnitudes (3): 7.92 5.44 8.10 11.34 8.88 11.81 11.86 7.28
Step 13201/20000   lossTr 5.1244   accTr 24.7   5421s = 0.76s/step
Step 13201/20000   Activation magnitudes (2): 0.66 0.74 0.89 1.17 0.84 1.28 0.73 0.64
Step 13201/20000   Activation magnitudes (3): 4.21 5.27 8.06 10.58 8.76 11.59 11.91 7.42
Step 13401/20000   lossTr 5.1119   accTr 24.9   5581s = 0.80s/step
Step 13401/20000   Activation magnitudes (2): 0.65 0.74 0.89 1.17 0.84 1.31 0.73 0.63
Step 13401/20000   Activation magnitudes (3): 8.03 5.27 8.38 9.71 8.98 11.88 13.47 7.94
Step 13501/20000   lossVa 5.0847   accVa 25.1
Step 13601/20000   lossTr 5.0830   accTr 25.2   5735s = 0.77s/step
Step 13601/20000   Activation magnitudes (2): 0.66 0.73 0.89 1.17 0.84 1.30 0.75 0.65
Step 13601/20000   Activation magnitudes (3): 8.11 5.32 8.22 11.35 9.46 11.81 12.05 7.48
Step 13801/20000   lossTr 5.0953   accTr 24.8   5885s = 0.75s/step
Step 13801/20000   Activation magnitudes (2): 0.67 0.73 0.89 1.18 0.84 1.33 0.76 0.65
Step 13801/20000   Activation magnitudes (3): 8.15 5.34 8.09 10.99 8.98 11.81 12.18 7.38
Step 14001/20000   lossVa 5.0785   accVa 25.2
Step 14001/20000   lossTr 5.0635   accTr 25.0   6039s = 0.77s/step
Step 14001/20000   Activation magnitudes (2): 0.67 0.73 0.89 1.17 0.84 1.34 0.74 0.64
Step 14001/20000   Activation magnitudes (3): 8.18 5.40 8.29 11.82 9.10 11.88 12.42 7.30
Step 14201/20000   lossTr 5.0842   accTr 25.0   6204s = 0.83s/step
Step 14201/20000   Activation magnitudes (2): 0.68 0.75 0.90 1.20 0.85 1.33 0.79 0.67
Step 14201/20000   Activation magnitudes (3): 8.29 5.34 8.44 10.20 9.10 11.71 12.89 8.47
Step 14401/20000   lossTr 5.0741   accTr 24.9   6369s = 0.82s/step
Step 14401/20000   Activation magnitudes (2): 0.68 0.74 0.90 1.20 0.85 1.34 0.77 0.66
Step 14401/20000   Activation magnitudes (3): 8.35 5.78 8.14 10.52 9.17 11.91 12.55 7.63
Step 14501/20000   lossVa 5.0550   accVa 25.3
Step 14601/20000   lossTr 5.0467   accTr 25.2   6534s = 0.83s/step
Step 14601/20000   Activation magnitudes (2): 0.68 0.73 0.90 1.17 0.84 1.36 0.78 0.66
Step 14601/20000   Activation magnitudes (3): 8.46 5.66 8.19 10.82 9.26 12.08 12.36 7.79
Step 14801/20000   lossTr 5.0747   accTr 25.0   6696s = 0.81s/step
Step 14801/20000   Activation magnitudes (2): 0.69 0.75 0.90 1.22 0.85 1.36 0.79 0.68
Step 14801/20000   Activation magnitudes (3): 8.50 5.31 7.88 11.23 9.34 12.71 13.67 8.42
Step 15001/20000   lossVa 5.0623   accVa 25.3
Step 15001/20000   lossTr 5.0284   accTr 25.4   6858s = 0.81s/step
Step 15001/20000   Activation magnitudes (2): 0.69 0.74 0.89 1.21 0.85 1.41 0.78 0.67
Step 15001/20000   Activation magnitudes (3): 8.49 5.61 8.25 12.27 9.20 12.80 12.43 7.53
Step 15201/20000   lossTr 5.0552   accTr 25.1   7032s = 0.87s/step
Step 15201/20000   Activation magnitudes (2): 0.70 0.76 0.90 1.22 0.85 1.37 0.81 0.68
Step 15201/20000   Activation magnitudes (3): 8.58 5.33 8.30 11.52 9.51 11.80 13.25 8.02
Step 15401/20000   lossTr 5.0199   accTr 25.3   7208s = 0.88s/step
Step 15401/20000   Activation magnitudes (2): 0.70 0.76 0.90 1.26 0.85 1.38 0.81 0.70
Step 15401/20000   Activation magnitudes (3): 8.64 5.64 8.16 11.41 9.55 11.39 13.33 8.02
Step 15501/20000   lossVa 5.0283   accVa 25.5
Step 15601/20000   lossTr 5.0277   accTr 25.4   7383s = 0.88s/step
Step 15601/20000   Activation magnitudes (2): 0.71 0.75 0.90 1.23 0.85 1.40 0.80 0.68
Step 15601/20000   Activation magnitudes (3): 8.72 6.12 8.35 11.65 9.26 11.96 13.65 8.19
Step 15801/20000   lossTr 5.0345   accTr 25.5   7558s = 0.88s/step
Step 15801/20000   Activation magnitudes (2): 0.71 0.75 0.90 1.25 0.85 1.41 0.81 0.69
Step 15801/20000   Activation magnitudes (3): 8.75 6.03 8.09 11.99 9.41 12.53 12.68 7.94
Step 16001/20000   lossVa 5.0112   accVa 25.6
Step 16001/20000   lossTr 5.0664   accTr 25.1   7733s = 0.88s/step
Step 16001/20000   Activation magnitudes (2): 0.71 0.75 0.90 1.24 0.86 1.45 0.80 0.68
Step 16001/20000   Activation magnitudes (3): 8.75 5.64 8.11 11.75 9.44 12.59 12.77 7.48
Step 16201/20000   lossTr 5.0279   accTr 25.3   7921s = 0.94s/step
Step 16201/20000   Activation magnitudes (2): 0.72 0.76 0.91 1.26 0.87 1.40 0.86 0.72
Step 16201/20000   Activation magnitudes (3): 8.82 5.43 8.33 11.41 9.23 11.91 13.46 7.66
Step 16401/20000   lossTr 5.0073   accTr 25.6   8111s = 0.95s/step
Step 16401/20000   Activation magnitudes (2): 0.73 0.76 0.91 1.30 0.85 1.40 0.82 0.71
Step 16401/20000   Activation magnitudes (3): 8.83 5.83 8.35 10.88 9.34 12.66 12.79 7.88
Step 16501/20000   lossVa 4.9968   accVa 25.7
Step 16601/20000   lossTr 5.0129   accTr 25.6   8300s = 0.94s/step
Step 16601/20000   Activation magnitudes (2): 0.73 0.75 0.91 1.28 0.86 1.43 0.84 0.71
Step 16601/20000   Activation magnitudes (3): 8.90 5.84 8.16 11.63 9.46 12.77 12.97 8.14
Step 16801/20000   lossTr 4.9953   accTr 25.6   8487s = 0.94s/step
Step 16801/20000   Activation magnitudes (2): 0.73 0.76 0.90 1.26 0.86 1.46 0.84 0.73
Step 16801/20000   Activation magnitudes (3): 4.46 5.71 7.89 11.41 9.24 11.68 13.42 8.53
Step 17001/20000   lossVa 4.9834   accVa 25.9
Step 17001/20000   lossTr 4.9605   accTr 25.9   8672s = 0.92s/step
Step 17001/20000   Activation magnitudes (2): 0.73 0.75 0.90 1.27 0.86 1.51 0.83 0.72
Step 17001/20000   Activation magnitudes (3): 9.02 5.80 8.18 12.31 9.55 12.26 12.81 8.02
Step 17201/20000   lossTr 5.0128   accTr 25.7   8870s = 0.99s/step
Step 17201/20000   Activation magnitudes (2): 0.74 0.76 0.91 1.28 0.86 1.46 0.86 0.73
Step 17201/20000   Activation magnitudes (3): 9.06 5.89 8.29 10.56 9.56 12.08 13.48 8.23
Step 17401/20000   lossTr 4.9733   accTr 25.7   9066s = 0.98s/step
Step 17401/20000   Activation magnitudes (2): 0.74 0.76 0.91 1.31 0.86 1.49 0.85 0.72
Step 17401/20000   Activation magnitudes (3): 9.12 5.83 8.15 11.96 9.58 12.59 13.23 7.90
Step 17501/20000   lossVa 4.9741   accVa 25.9
Step 17601/20000   lossTr 4.9713   accTr 25.8   9263s = 0.99s/step
Step 17601/20000   Activation magnitudes (2): 0.75 0.76 0.90 1.30 0.86 1.46 0.86 0.73
Step 17601/20000   Activation magnitudes (3): 9.18 5.78 8.10 11.73 9.39 12.31 12.76 8.18
Step 17801/20000   lossTr 4.9596   accTr 26.0   9460s = 0.98s/step
Step 17801/20000   Activation magnitudes (2): 0.75 0.75 0.91 1.29 0.87 1.52 0.89 0.74
Step 17801/20000   Activation magnitudes (3): 9.21 5.88 8.03 11.62 9.73 12.38 13.96 9.06
Step 18001/20000   lossVa 4.9613   accVa 26.0
Step 18001/20000   lossTr 4.9494   accTr 25.9   9656s = 0.98s/step
Step 18001/20000   Activation magnitudes (2): 0.75 0.75 0.91 1.29 0.87 1.56 0.86 0.74
Step 18001/20000   Activation magnitudes (3): 9.27 5.66 8.02 12.56 9.69 12.46 12.99 8.12
Step 18201/20000   lossTr 4.9481   accTr 26.2   9864s = 1.04s/step
Step 18201/20000   Activation magnitudes (2): 0.76 0.75 0.91 1.31 0.87 1.52 0.89 0.76
Step 18201/20000   Activation magnitudes (3): 9.30 5.73 8.12 11.20 9.47 12.32 13.09 8.75
Step 18401/20000   lossTr 4.9680   accTr 26.1   10072s = 1.04s/step
Step 18401/20000   Activation magnitudes (2): 0.77 0.77 0.92 1.35 0.86 1.49 0.88 0.77
Step 18401/20000   Activation magnitudes (3): 9.35 5.77 8.14 10.63 9.60 12.22 12.82 8.45
Step 18501/20000   lossVa 4.9423   accVa 26.2
Step 18601/20000   lossTr 4.9212   accTr 26.4   10280s = 1.04s/step
Step 18601/20000   Activation magnitudes (2): 0.77 0.76 0.91 1.34 0.86 1.52 0.88 0.76
Step 18601/20000   Activation magnitudes (3): 9.36 5.65 8.08 11.34 9.54 12.30 13.06 7.96
Step 18801/20000   lossTr 4.9552   accTr 25.9   10486s = 1.03s/step
Step 18801/20000   Activation magnitudes (2): 0.76 0.76 0.91 1.34 0.86 1.56 0.87 0.77
Step 18801/20000   Activation magnitudes (3): 9.35 5.55 8.09 11.48 9.53 12.43 13.88 8.55
Step 19001/20000   lossVa 4.9327   accVa 26.3
Step 19001/20000   lossTr 4.9495   accTr 26.2   10694s = 1.04s/step
Step 19001/20000   Activation magnitudes (2): 0.76 0.75 0.91 1.32 0.87 1.61 0.87 0.77
Step 19001/20000   Activation magnitudes (3): 9.37 5.76 8.08 12.27 9.48 12.41 12.98 8.30
Step 19201/20000   lossTr 4.9522   accTr 26.1   10920s = 1.13s/step
Step 19201/20000   Activation magnitudes (2): 0.77 0.76 0.92 1.33 0.87 1.54 0.89 0.78
Step 19201/20000   Activation magnitudes (3): 9.38 5.63 7.99 12.15 9.39 12.76 13.07 8.13
Step 19401/20000   lossTr 4.9428   accTr 26.3   11134s = 1.07s/step
Step 19401/20000   Activation magnitudes (2): 0.77 0.75 0.91 1.35 0.86 1.57 0.88 0.78
Step 19401/20000   Activation magnitudes (3): 9.39 5.85 8.00 12.47 9.45 12.55 13.63 8.89
Step 19501/20000   lossVa 4.9162   accVa 26.4
Step 19601/20000   lossTr 4.9155   accTr 26.2   11350s = 1.08s/step
Step 19601/20000   Activation magnitudes (2): 0.77 0.75 0.91 1.33 0.87 1.55 0.89 0.78
Step 19601/20000   Activation magnitudes (3): 9.41 5.76 7.94 11.10 9.63 12.91 13.32 8.26
Step 19801/20000   lossTr 4.9118   accTr 26.4   11572s = 1.11s/step
Step 19801/20000   Activation magnitudes (2): 0.77 0.76 0.91 1.36 0.87 1.57 0.89 0.79
Step 19801/20000   Activation magnitudes (3): 9.41 5.53 7.99 11.62 9.73 12.35 13.54 8.34
Step 20000/20000   lossVa 4.9070   accVa 26.5
Step 20000/20000   Peak CUDA memory usage: 6.275 GB
Step 20000/20000   lossTr 4.8824   accTr 26.5   11793s = 1.11s/step
Step 20000/20000   Activation magnitudes (2): 0.77 0.75 0.92 1.33 0.87 1.63 0.88 0.78
Step 20000/20000   Activation magnitudes (3): 9.42 5.72 8.06 11.90 9.47 12.18 13.44 8.37
Saving AF:       ./results-fineweb10B\ptAf-optimizedMlpAtt-layerSpecific-phase1-step20000.pt
Saving AF (CSV): ./results-fineweb10B\ptAf-optimizedMlpAtt-layerSpecific-phase1-step20000.csv
Saving plot (loss/acc): ./results-fineweb10B\tr-optimizedMlpAtt-layerSpecific-phase1-step020000-00031632-00033550.png
Saving plot (AF): ./results-fineweb10B\af-optimizedMlpAtt-layerSpecific-phase1-step020000.png
Saving plot (act): ./results-fineweb10B\act-optimizedMlpAtt-layerSpecific-phase1-step020000.png
