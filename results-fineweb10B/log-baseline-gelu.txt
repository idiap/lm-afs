Hyperparameters:
  runName = "baseline-gelu"
  seed = 0
  plotMinAcc = 0.0
  plotMaxAcc = 0.4
  plotMinLoss = 3.0
  plotMaxLoss = 15.0
  shuffleTrData = 1
  tokenDocSep = 50256
  dtype = torch.float32
  flexBlockSize = 32
  nLayers = 4
  nHeads = 4
  dimEmb = 256
  tieEmbeddings = True
  afType = ['gelu', 'linear']
  afLayerSpecific = 1
  afRange = 15
  afNAnchors = 64
  afLr = 0
  afInit = 0.01
  afFileToLoad = ""
  dirResults = "./results-fineweb10B"
  filesTokensTr = "./data-fineweb10B/fineweb_train_*.bin"
  filesTokensVa = "./data-fineweb10B/fineweb_val_*.bin"
  filesTokensTe = ""
  lr = 0.0006
  freezeEmbeddings = 0
  batchSize = 1
  seqLength = 4096
  nSteps = 20000
  nStepsWarmup = 2000
  nStepsCooldown = 10000
  wtDecay = 0.0
  nModels = 1
  staggeredStarts = 2
  sameDataAcrossModels = False
  sameInitAcrossModels = False
  adamB1 = 0.8
  adamB2 = 0.95
  afAdamB1 = 0.99
  afAdamB2 = 0.999
  gradientClipping = 0
  contextSize = 1024
  lrSchedule = "trap"
  nTokensVa = 10485760
  valEvery = 500
  saveModelEvery = -1
  saveAfEvery = -1
  plotEvery = 200
  plottingLevel = 2
====================================================================================================
pytorch 2.10.0.dev20251001+cu126, CUDA 12.6
Thu Feb 12 21:35:38 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 566.07                 Driver Version: 566.07         CUDA Version: 12.7     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 4090 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |
| N/A   52C    P3             23W /  101W |     236MiB /  16376MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     20600      C   ...nvs\modded-nanogpt-py312\python.exe      N/A      |
+-----------------------------------------------------------------------------------------+

Tr data: 899,999,680 tokens, 9 files (./data-fineweb10B/fineweb_train_*.bin)
Va data: 100,000,000 tokens, 1 files (./data-fineweb10B/fineweb_val_*.bin)
Number of validation batches: 2,560 (10,485,760 tokens, 4,096 tokens/sequence)
====================================================================================================
Projected number of training tokens:
  20,000 steps * 1 batchSize * 4,096 seqLength
  = 81,920,000 tokens
  = 0.09 epochs
Vocabulary size: 50,304
====================================================================================================
Number of model parameters (frozen): 0
Number of model parameters (trainable): 16,026,134
  _orig_mod.skipWeights                              2
  _orig_mod.transformer.wte.weight                   12,877,824
  _orig_mod.transformer.h.0.blockLambdas             2
  _orig_mod.transformer.h.0.attn.lambdas             1
  _orig_mod.transformer.h.0.attn.projAtt.weight      1
  _orig_mod.transformer.h.0.attn.projAtt.bias        1
  _orig_mod.transformer.h.0.attn.projQ.weight        65,536
  _orig_mod.transformer.h.0.attn.projK.weight        65,536
  _orig_mod.transformer.h.0.attn.projV.weight        65,536
  _orig_mod.transformer.h.0.attn.projOut.weight      65,536
  _orig_mod.transformer.h.0.mlp.fc.weight            262,144
  _orig_mod.transformer.h.0.mlp.proj.weight          262,144
  _orig_mod.transformer.h.0.normAtt.weight           256
  _orig_mod.transformer.h.0.normMlp.weight           256
  _orig_mod.transformer.h.1.blockLambdas             2
  _orig_mod.transformer.h.1.attn.lambdas             1
  _orig_mod.transformer.h.1.attn.projAtt.weight      1
  _orig_mod.transformer.h.1.attn.projAtt.bias        1
  _orig_mod.transformer.h.1.attn.projQ.weight        65,536
  _orig_mod.transformer.h.1.attn.projK.weight        65,536
  _orig_mod.transformer.h.1.attn.projV.weight        65,536
  _orig_mod.transformer.h.1.attn.projOut.weight      65,536
  _orig_mod.transformer.h.1.mlp.fc.weight            262,144
  _orig_mod.transformer.h.1.mlp.proj.weight          262,144
  _orig_mod.transformer.h.1.normAtt.weight           256
  _orig_mod.transformer.h.1.normMlp.weight           256
  _orig_mod.transformer.h.2.blockLambdas             2
  _orig_mod.transformer.h.2.attn.lambdas             1
  _orig_mod.transformer.h.2.attn.projAtt.weight      1
  _orig_mod.transformer.h.2.attn.projAtt.bias        1
  _orig_mod.transformer.h.2.attn.projQ.weight        65,536
  _orig_mod.transformer.h.2.attn.projK.weight        65,536
  _orig_mod.transformer.h.2.attn.projV.weight        65,536
  _orig_mod.transformer.h.2.attn.projOut.weight      65,536
  _orig_mod.transformer.h.2.mlp.fc.weight            262,144
  _orig_mod.transformer.h.2.mlp.proj.weight          262,144
  _orig_mod.transformer.h.2.normAtt.weight           256
  _orig_mod.transformer.h.2.normMlp.weight           256
  _orig_mod.transformer.h.3.blockLambdas             2
  _orig_mod.transformer.h.3.attn.lambdas             1
  _orig_mod.transformer.h.3.attn.projAtt.weight      1
  _orig_mod.transformer.h.3.attn.projAtt.bias        1
  _orig_mod.transformer.h.3.attn.projQ.weight        65,536
  _orig_mod.transformer.h.3.attn.projK.weight        65,536
  _orig_mod.transformer.h.3.attn.projV.weight        65,536
  _orig_mod.transformer.h.3.attn.projOut.weight      65,536
  _orig_mod.transformer.h.3.mlp.fc.weight            262,144
  _orig_mod.transformer.h.3.mlp.proj.weight          262,144
  _orig_mod.transformer.h.3.normAtt.weight           256
  _orig_mod.transformer.h.3.normMlp.weight           256
  _orig_mod.normWte.weight                           256
  _orig_mod.normWte.bias                             256
5.1 tokens/parameter
====================================================================================================
Optimizer #0/0: Adam
  Param group 0: lr=0.0006, weight_decay=0.0
  _orig_mod.skipWeights
  _orig_mod.transformer.wte.weight
  _orig_mod.transformer.h.0.blockLambdas
  _orig_mod.transformer.h.0.attn.lambdas
  _orig_mod.transformer.h.0.attn.projAtt.weight
  _orig_mod.transformer.h.0.attn.projAtt.bias
  _orig_mod.transformer.h.0.attn.projQ.weight
  _orig_mod.transformer.h.0.attn.projK.weight
  _orig_mod.transformer.h.0.attn.projV.weight
  _orig_mod.transformer.h.0.attn.projOut.weight
  _orig_mod.transformer.h.0.mlp.fc.weight
  _orig_mod.transformer.h.0.mlp.proj.weight
  _orig_mod.transformer.h.0.normAtt.weight
  _orig_mod.transformer.h.0.normMlp.weight
  _orig_mod.transformer.h.1.blockLambdas
  _orig_mod.transformer.h.1.attn.lambdas
  _orig_mod.transformer.h.1.attn.projAtt.weight
  _orig_mod.transformer.h.1.attn.projAtt.bias
  _orig_mod.transformer.h.1.attn.projQ.weight
  _orig_mod.transformer.h.1.attn.projK.weight
  _orig_mod.transformer.h.1.attn.projV.weight
  _orig_mod.transformer.h.1.attn.projOut.weight
  _orig_mod.transformer.h.1.mlp.fc.weight
  _orig_mod.transformer.h.1.mlp.proj.weight
  _orig_mod.transformer.h.1.normAtt.weight
  _orig_mod.transformer.h.1.normMlp.weight
  _orig_mod.transformer.h.2.blockLambdas
  _orig_mod.transformer.h.2.attn.lambdas
  _orig_mod.transformer.h.2.attn.projAtt.weight
  _orig_mod.transformer.h.2.attn.projAtt.bias
  _orig_mod.transformer.h.2.attn.projQ.weight
  _orig_mod.transformer.h.2.attn.projK.weight
  _orig_mod.transformer.h.2.attn.projV.weight
  _orig_mod.transformer.h.2.attn.projOut.weight
  _orig_mod.transformer.h.2.mlp.fc.weight
  _orig_mod.transformer.h.2.mlp.proj.weight
  _orig_mod.transformer.h.2.normAtt.weight
  _orig_mod.transformer.h.2.normMlp.weight
  _orig_mod.transformer.h.3.blockLambdas
  _orig_mod.transformer.h.3.attn.lambdas
  _orig_mod.transformer.h.3.attn.projAtt.weight
  _orig_mod.transformer.h.3.attn.projAtt.bias
  _orig_mod.transformer.h.3.attn.projQ.weight
  _orig_mod.transformer.h.3.attn.projK.weight
  _orig_mod.transformer.h.3.attn.projV.weight
  _orig_mod.transformer.h.3.attn.projOut.weight
  _orig_mod.transformer.h.3.mlp.fc.weight
  _orig_mod.transformer.h.3.mlp.proj.weight
  _orig_mod.transformer.h.3.normAtt.weight
  _orig_mod.transformer.h.3.normMlp.weight
  _orig_mod.normWte.weight
  _orig_mod.normWte.bias
====================================================================================================
Hyperparameters:
  runName = "baseline-gelu"
  seed = 0
  plotMinAcc = 0.0
  plotMaxAcc = 0.4
  plotMinLoss = 3.0
  plotMaxLoss = 15.0
  shuffleTrData = 1
  tokenDocSep = 50256
  dtype = torch.float32
  flexBlockSize = 32
  nLayers = 4
  nHeads = 4
  dimEmb = 256
  tieEmbeddings = True
  afType = ['gelu', 'linear']
  afLayerSpecific = False
  afRange = 15
  afNAnchors = 64
  afLr = 0
  afInit = 0.01
  afFileToLoad = ""
  dirResults = "./results-fineweb10B"
  filesTokensTr = "./data-fineweb10B/fineweb_train_*.bin"
  filesTokensVa = "./data-fineweb10B/fineweb_val_*.bin"
  filesTokensTe = ""
  lr = 0.0006
  freezeEmbeddings = 0
  batchSize = 1
  seqLength = 4096
  nSteps = 20000
  nStepsWarmup = 2000
  nStepsCooldown = 10000
  wtDecay = 0.0
  nModels = 1
  staggeredStarts = 2
  sameDataAcrossModels = False
  sameInitAcrossModels = False
  adamB1 = 0.8
  adamB2 = 0.95
  afAdamB1 = 0.99
  afAdamB2 = 0.999
  gradientClipping = 0
  contextSize = 1024
  lrSchedule = "trap"
  nTokensVa = 10485760
  valEvery = 500
  saveModelEvery = -1
  saveAfEvery = -1
  plotEvery = 200
  plottingLevel = 2
====================================================================================================
pytorch 2.10.0.dev20251001+cu126, CUDA 12.6
Thu Feb 12 21:37:13 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 566.07                 Driver Version: 566.07         CUDA Version: 12.7     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 4090 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |
| N/A   53C    P3             24W /  102W |     236MiB /  16376MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     11084      C   ...nvs\modded-nanogpt-py312\python.exe      N/A      |
+-----------------------------------------------------------------------------------------+

Tr data: 899,999,680 tokens, 9 files (./data-fineweb10B/fineweb_train_*.bin)
Va data: 100,000,000 tokens, 1 files (./data-fineweb10B/fineweb_val_*.bin)
Number of validation batches: 2,560 (10,485,760 tokens, 4,096 tokens/sequence)
Hyperparameters:
  runName = "baseline-gelu"
  seed = 0
  plotMinAcc = 0.0
  plotMaxAcc = 0.4
  plotMinLoss = 3.0
  plotMaxLoss = 15.0
  shuffleTrData = 1
  tokenDocSep = 50256
  dtype = torch.float32
  flexBlockSize = 32
  nLayers = 4
  nHeads = 4
  dimEmb = 256
  tieEmbeddings = True
  afType = ['gelu', 'linear']
  afLayerSpecific = 0
  afRange = 15
  afNAnchors = 64
  afLr = 0
  afInit = 0.01
  afFileToLoad = ""
  dirResults = "./results-fineweb10B"
  filesTokensTr = "./data-fineweb10B/fineweb_train_*.bin"
  filesTokensVa = "./data-fineweb10B/fineweb_val_*.bin"
  filesTokensTe = ""
  lr = 0.0006
  freezeEmbeddings = 0
  batchSize = 1
  seqLength = 4096
  nSteps = 20000
  nStepsWarmup = 2000
  nStepsCooldown = 10000
  wtDecay = 0.0
  nModels = 1
  staggeredStarts = 2
  sameDataAcrossModels = False
  sameInitAcrossModels = False
  adamB1 = 0.8
  adamB2 = 0.95
  afAdamB1 = 0.99
  afAdamB2 = 0.999
  gradientClipping = 0
  contextSize = 1024
  lrSchedule = "trap"
  nTokensVa = 10485760
  valEvery = 500
  saveModelEvery = -1
  saveAfEvery = -1
  plotEvery = 200
  plottingLevel = 2
====================================================================================================
pytorch 2.10.0.dev20251001+cu126, CUDA 12.6
Thu Feb 12 21:37:28 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 566.07                 Driver Version: 566.07         CUDA Version: 12.7     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 4090 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |
| N/A   55C    P3             24W /  101W |     236MiB /  16376MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     31196      C   ...nvs\modded-nanogpt-py312\python.exe      N/A      |
+-----------------------------------------------------------------------------------------+

Tr data: 899,999,680 tokens, 9 files (./data-fineweb10B/fineweb_train_*.bin)
Va data: 100,000,000 tokens, 1 files (./data-fineweb10B/fineweb_val_*.bin)
Number of validation batches: 2,560 (10,485,760 tokens, 4,096 tokens/sequence)
====================================================================================================
Projected number of training tokens:
  20,000 steps * 1 batchSize * 4,096 seqLength
  = 81,920,000 tokens
  = 0.09 epochs
Vocabulary size: 50,304
====================================================================================================
Number of model parameters (frozen): 0
Number of model parameters (trainable): 16,026,134
  _orig_mod.skipWeights                              2
  _orig_mod.transformer.wte.weight                   12,877,824
  _orig_mod.transformer.h.0.blockLambdas             2
  _orig_mod.transformer.h.0.attn.lambdas             1
  _orig_mod.transformer.h.0.attn.projAtt.weight      1
  _orig_mod.transformer.h.0.attn.projAtt.bias        1
  _orig_mod.transformer.h.0.attn.projQ.weight        65,536
  _orig_mod.transformer.h.0.attn.projK.weight        65,536
  _orig_mod.transformer.h.0.attn.projV.weight        65,536
  _orig_mod.transformer.h.0.attn.projOut.weight      65,536
  _orig_mod.transformer.h.0.mlp.fc.weight            262,144
  _orig_mod.transformer.h.0.mlp.proj.weight          262,144
  _orig_mod.transformer.h.0.normAtt.weight           256
  _orig_mod.transformer.h.0.normMlp.weight           256
  _orig_mod.transformer.h.1.blockLambdas             2
  _orig_mod.transformer.h.1.attn.lambdas             1
  _orig_mod.transformer.h.1.attn.projAtt.weight      1
  _orig_mod.transformer.h.1.attn.projAtt.bias        1
  _orig_mod.transformer.h.1.attn.projQ.weight        65,536
  _orig_mod.transformer.h.1.attn.projK.weight        65,536
  _orig_mod.transformer.h.1.attn.projV.weight        65,536
  _orig_mod.transformer.h.1.attn.projOut.weight      65,536
  _orig_mod.transformer.h.1.mlp.fc.weight            262,144
  _orig_mod.transformer.h.1.mlp.proj.weight          262,144
  _orig_mod.transformer.h.1.normAtt.weight           256
  _orig_mod.transformer.h.1.normMlp.weight           256
  _orig_mod.transformer.h.2.blockLambdas             2
  _orig_mod.transformer.h.2.attn.lambdas             1
  _orig_mod.transformer.h.2.attn.projAtt.weight      1
  _orig_mod.transformer.h.2.attn.projAtt.bias        1
  _orig_mod.transformer.h.2.attn.projQ.weight        65,536
  _orig_mod.transformer.h.2.attn.projK.weight        65,536
  _orig_mod.transformer.h.2.attn.projV.weight        65,536
  _orig_mod.transformer.h.2.attn.projOut.weight      65,536
  _orig_mod.transformer.h.2.mlp.fc.weight            262,144
  _orig_mod.transformer.h.2.mlp.proj.weight          262,144
  _orig_mod.transformer.h.2.normAtt.weight           256
  _orig_mod.transformer.h.2.normMlp.weight           256
  _orig_mod.transformer.h.3.blockLambdas             2
  _orig_mod.transformer.h.3.attn.lambdas             1
  _orig_mod.transformer.h.3.attn.projAtt.weight      1
  _orig_mod.transformer.h.3.attn.projAtt.bias        1
  _orig_mod.transformer.h.3.attn.projQ.weight        65,536
  _orig_mod.transformer.h.3.attn.projK.weight        65,536
  _orig_mod.transformer.h.3.attn.projV.weight        65,536
  _orig_mod.transformer.h.3.attn.projOut.weight      65,536
  _orig_mod.transformer.h.3.mlp.fc.weight            262,144
  _orig_mod.transformer.h.3.mlp.proj.weight          262,144
  _orig_mod.transformer.h.3.normAtt.weight           256
  _orig_mod.transformer.h.3.normMlp.weight           256
  _orig_mod.normWte.weight                           256
  _orig_mod.normWte.bias                             256
5.1 tokens/parameter
====================================================================================================
Optimizer #0/0: Adam
  Param group 0: lr=0.0006, weight_decay=0.0
  _orig_mod.skipWeights
  _orig_mod.transformer.wte.weight
  _orig_mod.transformer.h.0.blockLambdas
  _orig_mod.transformer.h.0.attn.lambdas
  _orig_mod.transformer.h.0.attn.projAtt.weight
  _orig_mod.transformer.h.0.attn.projAtt.bias
  _orig_mod.transformer.h.0.attn.projQ.weight
  _orig_mod.transformer.h.0.attn.projK.weight
  _orig_mod.transformer.h.0.attn.projV.weight
  _orig_mod.transformer.h.0.attn.projOut.weight
  _orig_mod.transformer.h.0.mlp.fc.weight
  _orig_mod.transformer.h.0.mlp.proj.weight
  _orig_mod.transformer.h.0.normAtt.weight
  _orig_mod.transformer.h.0.normMlp.weight
  _orig_mod.transformer.h.1.blockLambdas
  _orig_mod.transformer.h.1.attn.lambdas
  _orig_mod.transformer.h.1.attn.projAtt.weight
  _orig_mod.transformer.h.1.attn.projAtt.bias
  _orig_mod.transformer.h.1.attn.projQ.weight
  _orig_mod.transformer.h.1.attn.projK.weight
  _orig_mod.transformer.h.1.attn.projV.weight
  _orig_mod.transformer.h.1.attn.projOut.weight
  _orig_mod.transformer.h.1.mlp.fc.weight
  _orig_mod.transformer.h.1.mlp.proj.weight
  _orig_mod.transformer.h.1.normAtt.weight
  _orig_mod.transformer.h.1.normMlp.weight
  _orig_mod.transformer.h.2.blockLambdas
  _orig_mod.transformer.h.2.attn.lambdas
  _orig_mod.transformer.h.2.attn.projAtt.weight
  _orig_mod.transformer.h.2.attn.projAtt.bias
  _orig_mod.transformer.h.2.attn.projQ.weight
  _orig_mod.transformer.h.2.attn.projK.weight
  _orig_mod.transformer.h.2.attn.projV.weight
  _orig_mod.transformer.h.2.attn.projOut.weight
  _orig_mod.transformer.h.2.mlp.fc.weight
  _orig_mod.transformer.h.2.mlp.proj.weight
  _orig_mod.transformer.h.2.normAtt.weight
  _orig_mod.transformer.h.2.normMlp.weight
  _orig_mod.transformer.h.3.blockLambdas
  _orig_mod.transformer.h.3.attn.lambdas
  _orig_mod.transformer.h.3.attn.projAtt.weight
  _orig_mod.transformer.h.3.attn.projAtt.bias
  _orig_mod.transformer.h.3.attn.projQ.weight
  _orig_mod.transformer.h.3.attn.projK.weight
  _orig_mod.transformer.h.3.attn.projV.weight
  _orig_mod.transformer.h.3.attn.projOut.weight
  _orig_mod.transformer.h.3.mlp.fc.weight
  _orig_mod.transformer.h.3.mlp.proj.weight
  _orig_mod.transformer.h.3.normAtt.weight
  _orig_mod.transformer.h.3.normMlp.weight
  _orig_mod.normWte.weight
  _orig_mod.normWte.bias
====================================================================================================
Hyperparameters:
  runName = "baseline-gelu"
  seed = 0
  plotMinAcc = 0.0
  plotMaxAcc = 0.4
  plotMinLoss = 3.0
  plotMaxLoss = 15.0
  shuffleTrData = 1
  tokenDocSep = 50256
  dtype = torch.float32
  flexBlockSize = 32
  nLayers = 4
  nHeads = 4
  dimEmb = 256
  tieEmbeddings = True
  afType = ['gelu', 'linear']
  afLayerSpecific = 0
  afRange = 15
  afNAnchors = 64
  afLr = 0
  afInit = 0.01
  afFileToLoad = ""
  dirResults = "./results-fineweb10B"
  filesTokensTr = "./data-fineweb10B/fineweb_train_*.bin"
  filesTokensVa = "./data-fineweb10B/fineweb_val_*.bin"
  filesTokensTe = ""
  lr = 0.0006
  freezeEmbeddings = 0
  batchSize = 1
  seqLength = 4096
  nSteps = 20000
  nStepsWarmup = 2000
  nStepsCooldown = 10000
  wtDecay = 0.0
  nModels = 1
  staggeredStarts = 2
  sameDataAcrossModels = False
  sameInitAcrossModels = False
  adamB1 = 0.8
  adamB2 = 0.95
  afAdamB1 = 0.99
  afAdamB2 = 0.999
  gradientClipping = 0
  contextSize = 1024
  lrSchedule = "trap"
  nTokensVa = 10485760
  valEvery = 500
  saveModelEvery = -1
  saveAfEvery = -1
  plotEvery = 200
  plottingLevel = 2
====================================================================================================
pytorch 2.10.0.dev20251001+cu126, CUDA 12.6
Thu Feb 12 22:34:34 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 566.07                 Driver Version: 566.07         CUDA Version: 12.7     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 4090 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |
| N/A   55C    P3             25W /  102W |     471MiB /  16376MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A     22672      C   ...nvs\modded-nanogpt-py312\python.exe      N/A      |
+-----------------------------------------------------------------------------------------+

Tr data: 899,999,680 tokens, 9 files (./data-fineweb10B/fineweb_train_*.bin)
Va data: 100,000,000 tokens, 1 files (./data-fineweb10B/fineweb_val_*.bin)
Number of validation batches: 2,560 (10,485,760 tokens, 4,096 tokens/sequence)
====================================================================================================
Projected number of training tokens:
  20,000 steps * 1 batchSize * 4,096 seqLength
  = 81,920,000 tokens
  = 0.09 epochs
Vocabulary size: 50,304
====================================================================================================
Number of model parameters (frozen): 0
Number of model parameters (trainable): 16,026,134
  _orig_mod.skipWeights                              2
  _orig_mod.transformer.wte.weight                   12,877,824
  _orig_mod.transformer.h.0.blockLambdas             2
  _orig_mod.transformer.h.0.attn.lambdas             1
  _orig_mod.transformer.h.0.attn.projAtt.weight      1
  _orig_mod.transformer.h.0.attn.projAtt.bias        1
  _orig_mod.transformer.h.0.attn.projQ.weight        65,536
  _orig_mod.transformer.h.0.attn.projK.weight        65,536
  _orig_mod.transformer.h.0.attn.projV.weight        65,536
  _orig_mod.transformer.h.0.attn.projOut.weight      65,536
  _orig_mod.transformer.h.0.mlp.fc.weight            262,144
  _orig_mod.transformer.h.0.mlp.proj.weight          262,144
  _orig_mod.transformer.h.0.normAtt.weight           256
  _orig_mod.transformer.h.0.normMlp.weight           256
  _orig_mod.transformer.h.1.blockLambdas             2
  _orig_mod.transformer.h.1.attn.lambdas             1
  _orig_mod.transformer.h.1.attn.projAtt.weight      1
  _orig_mod.transformer.h.1.attn.projAtt.bias        1
  _orig_mod.transformer.h.1.attn.projQ.weight        65,536
  _orig_mod.transformer.h.1.attn.projK.weight        65,536
  _orig_mod.transformer.h.1.attn.projV.weight        65,536
  _orig_mod.transformer.h.1.attn.projOut.weight      65,536
  _orig_mod.transformer.h.1.mlp.fc.weight            262,144
  _orig_mod.transformer.h.1.mlp.proj.weight          262,144
  _orig_mod.transformer.h.1.normAtt.weight           256
  _orig_mod.transformer.h.1.normMlp.weight           256
  _orig_mod.transformer.h.2.blockLambdas             2
  _orig_mod.transformer.h.2.attn.lambdas             1
  _orig_mod.transformer.h.2.attn.projAtt.weight      1
  _orig_mod.transformer.h.2.attn.projAtt.bias        1
  _orig_mod.transformer.h.2.attn.projQ.weight        65,536
  _orig_mod.transformer.h.2.attn.projK.weight        65,536
  _orig_mod.transformer.h.2.attn.projV.weight        65,536
  _orig_mod.transformer.h.2.attn.projOut.weight      65,536
  _orig_mod.transformer.h.2.mlp.fc.weight            262,144
  _orig_mod.transformer.h.2.mlp.proj.weight          262,144
  _orig_mod.transformer.h.2.normAtt.weight           256
  _orig_mod.transformer.h.2.normMlp.weight           256
  _orig_mod.transformer.h.3.blockLambdas             2
  _orig_mod.transformer.h.3.attn.lambdas             1
  _orig_mod.transformer.h.3.attn.projAtt.weight      1
  _orig_mod.transformer.h.3.attn.projAtt.bias        1
  _orig_mod.transformer.h.3.attn.projQ.weight        65,536
  _orig_mod.transformer.h.3.attn.projK.weight        65,536
  _orig_mod.transformer.h.3.attn.projV.weight        65,536
  _orig_mod.transformer.h.3.attn.projOut.weight      65,536
  _orig_mod.transformer.h.3.mlp.fc.weight            262,144
  _orig_mod.transformer.h.3.mlp.proj.weight          262,144
  _orig_mod.transformer.h.3.normAtt.weight           256
  _orig_mod.transformer.h.3.normMlp.weight           256
  _orig_mod.normWte.weight                           256
  _orig_mod.normWte.bias                             256
5.1 tokens/parameter
====================================================================================================
Optimizer #0/0: Adam
  Param group 0: lr=0.0006, weight_decay=0.0
  _orig_mod.skipWeights
  _orig_mod.transformer.wte.weight
  _orig_mod.transformer.h.0.blockLambdas
  _orig_mod.transformer.h.0.attn.lambdas
  _orig_mod.transformer.h.0.attn.projAtt.weight
  _orig_mod.transformer.h.0.attn.projAtt.bias
  _orig_mod.transformer.h.0.attn.projQ.weight
  _orig_mod.transformer.h.0.attn.projK.weight
  _orig_mod.transformer.h.0.attn.projV.weight
  _orig_mod.transformer.h.0.attn.projOut.weight
  _orig_mod.transformer.h.0.mlp.fc.weight
  _orig_mod.transformer.h.0.mlp.proj.weight
  _orig_mod.transformer.h.0.normAtt.weight
  _orig_mod.transformer.h.0.normMlp.weight
  _orig_mod.transformer.h.1.blockLambdas
  _orig_mod.transformer.h.1.attn.lambdas
  _orig_mod.transformer.h.1.attn.projAtt.weight
  _orig_mod.transformer.h.1.attn.projAtt.bias
  _orig_mod.transformer.h.1.attn.projQ.weight
  _orig_mod.transformer.h.1.attn.projK.weight
  _orig_mod.transformer.h.1.attn.projV.weight
  _orig_mod.transformer.h.1.attn.projOut.weight
  _orig_mod.transformer.h.1.mlp.fc.weight
  _orig_mod.transformer.h.1.mlp.proj.weight
  _orig_mod.transformer.h.1.normAtt.weight
  _orig_mod.transformer.h.1.normMlp.weight
  _orig_mod.transformer.h.2.blockLambdas
  _orig_mod.transformer.h.2.attn.lambdas
  _orig_mod.transformer.h.2.attn.projAtt.weight
  _orig_mod.transformer.h.2.attn.projAtt.bias
  _orig_mod.transformer.h.2.attn.projQ.weight
  _orig_mod.transformer.h.2.attn.projK.weight
  _orig_mod.transformer.h.2.attn.projV.weight
  _orig_mod.transformer.h.2.attn.projOut.weight
  _orig_mod.transformer.h.2.mlp.fc.weight
  _orig_mod.transformer.h.2.mlp.proj.weight
  _orig_mod.transformer.h.2.normAtt.weight
  _orig_mod.transformer.h.2.normMlp.weight
  _orig_mod.transformer.h.3.blockLambdas
  _orig_mod.transformer.h.3.attn.lambdas
  _orig_mod.transformer.h.3.attn.projAtt.weight
  _orig_mod.transformer.h.3.attn.projAtt.bias
  _orig_mod.transformer.h.3.attn.projQ.weight
  _orig_mod.transformer.h.3.attn.projK.weight
  _orig_mod.transformer.h.3.attn.projV.weight
  _orig_mod.transformer.h.3.attn.projOut.weight
  _orig_mod.transformer.h.3.mlp.fc.weight
  _orig_mod.transformer.h.3.mlp.proj.weight
  _orig_mod.transformer.h.3.normAtt.weight
  _orig_mod.transformer.h.3.normMlp.weight
  _orig_mod.normWte.weight
  _orig_mod.normWte.bias
====================================================================================================
Step 1/20000   lossVa 15.2586   accVa 0.0
Step 1/20000   lossTr 15.3035   accTr 0.0   17s = 16.84s/step
Step 1/20000   Activation magnitudes (2): 0.80 0.25 0.80 0.26 0.80 0.25 0.80 0.26
Step 1/20000   Activation magnitudes (3): 4.77 1.64 4.77 1.65 4.77 1.59 4.77 1.69
Step 6/20000   Peak CUDA memory usage: 1.360 GB
Step 6/20000   lossTr 15.2699   accTr 0.0   28s = 2.17s/step
Step 6/20000   Activation magnitudes (2): 0.80 0.25 0.80 0.26 0.80 0.25 0.80 0.26
Step 6/20000   Activation magnitudes (3): 4.63 1.60 4.63 1.57 4.63 1.56 4.63 1.69
Step 11/20000   Peak CUDA memory usage: 1.302 GB
Step 201/20000   lossTr 13.4334   accTr 0.4   34s = 0.03s/step
Step 201/20000   Activation magnitudes (2): 0.80 0.33 0.81 0.36 0.82 0.49 0.82 0.51
Step 201/20000   Activation magnitudes (3): 4.65 2.28 4.69 2.14 4.71 2.04 4.65 2.01
Step 401/20000   lossTr 8.1116   accTr 4.9   40s = 0.03s/step
Step 401/20000   Activation magnitudes (2): 0.82 0.46 0.81 0.64 0.81 0.62 0.81 0.77
Step 401/20000   Activation magnitudes (3): 5.40 3.12 5.22 2.81 4.99 2.44 4.82 2.29
Step 501/20000   lossVa 7.1282   accVa 10.6
Step 601/20000   lossTr 7.1430   accTr 10.3   47s = 0.03s/step
Step 601/20000   Activation magnitudes (2): 0.82 0.46 0.81 0.49 0.81 0.55 0.81 0.49
Step 601/20000   Activation magnitudes (3): 4.85 3.56 5.15 2.69 5.02 2.57 4.92 2.54
Step 801/20000   lossTr 6.8377   accTr 12.5   53s = 0.03s/step
Step 801/20000   Activation magnitudes (2): 0.82 0.48 0.81 0.44 0.81 0.51 0.82 0.53
Step 801/20000   Activation magnitudes (3): 4.99 4.03 4.88 2.56 4.58 2.87 4.88 3.01
Step 1001/20000   lossVa 6.6425   accVa 13.8
Step 1001/20000   lossTr 6.6988   accTr 13.5   59s = 0.03s/step
Step 1001/20000   Activation magnitudes (2): 0.83 0.47 0.81 0.38 0.81 0.43 0.80 0.44
Step 1001/20000   Activation magnitudes (3): 5.03 3.96 5.40 2.68 5.00 3.06 5.08 3.25
Step 1201/20000   lossTr 6.5505   accTr 14.3   66s = 0.03s/step
Step 1201/20000   Activation magnitudes (2): 0.83 0.48 0.81 0.35 0.80 0.42 0.79 0.48
Step 1201/20000   Activation magnitudes (3): 4.90 3.96 5.55 3.08 6.50 3.32 6.69 3.94
Step 1401/20000   lossTr 6.4301   accTr 14.9   72s = 0.03s/step
Step 1401/20000   Activation magnitudes (2): 0.84 0.48 0.81 0.36 0.78 0.43 0.78 0.46
Step 1401/20000   Activation magnitudes (3): 4.96 3.80 6.64 3.05 7.79 3.52 7.94 4.44
Step 1501/20000   lossVa 6.3928   accVa 15.2
Step 1601/20000   lossTr 6.3566   accTr 15.4   79s = 0.03s/step
Step 1601/20000   Activation magnitudes (2): 0.85 0.49 0.81 0.39 0.78 0.44 0.78 0.47
Step 1601/20000   Activation magnitudes (3): 5.08 3.72 8.10 3.65 8.99 3.61 9.32 4.74
Step 1801/20000   lossTr 6.2678   accTr 15.7   85s = 0.03s/step
Step 1801/20000   Activation magnitudes (2): 0.86 0.50 0.80 0.43 0.75 0.48 0.75 0.45
Step 1801/20000   Activation magnitudes (3): 4.90 4.01 8.71 3.93 9.21 3.82 9.50 4.64
Step 2001/20000   lossVa 6.2032   accVa 16.0
Step 2001/20000   lossTr 6.2135   accTr 16.0   91s = 0.03s/step
Step 2001/20000   Activation magnitudes (2): 0.86 0.50 0.80 0.48 0.73 0.51 0.74 0.46
Step 2001/20000   Activation magnitudes (3): 5.13 3.82 8.17 4.07 8.90 4.06 9.75 4.44
Step 2201/20000   lossTr 6.1925   accTr 16.1   98s = 0.03s/step
Step 2201/20000   Activation magnitudes (2): 0.87 0.53 0.80 0.55 0.74 0.59 0.75 0.48
Step 2201/20000   Activation magnitudes (3): 5.91 4.15 7.22 4.65 9.49 4.64 9.53 4.39
Step 2401/20000   lossTr 6.1135   accTr 16.6   104s = 0.03s/step
Step 2401/20000   Activation magnitudes (2): 0.87 0.55 0.79 0.58 0.73 0.64 0.73 0.50
Step 2401/20000   Activation magnitudes (3): 6.39 4.13 7.20 5.19 9.57 4.78 10.19 4.62
Step 2501/20000   lossVa 6.0159   accVa 17.3
Step 2601/20000   lossTr 6.0297   accTr 17.1   110s = 0.03s/step
Step 2601/20000   Activation magnitudes (2): 0.88 0.56 0.79 0.65 0.75 0.70 0.74 0.52
Step 2601/20000   Activation magnitudes (3): 5.84 4.34 7.20 5.17 10.44 5.27 11.75 4.45
Step 2801/20000   lossTr 6.0001   accTr 17.5   116s = 0.03s/step
Step 2801/20000   Activation magnitudes (2): 0.89 0.59 0.78 0.70 0.75 0.73 0.74 0.54
Step 2801/20000   Activation magnitudes (3): 6.36 4.01 7.79 6.02 10.85 5.71 12.23 4.80
Step 3001/20000   lossVa 5.9372   accVa 17.7
Step 3001/20000   lossTr 5.9450   accTr 17.5   123s = 0.03s/step
Step 3001/20000   Activation magnitudes (2): 0.89 0.60 0.78 0.75 0.75 0.77 0.72 0.53
Step 3001/20000   Activation magnitudes (3): 6.54 4.37 8.03 5.26 11.57 5.63 12.96 5.16
Step 3201/20000   lossTr 5.8429   accTr 18.4   130s = 0.03s/step
Step 3201/20000   Activation magnitudes (2): 0.90 0.62 0.77 0.78 0.76 0.85 0.74 0.58
Step 3201/20000   Activation magnitudes (3): 6.56 4.55 8.82 5.73 11.61 5.88 13.55 5.11
Step 3401/20000   lossTr 5.8561   accTr 18.1   136s = 0.03s/step
Step 3401/20000   Activation magnitudes (2): 0.89 0.64 0.77 0.83 0.77 0.88 0.73 0.57
Step 3401/20000   Activation magnitudes (3): 6.81 4.72 9.13 6.28 11.23 6.18 13.10 5.03
Step 3501/20000   lossVa 5.8321   accVa 18.3
Step 3601/20000   lossTr 5.8411   accTr 18.2   142s = 0.03s/step
Step 3601/20000   Activation magnitudes (2): 0.90 0.64 0.77 0.89 0.78 0.93 0.74 0.59
Step 3601/20000   Activation magnitudes (3): 6.95 4.79 9.11 6.32 11.99 6.28 13.44 5.15
Step 3801/20000   lossTr 5.8115   accTr 18.5   149s = 0.03s/step
Step 3801/20000   Activation magnitudes (2): 0.90 0.68 0.77 0.91 0.79 0.96 0.74 0.59
Step 3801/20000   Activation magnitudes (3): 7.16 4.91 8.74 6.74 11.87 6.59 12.24 5.50
Step 4001/20000   lossVa 5.7644   accVa 18.8
Step 4001/20000   lossTr 5.7491   accTr 18.9   155s = 0.03s/step
Step 4001/20000   Activation magnitudes (2): 0.90 0.70 0.77 0.97 0.80 0.99 0.73 0.59
Step 4001/20000   Activation magnitudes (3): 7.43 4.93 9.24 6.37 12.39 7.42 12.29 4.97
Step 4201/20000   lossTr 5.7435   accTr 18.9   161s = 0.03s/step
Step 4201/20000   Activation magnitudes (2): 0.91 0.72 0.77 1.00 0.81 1.04 0.74 0.62
Step 4201/20000   Activation magnitudes (3): 7.54 4.81 9.11 6.25 12.43 7.25 12.06 4.95
Step 4401/20000   lossTr 5.7361   accTr 19.1   168s = 0.03s/step
Step 4401/20000   Activation magnitudes (2): 0.92 0.73 0.77 1.05 0.83 1.09 0.75 0.65
Step 4401/20000   Activation magnitudes (3): 7.59 4.76 9.66 6.55 12.65 7.49 12.46 5.08
Step 4501/20000   lossVa 5.7159   accVa 19.1
Step 4601/20000   lossTr 5.6955   accTr 19.1   174s = 0.03s/step
Step 4601/20000   Activation magnitudes (2): 0.93 0.75 0.76 1.06 0.85 1.10 0.75 0.63
Step 4601/20000   Activation magnitudes (3): 7.62 4.84 9.59 7.26 13.11 7.11 12.82 5.61
Step 4801/20000   lossTr 5.6848   accTr 19.5   181s = 0.03s/step
Step 4801/20000   Activation magnitudes (2): 0.92 0.76 0.76 1.09 0.86 1.09 0.75 0.61
Step 4801/20000   Activation magnitudes (3): 7.71 4.72 9.57 6.89 13.00 8.46 13.00 4.81
Step 5001/20000   lossVa 5.6580   accVa 19.5
Step 5001/20000   lossTr 5.6767   accTr 19.5   187s = 0.03s/step
Step 5001/20000   Activation magnitudes (2): 0.94 0.79 0.77 1.13 0.88 1.16 0.75 0.64
Step 5001/20000   Activation magnitudes (3): 7.72 5.02 9.57 6.84 14.12 7.32 13.72 5.05
Step 5201/20000   lossTr 5.6295   accTr 19.5   200s = 0.06s/step
Step 5201/20000   Activation magnitudes (2): 0.94 0.79 0.76 1.14 0.89 1.16 0.75 0.63
Step 5201/20000   Activation magnitudes (3): 7.54 5.21 9.76 6.89 13.75 8.29 13.46 5.19
Step 5401/20000   lossTr 5.6022   accTr 20.0   208s = 0.04s/step
Step 5401/20000   Activation magnitudes (2): 0.96 0.80 0.76 1.16 0.93 1.18 0.77 0.66
Step 5401/20000   Activation magnitudes (3): 7.42 5.45 9.67 8.57 14.49 7.24 14.04 5.65
Step 5501/20000   lossVa 5.5967   accVa 20.0
Step 5601/20000   lossTr 5.5764   accTr 20.1   215s = 0.04s/step
Step 5601/20000   Activation magnitudes (2): 0.98 0.81 0.75 1.16 0.96 1.22 0.79 0.69
Step 5601/20000   Activation magnitudes (3): 7.33 5.47 9.30 7.61 14.56 7.94 14.53 5.60
Step 5801/20000   lossTr 5.5502   accTr 20.5   223s = 0.04s/step
Step 5801/20000   Activation magnitudes (2): 1.00 0.83 0.75 1.25 1.01 1.28 0.79 0.70
Step 5801/20000   Activation magnitudes (3): 7.36 5.30 9.24 7.19 14.17 8.18 14.60 5.48
Step 6001/20000   lossVa 5.5252   accVa 20.8
Step 6001/20000   lossTr 5.5551   accTr 20.5   239s = 0.08s/step
Step 6001/20000   Activation magnitudes (2): 1.01 0.87 0.75 1.26 1.01 1.28 0.79 0.70
Step 6001/20000   Activation magnitudes (3): 7.10 5.36 9.24 7.17 15.38 8.70 15.21 6.36
Step 6201/20000   lossTr 5.5056   accTr 20.7   247s = 0.04s/step
Step 6201/20000   Activation magnitudes (2): 1.05 0.85 0.74 1.23 1.05 1.33 0.83 0.73
Step 6201/20000   Activation magnitudes (3): 7.10 5.78 9.04 7.26 15.05 7.56 15.83 5.43
Step 6401/20000   lossTr 5.4968   accTr 21.0   260s = 0.06s/step
Step 6401/20000   Activation magnitudes (2): 1.05 0.90 0.73 1.27 1.06 1.34 0.85 0.72
Step 6401/20000   Activation magnitudes (3): 7.23 6.12 8.19 7.78 14.61 7.79 15.39 5.58
Step 6501/20000   lossVa 5.4627   accVa 21.5
Step 6601/20000   lossTr 5.4433   accTr 21.5   269s = 0.05s/step
Step 6601/20000   Activation magnitudes (2): 1.08 0.89 0.73 1.27 1.10 1.34 0.86 0.75
Step 6601/20000   Activation magnitudes (3): 7.34 7.05 8.49 7.37 15.26 8.98 15.76 5.65
Step 6801/20000   lossTr 5.4446   accTr 21.5   278s = 0.04s/step
Step 6801/20000   Activation magnitudes (2): 1.10 0.92 0.73 1.27 1.13 1.37 0.87 0.76
Step 6801/20000   Activation magnitudes (3): 7.51 7.08 8.22 7.67 15.10 8.16 16.20 6.08
Step 7001/20000   lossVa 5.4095   accVa 22.1
Step 7001/20000   lossTr 5.4216   accTr 22.0   293s = 0.07s/step
Step 7001/20000   Activation magnitudes (2): 1.12 0.93 0.72 1.32 1.16 1.42 0.90 0.76
Step 7001/20000   Activation magnitudes (3): 7.44 7.70 8.54 7.67 16.02 9.09 15.95 6.03
Step 7201/20000   lossTr 5.4093   accTr 22.0   306s = 0.07s/step
Step 7201/20000   Activation magnitudes (2): 1.14 0.94 0.71 1.32 1.18 1.41 0.92 0.77
Step 7201/20000   Activation magnitudes (3): 7.34 8.53 8.16 7.71 15.45 8.27 16.65 6.15
Step 7401/20000   lossTr 5.3898   accTr 22.0   315s = 0.05s/step
Step 7401/20000   Activation magnitudes (2): 1.17 0.95 0.70 1.36 1.21 1.44 0.94 0.81
Step 7401/20000   Activation magnitudes (3): 7.44 7.73 8.04 7.94 15.69 8.94 16.95 6.30
Step 7501/20000   lossVa 5.3873   accVa 22.2
Step 7601/20000   lossTr 5.3831   accTr 22.3   325s = 0.05s/step
Step 7601/20000   Activation magnitudes (2): 1.19 0.95 0.70 1.36 1.23 1.49 0.97 0.84
Step 7601/20000   Activation magnitudes (3): 8.06 8.47 7.87 7.51 15.53 9.13 16.63 6.37
Step 7801/20000   lossTr 5.3126   accTr 22.6   334s = 0.05s/step
Step 7801/20000   Activation magnitudes (2): 1.19 0.99 0.69 1.41 1.24 1.50 0.98 0.85
Step 7801/20000   Activation magnitudes (3): 8.38 8.52 7.58 8.89 15.70 8.69 17.47 7.80
Step 8001/20000   lossVa 5.3314   accVa 22.9
Step 8001/20000   lossTr 5.3766   accTr 22.4   344s = 0.05s/step
Step 8001/20000   Activation magnitudes (2): 1.20 0.99 0.69 1.40 1.25 1.49 0.97 0.80
Step 8001/20000   Activation magnitudes (3): 8.09 8.82 7.72 8.08 16.94 8.82 17.58 6.50
Step 8201/20000   lossTr 5.2948   accTr 23.0   351s = 0.04s/step
Step 8201/20000   Activation magnitudes (2): 1.22 0.99 0.67 1.43 1.27 1.45 1.00 0.84
Step 8201/20000   Activation magnitudes (3): 7.91 6.90 7.38 7.87 16.37 7.86 17.02 6.53
Step 8401/20000   lossTr 5.2902   accTr 23.0   358s = 0.04s/step
Step 8401/20000   Activation magnitudes (2): 1.24 1.01 0.67 1.46 1.29 1.55 1.03 0.87
Step 8401/20000   Activation magnitudes (3): 8.32 9.32 7.57 8.25 15.73 9.34 18.00 6.99
Step 8501/20000   lossVa 5.3232   accVa 23.0
Step 8601/20000   lossTr 5.3508   accTr 22.6   367s = 0.04s/step
Step 8601/20000   Activation magnitudes (2): 1.25 1.01 0.66 1.39 1.29 1.53 1.03 0.84
Step 8601/20000   Activation magnitudes (3): 8.22 9.51 7.31 8.14 15.82 9.08 18.93 6.16
Step 8801/20000   lossTr 5.2775   accTr 23.2   376s = 0.05s/step
Step 8801/20000   Activation magnitudes (2): 1.25 1.04 0.66 1.48 1.32 1.57 1.07 0.88
Step 8801/20000   Activation magnitudes (3): 8.05 9.44 7.20 8.44 15.93 8.16 18.46 6.80
Step 9001/20000   lossVa 5.2672   accVa 23.3
Step 9001/20000   lossTr 5.2496   accTr 23.2   387s = 0.05s/step
Step 9001/20000   Activation magnitudes (2): 1.24 1.05 0.65 1.49 1.33 1.59 1.06 0.85
Step 9001/20000   Activation magnitudes (3): 8.38 9.76 6.96 8.19 16.38 8.96 17.86 6.94
Step 9201/20000   lossTr 5.3119   accTr 23.3   395s = 0.04s/step
Step 9201/20000   Activation magnitudes (2): 1.25 1.06 0.64 1.48 1.33 1.55 1.08 0.87
Step 9201/20000   Activation magnitudes (3): 9.24 9.75 7.04 8.61 16.60 9.49 18.77 6.59
Step 9401/20000   lossTr 5.2425   accTr 23.4   403s = 0.04s/step
Step 9401/20000   Activation magnitudes (2): 1.25 1.08 0.64 1.51 1.34 1.59 1.09 0.87
Step 9401/20000   Activation magnitudes (3): 9.49 9.72 6.81 8.56 16.68 9.93 19.53 6.60
Step 9501/20000   lossVa 5.2614   accVa 23.4
Step 9601/20000   lossTr 5.2701   accTr 23.4   416s = 0.06s/step
Step 9601/20000   Activation magnitudes (2): 1.26 1.06 0.63 1.47 1.34 1.61 1.12 0.90
Step 9601/20000   Activation magnitudes (3): 8.75 10.19 6.80 8.47 16.27 8.78 19.50 7.01
Step 9801/20000   lossTr 5.2428   accTr 23.7   424s = 0.04s/step
Step 9801/20000   Activation magnitudes (2): 1.26 1.10 0.63 1.53 1.35 1.58 1.11 0.86
Step 9801/20000   Activation magnitudes (3): 8.43 10.11 6.86 9.57 16.16 8.87 19.37 7.21
Step 10001/20000   lossVa 5.2331   accVa 23.7
Step 10001/20000   lossTr 5.2308   accTr 23.6   439s = 0.08s/step
Step 10001/20000   Activation magnitudes (2): 1.27 1.13 0.63 1.61 1.37 1.65 1.13 0.89
Step 10001/20000   Activation magnitudes (3): 8.41 10.29 6.78 9.33 17.14 8.88 19.22 6.89
Step 10201/20000   lossTr 5.2487   accTr 23.6   455s = 0.08s/step
Step 10201/20000   Activation magnitudes (2): 1.29 1.11 0.62 1.55 1.37 1.65 1.16 0.93
Step 10201/20000   Activation magnitudes (3): 8.69 9.98 6.71 9.91 16.69 9.53 19.66 7.19
Step 10401/20000   lossTr 5.2509   accTr 23.6   462s = 0.04s/step
Step 10401/20000   Activation magnitudes (2): 1.28 1.14 0.61 1.61 1.38 1.63 1.18 0.91
Step 10401/20000   Activation magnitudes (3): 8.78 10.68 6.48 8.93 17.07 8.73 20.04 8.35
Step 10501/20000   lossVa 5.2075   accVa 24.0
Step 10601/20000   lossTr 5.2032   accTr 23.9   473s = 0.05s/step
Step 10601/20000   Activation magnitudes (2): 1.28 1.13 0.61 1.55 1.38 1.61 1.15 0.89
Step 10601/20000   Activation magnitudes (3): 8.85 10.24 6.70 10.36 17.01 9.97 20.95 8.25
Step 10801/20000   lossTr 5.2463   accTr 23.7   483s = 0.05s/step
Step 10801/20000   Activation magnitudes (2): 1.30 1.14 0.60 1.58 1.40 1.66 1.20 0.93
Step 10801/20000   Activation magnitudes (3): 9.06 10.65 6.60 9.35 16.68 9.53 20.27 6.87
Step 11001/20000   lossVa 5.1978   accVa 24.0
Step 11001/20000   lossTr 5.1699   accTr 24.2   493s = 0.05s/step
Step 11001/20000   Activation magnitudes (2): 1.29 1.17 0.60 1.60 1.40 1.69 1.19 0.90
Step 11001/20000   Activation magnitudes (3): 9.26 11.48 6.37 9.46 17.86 9.22 20.64 7.02
Step 11201/20000   lossTr 5.2079   accTr 24.0   501s = 0.04s/step
Step 11201/20000   Activation magnitudes (2): 1.31 1.13 0.59 1.53 1.40 1.66 1.23 0.96
Step 11201/20000   Activation magnitudes (3): 9.31 10.97 6.38 9.46 16.05 10.54 20.63 7.08
Step 11401/20000   lossTr 5.2088   accTr 23.8   509s = 0.04s/step
Step 11401/20000   Activation magnitudes (2): 1.31 1.16 0.59 1.64 1.42 1.66 1.22 0.92
Step 11401/20000   Activation magnitudes (3): 9.57 10.86 6.44 10.00 16.62 10.12 21.72 6.90
Step 11501/20000   lossVa 5.1667   accVa 24.3
Step 11601/20000   lossTr 5.1717   accTr 24.0   519s = 0.05s/step
Step 11601/20000   Activation magnitudes (2): 1.32 1.17 0.58 1.59 1.40 1.71 1.24 0.94
Step 11601/20000   Activation magnitudes (3): 9.74 11.51 6.39 8.89 16.25 10.14 20.87 7.60
Step 11801/20000   lossTr 5.1519   accTr 24.3   533s = 0.07s/step
Step 11801/20000   Activation magnitudes (2): 1.33 1.19 0.58 1.63 1.43 1.70 1.26 0.95
Step 11801/20000   Activation magnitudes (3): 8.22 8.74 6.23 8.92 17.14 9.59 20.52 7.48
Step 12001/20000   lossVa 5.1519   accVa 24.4
Step 12001/20000   lossTr 5.1688   accTr 24.2   542s = 0.04s/step
Step 12001/20000   Activation magnitudes (2): 1.33 1.21 0.58 1.67 1.43 1.74 1.25 0.93
Step 12001/20000   Activation magnitudes (3): 10.07 11.47 6.24 9.65 17.90 9.94 21.24 7.88
Step 12201/20000   lossTr 5.1520   accTr 24.4   551s = 0.05s/step
Step 12201/20000   Activation magnitudes (2): 1.33 1.20 0.57 1.63 1.43 1.69 1.27 0.95
Step 12201/20000   Activation magnitudes (3): 10.13 11.44 6.36 10.11 16.61 10.63 21.48 7.19
Step 12401/20000   lossTr 5.1359   accTr 24.3   560s = 0.04s/step
Step 12401/20000   Activation magnitudes (2): 1.34 1.20 0.57 1.61 1.44 1.70 1.31 0.97
Step 12401/20000   Activation magnitudes (3): 10.40 11.38 6.33 8.81 16.87 11.76 21.14 7.89
Step 12501/20000   lossVa 5.1396   accVa 24.4
Step 12601/20000   lossTr 5.1322   accTr 24.5   568s = 0.04s/step
Step 12601/20000   Activation magnitudes (2): 1.36 1.18 0.56 1.59 1.44 1.69 1.33 0.97
Step 12601/20000   Activation magnitudes (3): 10.56 11.64 6.44 10.18 16.73 10.51 21.98 7.88
Step 12801/20000   lossTr 5.1479   accTr 24.5   576s = 0.04s/step
Step 12801/20000   Activation magnitudes (2): 1.36 1.22 0.57 1.64 1.45 1.73 1.32 0.95
Step 12801/20000   Activation magnitudes (3): 10.67 10.98 6.21 9.63 16.18 10.52 22.05 8.06
Step 13001/20000   lossVa 5.1247   accVa 24.6
Step 13001/20000   lossTr 5.1047   accTr 24.7   587s = 0.05s/step
Step 13001/20000   Activation magnitudes (2): 1.37 1.24 0.56 1.68 1.46 1.76 1.30 0.94
Step 13001/20000   Activation magnitudes (3): 10.79 11.38 6.27 9.88 17.41 11.40 22.28 8.36
Step 13201/20000   lossTr 5.1242   accTr 24.6   599s = 0.06s/step
Step 13201/20000   Activation magnitudes (2): 1.39 1.20 0.55 1.61 1.46 1.73 1.35 0.98
Step 13201/20000   Activation magnitudes (3): 10.93 11.58 6.20 9.85 16.66 12.79 22.49 8.34
Step 13401/20000   lossTr 5.1203   accTr 24.4   611s = 0.06s/step
Step 13401/20000   Activation magnitudes (2): 1.41 1.23 0.55 1.68 1.46 1.70 1.35 0.97
Step 13401/20000   Activation magnitudes (3): 8.87 7.95 5.96 9.28 16.50 10.27 22.76 7.87
Step 13501/20000   lossVa 5.0970   accVa 24.8
Step 13601/20000   lossTr 5.1322   accTr 24.7   622s = 0.05s/step
Step 13601/20000   Activation magnitudes (2): 1.39 1.24 0.56 1.73 1.49 1.71 1.38 0.93
Step 13601/20000   Activation magnitudes (3): 11.28 11.13 5.99 10.13 16.32 11.86 22.78 8.45
Step 13801/20000   lossTr 5.0901   accTr 24.9   630s = 0.04s/step
Step 13801/20000   Activation magnitudes (2): 1.42 1.24 0.54 1.65 1.47 1.78 1.38 1.01
Step 13801/20000   Activation magnitudes (3): 11.38 11.28 6.11 9.58 16.71 11.79 22.60 9.62
Step 14001/20000   lossVa 5.1043   accVa 24.9
Step 14001/20000   lossTr 5.0761   accTr 25.0   644s = 0.07s/step
Step 14001/20000   Activation magnitudes (2): 1.41 1.25 0.54 1.67 1.48 1.79 1.35 0.97
Step 14001/20000   Activation magnitudes (3): 11.56 11.11 6.01 10.66 17.25 12.32 23.26 9.03
Step 14201/20000   lossTr 5.0829   accTr 25.0   654s = 0.05s/step
Step 14201/20000   Activation magnitudes (2): 1.43 1.23 0.54 1.63 1.48 1.75 1.41 0.98
Step 14201/20000   Activation magnitudes (3): 11.67 11.12 6.00 9.38 16.50 12.08 23.19 9.88
Step 14401/20000   lossTr 5.1022   accTr 25.1   670s = 0.08s/step
Step 14401/20000   Activation magnitudes (2): 1.42 1.25 0.54 1.74 1.51 1.64 1.43 0.96
Step 14401/20000   Activation magnitudes (3): 11.84 11.35 5.99 9.77 16.52 11.61 23.53 8.75
Step 14501/20000   lossVa 5.0830   accVa 25.0
Step 14601/20000   lossTr 5.0823   accTr 25.0   679s = 0.05s/step
Step 14601/20000   Activation magnitudes (2): 1.45 1.23 0.54 1.66 1.50 1.75 1.43 1.00
Step 14601/20000   Activation magnitudes (3): 11.96 11.65 5.94 9.89 16.34 12.32 23.30 9.04
Step 14801/20000   lossTr 5.0756   accTr 24.8   688s = 0.04s/step
Step 14801/20000   Activation magnitudes (2): 1.46 1.25 0.53 1.63 1.50 1.79 1.42 0.98
Step 14801/20000   Activation magnitudes (3): 12.06 11.17 5.81 10.01 17.18 11.92 24.54 10.14
Step 15001/20000   lossVa 5.0715   accVa 25.0
Step 15001/20000   lossTr 5.0605   accTr 25.2   697s = 0.04s/step
Step 15001/20000   Activation magnitudes (2): 1.46 1.26 0.53 1.70 1.51 1.83 1.42 0.98
Step 15001/20000   Activation magnitudes (3): 12.15 11.43 5.84 10.47 17.46 13.56 23.72 10.44
Step 15201/20000   lossTr 5.0646   accTr 25.1   708s = 0.06s/step
Step 15201/20000   Activation magnitudes (2): 1.47 1.25 0.53 1.70 1.52 1.79 1.46 0.99
Step 15201/20000   Activation magnitudes (3): 12.29 11.16 5.86 11.42 16.70 13.06 24.49 10.30
Step 15401/20000   lossTr 5.0489   accTr 25.4   719s = 0.06s/step
Step 15401/20000   Activation magnitudes (2): 1.47 1.25 0.53 1.70 1.52 1.74 1.45 0.97
Step 15401/20000   Activation magnitudes (3): 12.46 11.02 5.92 10.16 16.99 14.55 24.81 10.87
Step 15501/20000   lossVa 5.0516   accVa 25.3
Step 15601/20000   lossTr 5.0838   accTr 24.8   729s = 0.05s/step
Step 15601/20000   Activation magnitudes (2): 1.50 1.25 0.52 1.65 1.52 1.82 1.49 1.02
Step 15601/20000   Activation magnitudes (3): 12.54 11.50 5.85 9.65 16.59 13.18 24.31 10.55
Step 15801/20000   lossTr 5.0407   accTr 25.1   746s = 0.08s/step
Step 15801/20000   Activation magnitudes (2): 1.52 1.24 0.52 1.66 1.54 1.81 1.53 1.06
Step 15801/20000   Activation magnitudes (3): 12.64 11.02 5.88 10.14 16.66 13.62 24.78 10.43
Step 16001/20000   lossVa 5.0375   accVa 25.3
Step 16001/20000   lossTr 5.0408   accTr 25.2   754s = 0.04s/step
Step 16001/20000   Activation magnitudes (2): 1.52 1.26 0.52 1.69 1.54 1.87 1.47 1.00
Step 16001/20000   Activation magnitudes (3): 12.71 11.26 5.75 9.82 17.18 14.27 25.27 11.10
Step 16201/20000   lossTr 5.0399   accTr 25.1   762s = 0.04s/step
Step 16201/20000   Activation magnitudes (2): 1.53 1.27 0.52 1.68 1.53 1.81 1.50 1.02
Step 16201/20000   Activation magnitudes (3): 12.87 11.11 5.79 11.28 16.69 13.78 25.88 10.26
Step 16401/20000   lossTr 5.0403   accTr 25.2   775s = 0.06s/step
Step 16401/20000   Activation magnitudes (2): 1.55 1.25 0.52 1.65 1.55 1.82 1.55 1.05
Step 16401/20000   Activation magnitudes (3): 12.98 11.06 5.77 9.86 16.81 13.81 26.60 11.00
Step 16501/20000   lossVa 5.0468   accVa 25.5
Step 16601/20000   lossTr 4.9901   accTr 25.8   789s = 0.07s/step
Step 16601/20000   Activation magnitudes (2): 1.55 1.27 0.52 1.67 1.55 1.76 1.55 1.03
Step 16601/20000   Activation magnitudes (3): 13.06 11.04 5.69 9.98 16.70 14.20 26.57 11.61
Step 16801/20000   lossTr 5.0235   accTr 25.6   797s = 0.04s/step
Step 16801/20000   Activation magnitudes (2): 1.56 1.25 0.51 1.65 1.55 1.78 1.53 1.01
Step 16801/20000   Activation magnitudes (3): 13.17 11.14 5.74 10.37 16.84 13.74 27.44 11.29
Step 17001/20000   lossVa 5.0062   accVa 25.6
Step 17001/20000   lossTr 4.9966   accTr 25.6   807s = 0.05s/step
Step 17001/20000   Activation magnitudes (2): 1.57 1.26 0.52 1.71 1.57 1.84 1.52 1.01
Step 17001/20000   Activation magnitudes (3): 13.28 11.26 5.66 10.40 17.02 14.15 28.87 12.00
Step 17201/20000   lossTr 5.0482   accTr 25.4   816s = 0.04s/step
Step 17201/20000   Activation magnitudes (2): 1.59 1.25 0.51 1.66 1.56 1.85 1.56 1.05
Step 17201/20000   Activation magnitudes (3): 13.35 11.16 5.66 10.14 16.87 15.08 28.78 12.22
Step 17401/20000   lossTr 5.0455   accTr 25.3   831s = 0.07s/step
Step 17401/20000   Activation magnitudes (2): 1.59 1.24 0.51 1.66 1.57 1.79 1.54 1.02
Step 17401/20000   Activation magnitudes (3): 13.41 11.03 5.60 9.39 16.88 14.16 29.34 12.51
Step 17501/20000   lossVa 4.9971   accVa 25.7
Step 17601/20000   lossTr 5.0215   accTr 25.4   839s = 0.04s/step
Step 17601/20000   Activation magnitudes (2): 1.61 1.26 0.51 1.67 1.56 1.88 1.58 1.05
Step 17601/20000   Activation magnitudes (3): 13.51 11.09 5.58 10.40 17.02 13.51 29.39 12.41
Step 17801/20000   lossTr 5.0127   accTr 25.4   860s = 0.10s/step
Step 17801/20000   Activation magnitudes (2): 1.61 1.24 0.51 1.65 1.57 1.84 1.60 1.06
Step 17801/20000   Activation magnitudes (3): 13.58 10.96 5.53 9.75 17.07 15.72 29.16 12.29
Step 18001/20000   lossVa 4.9831   accVa 25.8
Step 18001/20000   lossTr 5.0208   accTr 25.4   868s = 0.04s/step
Step 18001/20000   Activation magnitudes (2): 1.61 1.25 0.51 1.67 1.59 1.88 1.56 1.03
Step 18001/20000   Activation magnitudes (3): 13.64 11.10 5.57 9.36 16.98 15.49 30.20 13.33
Step 18201/20000   lossTr 4.9531   accTr 26.2   877s = 0.04s/step
Step 18201/20000   Activation magnitudes (2): 1.63 1.25 0.51 1.71 1.59 1.82 1.62 1.06
Step 18201/20000   Activation magnitudes (3): 13.70 11.24 5.51 10.69 16.98 15.98 30.64 13.55
Step 18401/20000   lossTr 4.9431   accTr 26.1   890s = 0.07s/step
Step 18401/20000   Activation magnitudes (2): 1.63 1.26 0.51 1.69 1.59 1.88 1.59 1.06
Step 18401/20000   Activation magnitudes (3): 13.75 11.15 5.52 9.87 17.00 16.17 31.12 13.82
Step 18501/20000   lossVa 4.9701   accVa 25.9
Step 18601/20000   lossTr 4.9600   accTr 26.2   899s = 0.04s/step
Step 18601/20000   Activation magnitudes (2): 1.64 1.26 0.51 1.74 1.60 1.87 1.63 1.10
Step 18601/20000   Activation magnitudes (3): 10.57 10.63 5.48 9.96 16.28 16.87 30.04 13.56
Step 18801/20000   lossTr 4.9505   accTr 26.0   909s = 0.05s/step
Step 18801/20000   Activation magnitudes (2): 1.65 1.24 0.51 1.64 1.60 1.86 1.59 1.06
Step 18801/20000   Activation magnitudes (3): 13.90 11.36 5.53 9.62 16.90 16.82 31.12 13.78
Step 19001/20000   lossVa 4.9559   accVa 26.0
Step 19001/20000   lossTr 4.9596   accTr 26.0   923s = 0.07s/step
Step 19001/20000   Activation magnitudes (2): 1.64 1.25 0.51 1.67 1.60 1.87 1.57 1.04
Step 19001/20000   Activation magnitudes (3): 13.92 10.89 5.52 9.72 16.95 16.41 31.69 14.18
Step 19201/20000   lossTr 4.9787   accTr 25.9   936s = 0.07s/step
Step 19201/20000   Activation magnitudes (2): 1.66 1.25 0.51 1.66 1.60 1.88 1.62 1.08
Step 19201/20000   Activation magnitudes (3): 13.98 10.98 5.56 9.91 17.07 16.80 31.12 14.17
Step 19401/20000   lossTr 4.9637   accTr 26.0   945s = 0.05s/step
Step 19401/20000   Activation magnitudes (2): 1.66 1.25 0.51 1.69 1.61 1.86 1.60 1.08
Step 19401/20000   Activation magnitudes (3): 14.02 10.80 5.43 9.85 17.07 17.17 31.51 13.32
Step 19501/20000   lossVa 4.9426   accVa 26.2
Step 19601/20000   lossTr 4.9295   accTr 26.1   953s = 0.04s/step
Step 19601/20000   Activation magnitudes (2): 1.66 1.24 0.51 1.66 1.61 1.83 1.62 1.09
Step 19601/20000   Activation magnitudes (3): 14.02 10.88 5.49 10.89 17.10 17.03 31.53 14.36
Step 19801/20000   lossTr 4.9654   accTr 26.1   969s = 0.08s/step
Step 19801/20000   Activation magnitudes (2): 1.66 1.24 0.51 1.65 1.61 1.88 1.64 1.11
Step 19801/20000   Activation magnitudes (3): 14.04 10.87 5.41 10.27 17.15 18.21 31.37 14.18
Step 20000/20000   lossVa 4.9334   accVa 26.3
Step 20000/20000   Peak CUDA memory usage: 1.302 GB
Step 20000/20000   lossTr 4.9519   accTr 26.3   977s = 0.04s/step
Step 20000/20000   Activation magnitudes (2): 1.66 1.25 0.51 1.69 1.62 1.87 1.58 1.07
Step 20000/20000   Activation magnitudes (3): 14.04 10.85 5.49 9.98 17.13 16.83 32.21 14.56
Saving plot (loss/acc): ./results-fineweb10B\tr-baseline-gelu-step020000-00001616-00004616.png
Saving plot (AF): ./results-fineweb10B\af-baseline-gelu-step000000.png
Saving plot (act): ./results-fineweb10B\act-baseline-gelu-step020000.png
