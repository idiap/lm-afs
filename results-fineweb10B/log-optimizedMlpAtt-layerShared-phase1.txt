Hyperparameters:
  runName = "optimizedMlpAtt-layerShared-phase1"
  seed = 1234
  plotMinAcc = 0.0
  plotMaxAcc = 0.4
  plotMinLoss = 3.0
  plotMaxLoss = 15.0
  shuffleTrData = 1
  tokenDocSep = 50256
  nLayers = 4
  nHeads = 4
  dimEmb = 256
  tieEmbeddings = True
  lr = 0.0006
  freezeEmbeddings = 0
  batchSize = 1
  seqLength = 4096
  contextSize = 1024
  nSteps = 20000
  nStepsWarmup = 2000
  nStepsCooldown = 10000
  lrSchedule = "trap"
  wtDecay = 0.0
  nModels = 20
  staggeredStarts = 2
  sameDataAcrossModels = False
  sameInitAcrossModels = False
  adamB1 = 0.8
  adamB2 = 0.95
  gradientClipping = 0
  afType = ['spline', 'spline']
  afLayerSpecific = 0
  afRange = 15
  afNAnchors = 64
  afInit = 0.01
  afLr = 0.1
  afAdamB1 = 0.99
  afAdamB2 = 0.999
  afFileToLoad = ""
  dirResults = "./results-fineweb10B"
  filesTokensTr = "./data-fineweb10B/fineweb_train_*.bin"
  filesTokensVa = "./data-fineweb10B/fineweb_val_*.bin"
  nTokensVa = 10485760
  valEvery = 500
  saveModelEvery = -1
  saveAfEvery = 0
  plotEvery = 200
  plottingLevel = 2
  dtype = torch.float32
  flexBlockSize = 32
====================================================================================================
pytorch 2.10.0.dev20251001+cu126, CUDA 12.6
Fri Feb 13 15:58:40 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 566.07                 Driver Version: 566.07         CUDA Version: 12.7     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 4090 ...  WDDM  |   00000000:01:00.0 Off |                  N/A |
| N/A   65C    P3             27W /  110W |     236MiB /  16376MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A      5772      C   ...nvs\modded-nanogpt-py312\python.exe      N/A      |
+-----------------------------------------------------------------------------------------+

Tr data: 899,999,680 tokens, 9 files (./data-fineweb10B/fineweb_train_*.bin)
Va data: 100,000,000 tokens, 1 files (./data-fineweb10B/fineweb_val_*.bin)
Number of validation batches: 2,560 (10,485,760 tokens, 4,096 tokens/sequence)
====================================================================================================
Projected number of training tokens:
  20,000 steps * 1 batchSize * 4,096 seqLength
  = 81,920,000 tokens
  = 0.09 epochs
Vocabulary size: 50,304
====================================================================================================
Number of model parameters (frozen): 0
Number of model parameters (trainable): 16,026,262
  _orig_mod.skipWeights                              2
  _orig_mod.transformer.wte.weight                   12,877,824
  _orig_mod.transformer.h.0.blockLambdas             2
  _orig_mod.transformer.h.0.attn.lambdas             1
  _orig_mod.transformer.h.0.attn.projAtt.weight      1
  _orig_mod.transformer.h.0.attn.projAtt.bias        1
  _orig_mod.transformer.h.0.attn.af.afVals           64
  _orig_mod.transformer.h.0.attn.projQ.weight        65,536
  _orig_mod.transformer.h.0.attn.projK.weight        65,536
  _orig_mod.transformer.h.0.attn.projV.weight        65,536
  _orig_mod.transformer.h.0.attn.projOut.weight      65,536
  _orig_mod.transformer.h.0.mlp.fc.weight            262,144
  _orig_mod.transformer.h.0.mlp.proj.weight          262,144
  _orig_mod.transformer.h.0.mlp.af.afVals            64
  _orig_mod.transformer.h.0.normAtt.weight           256
  _orig_mod.transformer.h.0.normMlp.weight           256
  _orig_mod.transformer.h.1.blockLambdas             2
  _orig_mod.transformer.h.1.attn.lambdas             1
  _orig_mod.transformer.h.1.attn.projAtt.weight      1
  _orig_mod.transformer.h.1.attn.projAtt.bias        1
  _orig_mod.transformer.h.1.attn.projQ.weight        65,536
  _orig_mod.transformer.h.1.attn.projK.weight        65,536
  _orig_mod.transformer.h.1.attn.projV.weight        65,536
  _orig_mod.transformer.h.1.attn.projOut.weight      65,536
  _orig_mod.transformer.h.1.mlp.fc.weight            262,144
  _orig_mod.transformer.h.1.mlp.proj.weight          262,144
  _orig_mod.transformer.h.1.normAtt.weight           256
  _orig_mod.transformer.h.1.normMlp.weight           256
  _orig_mod.transformer.h.2.blockLambdas             2
  _orig_mod.transformer.h.2.attn.lambdas             1
  _orig_mod.transformer.h.2.attn.projAtt.weight      1
  _orig_mod.transformer.h.2.attn.projAtt.bias        1
  _orig_mod.transformer.h.2.attn.projQ.weight        65,536
  _orig_mod.transformer.h.2.attn.projK.weight        65,536
  _orig_mod.transformer.h.2.attn.projV.weight        65,536
  _orig_mod.transformer.h.2.attn.projOut.weight      65,536
  _orig_mod.transformer.h.2.mlp.fc.weight            262,144
  _orig_mod.transformer.h.2.mlp.proj.weight          262,144
  _orig_mod.transformer.h.2.normAtt.weight           256
  _orig_mod.transformer.h.2.normMlp.weight           256
  _orig_mod.transformer.h.3.blockLambdas             2
  _orig_mod.transformer.h.3.attn.lambdas             1
  _orig_mod.transformer.h.3.attn.projAtt.weight      1
  _orig_mod.transformer.h.3.attn.projAtt.bias        1
  _orig_mod.transformer.h.3.attn.projQ.weight        65,536
  _orig_mod.transformer.h.3.attn.projK.weight        65,536
  _orig_mod.transformer.h.3.attn.projV.weight        65,536
  _orig_mod.transformer.h.3.attn.projOut.weight      65,536
  _orig_mod.transformer.h.3.mlp.fc.weight            262,144
  _orig_mod.transformer.h.3.mlp.proj.weight          262,144
  _orig_mod.transformer.h.3.normAtt.weight           256
  _orig_mod.transformer.h.3.normMlp.weight           256
  _orig_mod.normWte.weight                           256
  _orig_mod.normWte.bias                             256
5.1 tokens/parameter
====================================================================================================
Optimizer #0/20: Adam
  Param group 0: lr=0.0006, weight_decay=0.0
  models.0._orig_mod.skipWeights
  models.0._orig_mod.transformer.wte.weight
  models.0._orig_mod.transformer.h.0.blockLambdas
  models.0._orig_mod.transformer.h.0.attn.lambdas
  models.0._orig_mod.transformer.h.0.attn.projAtt.weight
  models.0._orig_mod.transformer.h.0.attn.projAtt.bias
  models.0._orig_mod.transformer.h.0.attn.projQ.weight
  models.0._orig_mod.transformer.h.0.attn.projK.weight
  models.0._orig_mod.transformer.h.0.attn.projV.weight
  models.0._orig_mod.transformer.h.0.attn.projOut.weight
  models.0._orig_mod.transformer.h.0.mlp.fc.weight
  models.0._orig_mod.transformer.h.0.mlp.proj.weight
  models.0._orig_mod.transformer.h.0.normAtt.weight
  models.0._orig_mod.transformer.h.0.normMlp.weight
  models.0._orig_mod.transformer.h.1.blockLambdas
  models.0._orig_mod.transformer.h.1.attn.lambdas
  models.0._orig_mod.transformer.h.1.attn.projAtt.weight
  models.0._orig_mod.transformer.h.1.attn.projAtt.bias
  models.0._orig_mod.transformer.h.1.attn.projQ.weight
  models.0._orig_mod.transformer.h.1.attn.projK.weight
  models.0._orig_mod.transformer.h.1.attn.projV.weight
  models.0._orig_mod.transformer.h.1.attn.projOut.weight
  models.0._orig_mod.transformer.h.1.mlp.fc.weight
  models.0._orig_mod.transformer.h.1.mlp.proj.weight
  models.0._orig_mod.transformer.h.1.normAtt.weight
  models.0._orig_mod.transformer.h.1.normMlp.weight
  models.0._orig_mod.transformer.h.2.blockLambdas
  models.0._orig_mod.transformer.h.2.attn.lambdas
  models.0._orig_mod.transformer.h.2.attn.projAtt.weight
  models.0._orig_mod.transformer.h.2.attn.projAtt.bias
  models.0._orig_mod.transformer.h.2.attn.projQ.weight
  models.0._orig_mod.transformer.h.2.attn.projK.weight
  models.0._orig_mod.transformer.h.2.attn.projV.weight
  models.0._orig_mod.transformer.h.2.attn.projOut.weight
  models.0._orig_mod.transformer.h.2.mlp.fc.weight
  models.0._orig_mod.transformer.h.2.mlp.proj.weight
  models.0._orig_mod.transformer.h.2.normAtt.weight
  models.0._orig_mod.transformer.h.2.normMlp.weight
  models.0._orig_mod.transformer.h.3.blockLambdas
  models.0._orig_mod.transformer.h.3.attn.lambdas
  models.0._orig_mod.transformer.h.3.attn.projAtt.weight
  models.0._orig_mod.transformer.h.3.attn.projAtt.bias
  models.0._orig_mod.transformer.h.3.attn.projQ.weight
  models.0._orig_mod.transformer.h.3.attn.projK.weight
  models.0._orig_mod.transformer.h.3.attn.projV.weight
  models.0._orig_mod.transformer.h.3.attn.projOut.weight
  models.0._orig_mod.transformer.h.3.mlp.fc.weight
  models.0._orig_mod.transformer.h.3.mlp.proj.weight
  models.0._orig_mod.transformer.h.3.normAtt.weight
  models.0._orig_mod.transformer.h.3.normMlp.weight
  models.0._orig_mod.normWte.weight
  models.0._orig_mod.normWte.bias
Optimizer #1/20: Adam
  Param group 0: lr=0.0006, weight_decay=0.0
  models.1._orig_mod.skipWeights
  models.1._orig_mod.transformer.wte.weight
  models.1._orig_mod.transformer.h.0.blockLambdas
  models.1._orig_mod.transformer.h.0.attn.lambdas
  models.1._orig_mod.transformer.h.0.attn.projAtt.weight
  models.1._orig_mod.transformer.h.0.attn.projAtt.bias
  models.1._orig_mod.transformer.h.0.attn.projQ.weight
  models.1._orig_mod.transformer.h.0.attn.projK.weight
  models.1._orig_mod.transformer.h.0.attn.projV.weight
  models.1._orig_mod.transformer.h.0.attn.projOut.weight
  models.1._orig_mod.transformer.h.0.mlp.fc.weight
  models.1._orig_mod.transformer.h.0.mlp.proj.weight
  models.1._orig_mod.transformer.h.0.normAtt.weight
  models.1._orig_mod.transformer.h.0.normMlp.weight
  models.1._orig_mod.transformer.h.1.blockLambdas
  models.1._orig_mod.transformer.h.1.attn.lambdas
  models.1._orig_mod.transformer.h.1.attn.projAtt.weight
  models.1._orig_mod.transformer.h.1.attn.projAtt.bias
  models.1._orig_mod.transformer.h.1.attn.projQ.weight
  models.1._orig_mod.transformer.h.1.attn.projK.weight
  models.1._orig_mod.transformer.h.1.attn.projV.weight
  models.1._orig_mod.transformer.h.1.attn.projOut.weight
  models.1._orig_mod.transformer.h.1.mlp.fc.weight
  models.1._orig_mod.transformer.h.1.mlp.proj.weight
  models.1._orig_mod.transformer.h.1.normAtt.weight
  models.1._orig_mod.transformer.h.1.normMlp.weight
  models.1._orig_mod.transformer.h.2.blockLambdas
  models.1._orig_mod.transformer.h.2.attn.lambdas
  models.1._orig_mod.transformer.h.2.attn.projAtt.weight
  models.1._orig_mod.transformer.h.2.attn.projAtt.bias
  models.1._orig_mod.transformer.h.2.attn.projQ.weight
  models.1._orig_mod.transformer.h.2.attn.projK.weight
  models.1._orig_mod.transformer.h.2.attn.projV.weight
  models.1._orig_mod.transformer.h.2.attn.projOut.weight
  models.1._orig_mod.transformer.h.2.mlp.fc.weight
  models.1._orig_mod.transformer.h.2.mlp.proj.weight
  models.1._orig_mod.transformer.h.2.normAtt.weight
  models.1._orig_mod.transformer.h.2.normMlp.weight
  models.1._orig_mod.transformer.h.3.blockLambdas
  models.1._orig_mod.transformer.h.3.attn.lambdas
  models.1._orig_mod.transformer.h.3.attn.projAtt.weight
  models.1._orig_mod.transformer.h.3.attn.projAtt.bias
  models.1._orig_mod.transformer.h.3.attn.projQ.weight
  models.1._orig_mod.transformer.h.3.attn.projK.weight
  models.1._orig_mod.transformer.h.3.attn.projV.weight
  models.1._orig_mod.transformer.h.3.attn.projOut.weight
  models.1._orig_mod.transformer.h.3.mlp.fc.weight
  models.1._orig_mod.transformer.h.3.mlp.proj.weight
  models.1._orig_mod.transformer.h.3.normAtt.weight
  models.1._orig_mod.transformer.h.3.normMlp.weight
  models.1._orig_mod.normWte.weight
  models.1._orig_mod.normWte.bias
Optimizer #2/20: Adam
  Param group 0: lr=0.0006, weight_decay=0.0
  models.2._orig_mod.skipWeights
  models.2._orig_mod.transformer.wte.weight
  models.2._orig_mod.transformer.h.0.blockLambdas
  models.2._orig_mod.transformer.h.0.attn.lambdas
  models.2._orig_mod.transformer.h.0.attn.projAtt.weight
  models.2._orig_mod.transformer.h.0.attn.projAtt.bias
  models.2._orig_mod.transformer.h.0.attn.projQ.weight
  models.2._orig_mod.transformer.h.0.attn.projK.weight
  models.2._orig_mod.transformer.h.0.attn.projV.weight
  models.2._orig_mod.transformer.h.0.attn.projOut.weight
  models.2._orig_mod.transformer.h.0.mlp.fc.weight
  models.2._orig_mod.transformer.h.0.mlp.proj.weight
  models.2._orig_mod.transformer.h.0.normAtt.weight
  models.2._orig_mod.transformer.h.0.normMlp.weight
  models.2._orig_mod.transformer.h.1.blockLambdas
  models.2._orig_mod.transformer.h.1.attn.lambdas
  models.2._orig_mod.transformer.h.1.attn.projAtt.weight
  models.2._orig_mod.transformer.h.1.attn.projAtt.bias
  models.2._orig_mod.transformer.h.1.attn.projQ.weight
  models.2._orig_mod.transformer.h.1.attn.projK.weight
  models.2._orig_mod.transformer.h.1.attn.projV.weight
  models.2._orig_mod.transformer.h.1.attn.projOut.weight
  models.2._orig_mod.transformer.h.1.mlp.fc.weight
  models.2._orig_mod.transformer.h.1.mlp.proj.weight
  models.2._orig_mod.transformer.h.1.normAtt.weight
  models.2._orig_mod.transformer.h.1.normMlp.weight
  models.2._orig_mod.transformer.h.2.blockLambdas
  models.2._orig_mod.transformer.h.2.attn.lambdas
  models.2._orig_mod.transformer.h.2.attn.projAtt.weight
  models.2._orig_mod.transformer.h.2.attn.projAtt.bias
  models.2._orig_mod.transformer.h.2.attn.projQ.weight
  models.2._orig_mod.transformer.h.2.attn.projK.weight
  models.2._orig_mod.transformer.h.2.attn.projV.weight
  models.2._orig_mod.transformer.h.2.attn.projOut.weight
  models.2._orig_mod.transformer.h.2.mlp.fc.weight
  models.2._orig_mod.transformer.h.2.mlp.proj.weight
  models.2._orig_mod.transformer.h.2.normAtt.weight
  models.2._orig_mod.transformer.h.2.normMlp.weight
  models.2._orig_mod.transformer.h.3.blockLambdas
  models.2._orig_mod.transformer.h.3.attn.lambdas
  models.2._orig_mod.transformer.h.3.attn.projAtt.weight
  models.2._orig_mod.transformer.h.3.attn.projAtt.bias
  models.2._orig_mod.transformer.h.3.attn.projQ.weight
  models.2._orig_mod.transformer.h.3.attn.projK.weight
  models.2._orig_mod.transformer.h.3.attn.projV.weight
  models.2._orig_mod.transformer.h.3.attn.projOut.weight
  models.2._orig_mod.transformer.h.3.mlp.fc.weight
  models.2._orig_mod.transformer.h.3.mlp.proj.weight
  models.2._orig_mod.transformer.h.3.normAtt.weight
  models.2._orig_mod.transformer.h.3.normMlp.weight
  models.2._orig_mod.normWte.weight
  models.2._orig_mod.normWte.bias
Optimizer #3/20: Adam
  Param group 0: lr=0.0006, weight_decay=0.0
  models.3._orig_mod.skipWeights
  models.3._orig_mod.transformer.wte.weight
  models.3._orig_mod.transformer.h.0.blockLambdas
  models.3._orig_mod.transformer.h.0.attn.lambdas
  models.3._orig_mod.transformer.h.0.attn.projAtt.weight
  models.3._orig_mod.transformer.h.0.attn.projAtt.bias
  models.3._orig_mod.transformer.h.0.attn.projQ.weight
  models.3._orig_mod.transformer.h.0.attn.projK.weight
  models.3._orig_mod.transformer.h.0.attn.projV.weight
  models.3._orig_mod.transformer.h.0.attn.projOut.weight
  models.3._orig_mod.transformer.h.0.mlp.fc.weight
  models.3._orig_mod.transformer.h.0.mlp.proj.weight
  models.3._orig_mod.transformer.h.0.normAtt.weight
  models.3._orig_mod.transformer.h.0.normMlp.weight
  models.3._orig_mod.transformer.h.1.blockLambdas
  models.3._orig_mod.transformer.h.1.attn.lambdas
  models.3._orig_mod.transformer.h.1.attn.projAtt.weight
  models.3._orig_mod.transformer.h.1.attn.projAtt.bias
  models.3._orig_mod.transformer.h.1.attn.projQ.weight
  models.3._orig_mod.transformer.h.1.attn.projK.weight
  models.3._orig_mod.transformer.h.1.attn.projV.weight
  models.3._orig_mod.transformer.h.1.attn.projOut.weight
  models.3._orig_mod.transformer.h.1.mlp.fc.weight
  models.3._orig_mod.transformer.h.1.mlp.proj.weight
  models.3._orig_mod.transformer.h.1.normAtt.weight
  models.3._orig_mod.transformer.h.1.normMlp.weight
  models.3._orig_mod.transformer.h.2.blockLambdas
  models.3._orig_mod.transformer.h.2.attn.lambdas
  models.3._orig_mod.transformer.h.2.attn.projAtt.weight
  models.3._orig_mod.transformer.h.2.attn.projAtt.bias
  models.3._orig_mod.transformer.h.2.attn.projQ.weight
  models.3._orig_mod.transformer.h.2.attn.projK.weight
  models.3._orig_mod.transformer.h.2.attn.projV.weight
  models.3._orig_mod.transformer.h.2.attn.projOut.weight
  models.3._orig_mod.transformer.h.2.mlp.fc.weight
  models.3._orig_mod.transformer.h.2.mlp.proj.weight
  models.3._orig_mod.transformer.h.2.normAtt.weight
  models.3._orig_mod.transformer.h.2.normMlp.weight
  models.3._orig_mod.transformer.h.3.blockLambdas
  models.3._orig_mod.transformer.h.3.attn.lambdas
  models.3._orig_mod.transformer.h.3.attn.projAtt.weight
  models.3._orig_mod.transformer.h.3.attn.projAtt.bias
  models.3._orig_mod.transformer.h.3.attn.projQ.weight
  models.3._orig_mod.transformer.h.3.attn.projK.weight
  models.3._orig_mod.transformer.h.3.attn.projV.weight
  models.3._orig_mod.transformer.h.3.attn.projOut.weight
  models.3._orig_mod.transformer.h.3.mlp.fc.weight
  models.3._orig_mod.transformer.h.3.mlp.proj.weight
  models.3._orig_mod.transformer.h.3.normAtt.weight
  models.3._orig_mod.transformer.h.3.normMlp.weight
  models.3._orig_mod.normWte.weight
  models.3._orig_mod.normWte.bias
Optimizer #4/20: Adam
  Param group 0: lr=0.0006, weight_decay=0.0
  models.4._orig_mod.skipWeights
  models.4._orig_mod.transformer.wte.weight
  models.4._orig_mod.transformer.h.0.blockLambdas
  models.4._orig_mod.transformer.h.0.attn.lambdas
  models.4._orig_mod.transformer.h.0.attn.projAtt.weight
  models.4._orig_mod.transformer.h.0.attn.projAtt.bias
  models.4._orig_mod.transformer.h.0.attn.projQ.weight
  models.4._orig_mod.transformer.h.0.attn.projK.weight
  models.4._orig_mod.transformer.h.0.attn.projV.weight
  models.4._orig_mod.transformer.h.0.attn.projOut.weight
  models.4._orig_mod.transformer.h.0.mlp.fc.weight
  models.4._orig_mod.transformer.h.0.mlp.proj.weight
  models.4._orig_mod.transformer.h.0.normAtt.weight
  models.4._orig_mod.transformer.h.0.normMlp.weight
  models.4._orig_mod.transformer.h.1.blockLambdas
  models.4._orig_mod.transformer.h.1.attn.lambdas
  models.4._orig_mod.transformer.h.1.attn.projAtt.weight
  models.4._orig_mod.transformer.h.1.attn.projAtt.bias
  models.4._orig_mod.transformer.h.1.attn.projQ.weight
  models.4._orig_mod.transformer.h.1.attn.projK.weight
  models.4._orig_mod.transformer.h.1.attn.projV.weight
  models.4._orig_mod.transformer.h.1.attn.projOut.weight
  models.4._orig_mod.transformer.h.1.mlp.fc.weight
  models.4._orig_mod.transformer.h.1.mlp.proj.weight
  models.4._orig_mod.transformer.h.1.normAtt.weight
  models.4._orig_mod.transformer.h.1.normMlp.weight
  models.4._orig_mod.transformer.h.2.blockLambdas
  models.4._orig_mod.transformer.h.2.attn.lambdas
  models.4._orig_mod.transformer.h.2.attn.projAtt.weight
  models.4._orig_mod.transformer.h.2.attn.projAtt.bias
  models.4._orig_mod.transformer.h.2.attn.projQ.weight
  models.4._orig_mod.transformer.h.2.attn.projK.weight
  models.4._orig_mod.transformer.h.2.attn.projV.weight
  models.4._orig_mod.transformer.h.2.attn.projOut.weight
  models.4._orig_mod.transformer.h.2.mlp.fc.weight
  models.4._orig_mod.transformer.h.2.mlp.proj.weight
  models.4._orig_mod.transformer.h.2.normAtt.weight
  models.4._orig_mod.transformer.h.2.normMlp.weight
  models.4._orig_mod.transformer.h.3.blockLambdas
  models.4._orig_mod.transformer.h.3.attn.lambdas
  models.4._orig_mod.transformer.h.3.attn.projAtt.weight
  models.4._orig_mod.transformer.h.3.attn.projAtt.bias
  models.4._orig_mod.transformer.h.3.attn.projQ.weight
  models.4._orig_mod.transformer.h.3.attn.projK.weight
  models.4._orig_mod.transformer.h.3.attn.projV.weight
  models.4._orig_mod.transformer.h.3.attn.projOut.weight
  models.4._orig_mod.transformer.h.3.mlp.fc.weight
  models.4._orig_mod.transformer.h.3.mlp.proj.weight
  models.4._orig_mod.transformer.h.3.normAtt.weight
  models.4._orig_mod.transformer.h.3.normMlp.weight
  models.4._orig_mod.normWte.weight
  models.4._orig_mod.normWte.bias
Optimizer #5/20: Adam
  Param group 0: lr=0.0006, weight_decay=0.0
  models.5._orig_mod.skipWeights
  models.5._orig_mod.transformer.wte.weight
  models.5._orig_mod.transformer.h.0.blockLambdas
  models.5._orig_mod.transformer.h.0.attn.lambdas
  models.5._orig_mod.transformer.h.0.attn.projAtt.weight
  models.5._orig_mod.transformer.h.0.attn.projAtt.bias
  models.5._orig_mod.transformer.h.0.attn.projQ.weight
  models.5._orig_mod.transformer.h.0.attn.projK.weight
  models.5._orig_mod.transformer.h.0.attn.projV.weight
  models.5._orig_mod.transformer.h.0.attn.projOut.weight
  models.5._orig_mod.transformer.h.0.mlp.fc.weight
  models.5._orig_mod.transformer.h.0.mlp.proj.weight
  models.5._orig_mod.transformer.h.0.normAtt.weight
  models.5._orig_mod.transformer.h.0.normMlp.weight
  models.5._orig_mod.transformer.h.1.blockLambdas
  models.5._orig_mod.transformer.h.1.attn.lambdas
  models.5._orig_mod.transformer.h.1.attn.projAtt.weight
  models.5._orig_mod.transformer.h.1.attn.projAtt.bias
  models.5._orig_mod.transformer.h.1.attn.projQ.weight
  models.5._orig_mod.transformer.h.1.attn.projK.weight
  models.5._orig_mod.transformer.h.1.attn.projV.weight
  models.5._orig_mod.transformer.h.1.attn.projOut.weight
  models.5._orig_mod.transformer.h.1.mlp.fc.weight
  models.5._orig_mod.transformer.h.1.mlp.proj.weight
  models.5._orig_mod.transformer.h.1.normAtt.weight
  models.5._orig_mod.transformer.h.1.normMlp.weight
  models.5._orig_mod.transformer.h.2.blockLambdas
  models.5._orig_mod.transformer.h.2.attn.lambdas
  models.5._orig_mod.transformer.h.2.attn.projAtt.weight
  models.5._orig_mod.transformer.h.2.attn.projAtt.bias
  models.5._orig_mod.transformer.h.2.attn.projQ.weight
  models.5._orig_mod.transformer.h.2.attn.projK.weight
  models.5._orig_mod.transformer.h.2.attn.projV.weight
  models.5._orig_mod.transformer.h.2.attn.projOut.weight
  models.5._orig_mod.transformer.h.2.mlp.fc.weight
  models.5._orig_mod.transformer.h.2.mlp.proj.weight
  models.5._orig_mod.transformer.h.2.normAtt.weight
  models.5._orig_mod.transformer.h.2.normMlp.weight
  models.5._orig_mod.transformer.h.3.blockLambdas
  models.5._orig_mod.transformer.h.3.attn.lambdas
  models.5._orig_mod.transformer.h.3.attn.projAtt.weight
  models.5._orig_mod.transformer.h.3.attn.projAtt.bias
  models.5._orig_mod.transformer.h.3.attn.projQ.weight
  models.5._orig_mod.transformer.h.3.attn.projK.weight
  models.5._orig_mod.transformer.h.3.attn.projV.weight
  models.5._orig_mod.transformer.h.3.attn.projOut.weight
  models.5._orig_mod.transformer.h.3.mlp.fc.weight
  models.5._orig_mod.transformer.h.3.mlp.proj.weight
  models.5._orig_mod.transformer.h.3.normAtt.weight
  models.5._orig_mod.transformer.h.3.normMlp.weight
  models.5._orig_mod.normWte.weight
  models.5._orig_mod.normWte.bias
Optimizer #6/20: Adam
  Param group 0: lr=0.0006, weight_decay=0.0
  models.6._orig_mod.skipWeights
  models.6._orig_mod.transformer.wte.weight
  models.6._orig_mod.transformer.h.0.blockLambdas
  models.6._orig_mod.transformer.h.0.attn.lambdas
  models.6._orig_mod.transformer.h.0.attn.projAtt.weight
  models.6._orig_mod.transformer.h.0.attn.projAtt.bias
  models.6._orig_mod.transformer.h.0.attn.projQ.weight
  models.6._orig_mod.transformer.h.0.attn.projK.weight
  models.6._orig_mod.transformer.h.0.attn.projV.weight
  models.6._orig_mod.transformer.h.0.attn.projOut.weight
  models.6._orig_mod.transformer.h.0.mlp.fc.weight
  models.6._orig_mod.transformer.h.0.mlp.proj.weight
  models.6._orig_mod.transformer.h.0.normAtt.weight
  models.6._orig_mod.transformer.h.0.normMlp.weight
  models.6._orig_mod.transformer.h.1.blockLambdas
  models.6._orig_mod.transformer.h.1.attn.lambdas
  models.6._orig_mod.transformer.h.1.attn.projAtt.weight
  models.6._orig_mod.transformer.h.1.attn.projAtt.bias
  models.6._orig_mod.transformer.h.1.attn.projQ.weight
  models.6._orig_mod.transformer.h.1.attn.projK.weight
  models.6._orig_mod.transformer.h.1.attn.projV.weight
  models.6._orig_mod.transformer.h.1.attn.projOut.weight
  models.6._orig_mod.transformer.h.1.mlp.fc.weight
  models.6._orig_mod.transformer.h.1.mlp.proj.weight
  models.6._orig_mod.transformer.h.1.normAtt.weight
  models.6._orig_mod.transformer.h.1.normMlp.weight
  models.6._orig_mod.transformer.h.2.blockLambdas
  models.6._orig_mod.transformer.h.2.attn.lambdas
  models.6._orig_mod.transformer.h.2.attn.projAtt.weight
  models.6._orig_mod.transformer.h.2.attn.projAtt.bias
  models.6._orig_mod.transformer.h.2.attn.projQ.weight
  models.6._orig_mod.transformer.h.2.attn.projK.weight
  models.6._orig_mod.transformer.h.2.attn.projV.weight
  models.6._orig_mod.transformer.h.2.attn.projOut.weight
  models.6._orig_mod.transformer.h.2.mlp.fc.weight
  models.6._orig_mod.transformer.h.2.mlp.proj.weight
  models.6._orig_mod.transformer.h.2.normAtt.weight
  models.6._orig_mod.transformer.h.2.normMlp.weight
  models.6._orig_mod.transformer.h.3.blockLambdas
  models.6._orig_mod.transformer.h.3.attn.lambdas
  models.6._orig_mod.transformer.h.3.attn.projAtt.weight
  models.6._orig_mod.transformer.h.3.attn.projAtt.bias
  models.6._orig_mod.transformer.h.3.attn.projQ.weight
  models.6._orig_mod.transformer.h.3.attn.projK.weight
  models.6._orig_mod.transformer.h.3.attn.projV.weight
  models.6._orig_mod.transformer.h.3.attn.projOut.weight
  models.6._orig_mod.transformer.h.3.mlp.fc.weight
  models.6._orig_mod.transformer.h.3.mlp.proj.weight
  models.6._orig_mod.transformer.h.3.normAtt.weight
  models.6._orig_mod.transformer.h.3.normMlp.weight
  models.6._orig_mod.normWte.weight
  models.6._orig_mod.normWte.bias
Optimizer #7/20: Adam
  Param group 0: lr=0.0006, weight_decay=0.0
  models.7._orig_mod.skipWeights
  models.7._orig_mod.transformer.wte.weight
  models.7._orig_mod.transformer.h.0.blockLambdas
  models.7._orig_mod.transformer.h.0.attn.lambdas
  models.7._orig_mod.transformer.h.0.attn.projAtt.weight
  models.7._orig_mod.transformer.h.0.attn.projAtt.bias
  models.7._orig_mod.transformer.h.0.attn.projQ.weight
  models.7._orig_mod.transformer.h.0.attn.projK.weight
  models.7._orig_mod.transformer.h.0.attn.projV.weight
  models.7._orig_mod.transformer.h.0.attn.projOut.weight
  models.7._orig_mod.transformer.h.0.mlp.fc.weight
  models.7._orig_mod.transformer.h.0.mlp.proj.weight
  models.7._orig_mod.transformer.h.0.normAtt.weight
  models.7._orig_mod.transformer.h.0.normMlp.weight
  models.7._orig_mod.transformer.h.1.blockLambdas
  models.7._orig_mod.transformer.h.1.attn.lambdas
  models.7._orig_mod.transformer.h.1.attn.projAtt.weight
  models.7._orig_mod.transformer.h.1.attn.projAtt.bias
  models.7._orig_mod.transformer.h.1.attn.projQ.weight
  models.7._orig_mod.transformer.h.1.attn.projK.weight
  models.7._orig_mod.transformer.h.1.attn.projV.weight
  models.7._orig_mod.transformer.h.1.attn.projOut.weight
  models.7._orig_mod.transformer.h.1.mlp.fc.weight
  models.7._orig_mod.transformer.h.1.mlp.proj.weight
  models.7._orig_mod.transformer.h.1.normAtt.weight
  models.7._orig_mod.transformer.h.1.normMlp.weight
  models.7._orig_mod.transformer.h.2.blockLambdas
  models.7._orig_mod.transformer.h.2.attn.lambdas
  models.7._orig_mod.transformer.h.2.attn.projAtt.weight
  models.7._orig_mod.transformer.h.2.attn.projAtt.bias
  models.7._orig_mod.transformer.h.2.attn.projQ.weight
  models.7._orig_mod.transformer.h.2.attn.projK.weight
  models.7._orig_mod.transformer.h.2.attn.projV.weight
  models.7._orig_mod.transformer.h.2.attn.projOut.weight
  models.7._orig_mod.transformer.h.2.mlp.fc.weight
  models.7._orig_mod.transformer.h.2.mlp.proj.weight
  models.7._orig_mod.transformer.h.2.normAtt.weight
  models.7._orig_mod.transformer.h.2.normMlp.weight
  models.7._orig_mod.transformer.h.3.blockLambdas
  models.7._orig_mod.transformer.h.3.attn.lambdas
  models.7._orig_mod.transformer.h.3.attn.projAtt.weight
  models.7._orig_mod.transformer.h.3.attn.projAtt.bias
  models.7._orig_mod.transformer.h.3.attn.projQ.weight
  models.7._orig_mod.transformer.h.3.attn.projK.weight
  models.7._orig_mod.transformer.h.3.attn.projV.weight
  models.7._orig_mod.transformer.h.3.attn.projOut.weight
  models.7._orig_mod.transformer.h.3.mlp.fc.weight
  models.7._orig_mod.transformer.h.3.mlp.proj.weight
  models.7._orig_mod.transformer.h.3.normAtt.weight
  models.7._orig_mod.transformer.h.3.normMlp.weight
  models.7._orig_mod.normWte.weight
  models.7._orig_mod.normWte.bias
Optimizer #8/20: Adam
  Param group 0: lr=0.0006, weight_decay=0.0
  models.8._orig_mod.skipWeights
  models.8._orig_mod.transformer.wte.weight
  models.8._orig_mod.transformer.h.0.blockLambdas
  models.8._orig_mod.transformer.h.0.attn.lambdas
  models.8._orig_mod.transformer.h.0.attn.projAtt.weight
  models.8._orig_mod.transformer.h.0.attn.projAtt.bias
  models.8._orig_mod.transformer.h.0.attn.projQ.weight
  models.8._orig_mod.transformer.h.0.attn.projK.weight
  models.8._orig_mod.transformer.h.0.attn.projV.weight
  models.8._orig_mod.transformer.h.0.attn.projOut.weight
  models.8._orig_mod.transformer.h.0.mlp.fc.weight
  models.8._orig_mod.transformer.h.0.mlp.proj.weight
  models.8._orig_mod.transformer.h.0.normAtt.weight
  models.8._orig_mod.transformer.h.0.normMlp.weight
  models.8._orig_mod.transformer.h.1.blockLambdas
  models.8._orig_mod.transformer.h.1.attn.lambdas
  models.8._orig_mod.transformer.h.1.attn.projAtt.weight
  models.8._orig_mod.transformer.h.1.attn.projAtt.bias
  models.8._orig_mod.transformer.h.1.attn.projQ.weight
  models.8._orig_mod.transformer.h.1.attn.projK.weight
  models.8._orig_mod.transformer.h.1.attn.projV.weight
  models.8._orig_mod.transformer.h.1.attn.projOut.weight
  models.8._orig_mod.transformer.h.1.mlp.fc.weight
  models.8._orig_mod.transformer.h.1.mlp.proj.weight
  models.8._orig_mod.transformer.h.1.normAtt.weight
  models.8._orig_mod.transformer.h.1.normMlp.weight
  models.8._orig_mod.transformer.h.2.blockLambdas
  models.8._orig_mod.transformer.h.2.attn.lambdas
  models.8._orig_mod.transformer.h.2.attn.projAtt.weight
  models.8._orig_mod.transformer.h.2.attn.projAtt.bias
  models.8._orig_mod.transformer.h.2.attn.projQ.weight
  models.8._orig_mod.transformer.h.2.attn.projK.weight
  models.8._orig_mod.transformer.h.2.attn.projV.weight
  models.8._orig_mod.transformer.h.2.attn.projOut.weight
  models.8._orig_mod.transformer.h.2.mlp.fc.weight
  models.8._orig_mod.transformer.h.2.mlp.proj.weight
  models.8._orig_mod.transformer.h.2.normAtt.weight
  models.8._orig_mod.transformer.h.2.normMlp.weight
  models.8._orig_mod.transformer.h.3.blockLambdas
  models.8._orig_mod.transformer.h.3.attn.lambdas
  models.8._orig_mod.transformer.h.3.attn.projAtt.weight
  models.8._orig_mod.transformer.h.3.attn.projAtt.bias
  models.8._orig_mod.transformer.h.3.attn.projQ.weight
  models.8._orig_mod.transformer.h.3.attn.projK.weight
  models.8._orig_mod.transformer.h.3.attn.projV.weight
  models.8._orig_mod.transformer.h.3.attn.projOut.weight
  models.8._orig_mod.transformer.h.3.mlp.fc.weight
  models.8._orig_mod.transformer.h.3.mlp.proj.weight
  models.8._orig_mod.transformer.h.3.normAtt.weight
  models.8._orig_mod.transformer.h.3.normMlp.weight
  models.8._orig_mod.normWte.weight
  models.8._orig_mod.normWte.bias
Optimizer #9/20: Adam
  Param group 0: lr=0.0006, weight_decay=0.0
  models.9._orig_mod.skipWeights
  models.9._orig_mod.transformer.wte.weight
  models.9._orig_mod.transformer.h.0.blockLambdas
  models.9._orig_mod.transformer.h.0.attn.lambdas
  models.9._orig_mod.transformer.h.0.attn.projAtt.weight
  models.9._orig_mod.transformer.h.0.attn.projAtt.bias
  models.9._orig_mod.transformer.h.0.attn.projQ.weight
  models.9._orig_mod.transformer.h.0.attn.projK.weight
  models.9._orig_mod.transformer.h.0.attn.projV.weight
  models.9._orig_mod.transformer.h.0.attn.projOut.weight
  models.9._orig_mod.transformer.h.0.mlp.fc.weight
  models.9._orig_mod.transformer.h.0.mlp.proj.weight
  models.9._orig_mod.transformer.h.0.normAtt.weight
  models.9._orig_mod.transformer.h.0.normMlp.weight
  models.9._orig_mod.transformer.h.1.blockLambdas
  models.9._orig_mod.transformer.h.1.attn.lambdas
  models.9._orig_mod.transformer.h.1.attn.projAtt.weight
  models.9._orig_mod.transformer.h.1.attn.projAtt.bias
  models.9._orig_mod.transformer.h.1.attn.projQ.weight
  models.9._orig_mod.transformer.h.1.attn.projK.weight
  models.9._orig_mod.transformer.h.1.attn.projV.weight
  models.9._orig_mod.transformer.h.1.attn.projOut.weight
  models.9._orig_mod.transformer.h.1.mlp.fc.weight
  models.9._orig_mod.transformer.h.1.mlp.proj.weight
  models.9._orig_mod.transformer.h.1.normAtt.weight
  models.9._orig_mod.transformer.h.1.normMlp.weight
  models.9._orig_mod.transformer.h.2.blockLambdas
  models.9._orig_mod.transformer.h.2.attn.lambdas
  models.9._orig_mod.transformer.h.2.attn.projAtt.weight
  models.9._orig_mod.transformer.h.2.attn.projAtt.bias
  models.9._orig_mod.transformer.h.2.attn.projQ.weight
  models.9._orig_mod.transformer.h.2.attn.projK.weight
  models.9._orig_mod.transformer.h.2.attn.projV.weight
  models.9._orig_mod.transformer.h.2.attn.projOut.weight
  models.9._orig_mod.transformer.h.2.mlp.fc.weight
  models.9._orig_mod.transformer.h.2.mlp.proj.weight
  models.9._orig_mod.transformer.h.2.normAtt.weight
  models.9._orig_mod.transformer.h.2.normMlp.weight
  models.9._orig_mod.transformer.h.3.blockLambdas
  models.9._orig_mod.transformer.h.3.attn.lambdas
  models.9._orig_mod.transformer.h.3.attn.projAtt.weight
  models.9._orig_mod.transformer.h.3.attn.projAtt.bias
  models.9._orig_mod.transformer.h.3.attn.projQ.weight
  models.9._orig_mod.transformer.h.3.attn.projK.weight
  models.9._orig_mod.transformer.h.3.attn.projV.weight
  models.9._orig_mod.transformer.h.3.attn.projOut.weight
  models.9._orig_mod.transformer.h.3.mlp.fc.weight
  models.9._orig_mod.transformer.h.3.mlp.proj.weight
  models.9._orig_mod.transformer.h.3.normAtt.weight
  models.9._orig_mod.transformer.h.3.normMlp.weight
  models.9._orig_mod.normWte.weight
  models.9._orig_mod.normWte.bias
Optimizer #10/20: Adam
  Param group 0: lr=0.0006, weight_decay=0.0
  models.10._orig_mod.skipWeights
  models.10._orig_mod.transformer.wte.weight
  models.10._orig_mod.transformer.h.0.blockLambdas
  models.10._orig_mod.transformer.h.0.attn.lambdas
  models.10._orig_mod.transformer.h.0.attn.projAtt.weight
  models.10._orig_mod.transformer.h.0.attn.projAtt.bias
  models.10._orig_mod.transformer.h.0.attn.projQ.weight
  models.10._orig_mod.transformer.h.0.attn.projK.weight
  models.10._orig_mod.transformer.h.0.attn.projV.weight
  models.10._orig_mod.transformer.h.0.attn.projOut.weight
  models.10._orig_mod.transformer.h.0.mlp.fc.weight
  models.10._orig_mod.transformer.h.0.mlp.proj.weight
  models.10._orig_mod.transformer.h.0.normAtt.weight
  models.10._orig_mod.transformer.h.0.normMlp.weight
  models.10._orig_mod.transformer.h.1.blockLambdas
  models.10._orig_mod.transformer.h.1.attn.lambdas
  models.10._orig_mod.transformer.h.1.attn.projAtt.weight
  models.10._orig_mod.transformer.h.1.attn.projAtt.bias
  models.10._orig_mod.transformer.h.1.attn.projQ.weight
  models.10._orig_mod.transformer.h.1.attn.projK.weight
  models.10._orig_mod.transformer.h.1.attn.projV.weight
  models.10._orig_mod.transformer.h.1.attn.projOut.weight
  models.10._orig_mod.transformer.h.1.mlp.fc.weight
  models.10._orig_mod.transformer.h.1.mlp.proj.weight
  models.10._orig_mod.transformer.h.1.normAtt.weight
  models.10._orig_mod.transformer.h.1.normMlp.weight
  models.10._orig_mod.transformer.h.2.blockLambdas
  models.10._orig_mod.transformer.h.2.attn.lambdas
  models.10._orig_mod.transformer.h.2.attn.projAtt.weight
  models.10._orig_mod.transformer.h.2.attn.projAtt.bias
  models.10._orig_mod.transformer.h.2.attn.projQ.weight
  models.10._orig_mod.transformer.h.2.attn.projK.weight
  models.10._orig_mod.transformer.h.2.attn.projV.weight
  models.10._orig_mod.transformer.h.2.attn.projOut.weight
  models.10._orig_mod.transformer.h.2.mlp.fc.weight
  models.10._orig_mod.transformer.h.2.mlp.proj.weight
  models.10._orig_mod.transformer.h.2.normAtt.weight
  models.10._orig_mod.transformer.h.2.normMlp.weight
  models.10._orig_mod.transformer.h.3.blockLambdas
  models.10._orig_mod.transformer.h.3.attn.lambdas
  models.10._orig_mod.transformer.h.3.attn.projAtt.weight
  models.10._orig_mod.transformer.h.3.attn.projAtt.bias
  models.10._orig_mod.transformer.h.3.attn.projQ.weight
  models.10._orig_mod.transformer.h.3.attn.projK.weight
  models.10._orig_mod.transformer.h.3.attn.projV.weight
  models.10._orig_mod.transformer.h.3.attn.projOut.weight
  models.10._orig_mod.transformer.h.3.mlp.fc.weight
  models.10._orig_mod.transformer.h.3.mlp.proj.weight
  models.10._orig_mod.transformer.h.3.normAtt.weight
  models.10._orig_mod.transformer.h.3.normMlp.weight
  models.10._orig_mod.normWte.weight
  models.10._orig_mod.normWte.bias
Optimizer #11/20: Adam
  Param group 0: lr=0.0006, weight_decay=0.0
  models.11._orig_mod.skipWeights
  models.11._orig_mod.transformer.wte.weight
  models.11._orig_mod.transformer.h.0.blockLambdas
  models.11._orig_mod.transformer.h.0.attn.lambdas
  models.11._orig_mod.transformer.h.0.attn.projAtt.weight
  models.11._orig_mod.transformer.h.0.attn.projAtt.bias
  models.11._orig_mod.transformer.h.0.attn.projQ.weight
  models.11._orig_mod.transformer.h.0.attn.projK.weight
  models.11._orig_mod.transformer.h.0.attn.projV.weight
  models.11._orig_mod.transformer.h.0.attn.projOut.weight
  models.11._orig_mod.transformer.h.0.mlp.fc.weight
  models.11._orig_mod.transformer.h.0.mlp.proj.weight
  models.11._orig_mod.transformer.h.0.normAtt.weight
  models.11._orig_mod.transformer.h.0.normMlp.weight
  models.11._orig_mod.transformer.h.1.blockLambdas
  models.11._orig_mod.transformer.h.1.attn.lambdas
  models.11._orig_mod.transformer.h.1.attn.projAtt.weight
  models.11._orig_mod.transformer.h.1.attn.projAtt.bias
  models.11._orig_mod.transformer.h.1.attn.projQ.weight
  models.11._orig_mod.transformer.h.1.attn.projK.weight
  models.11._orig_mod.transformer.h.1.attn.projV.weight
  models.11._orig_mod.transformer.h.1.attn.projOut.weight
  models.11._orig_mod.transformer.h.1.mlp.fc.weight
  models.11._orig_mod.transformer.h.1.mlp.proj.weight
  models.11._orig_mod.transformer.h.1.normAtt.weight
  models.11._orig_mod.transformer.h.1.normMlp.weight
  models.11._orig_mod.transformer.h.2.blockLambdas
  models.11._orig_mod.transformer.h.2.attn.lambdas
  models.11._orig_mod.transformer.h.2.attn.projAtt.weight
  models.11._orig_mod.transformer.h.2.attn.projAtt.bias
  models.11._orig_mod.transformer.h.2.attn.projQ.weight
  models.11._orig_mod.transformer.h.2.attn.projK.weight
  models.11._orig_mod.transformer.h.2.attn.projV.weight
  models.11._orig_mod.transformer.h.2.attn.projOut.weight
  models.11._orig_mod.transformer.h.2.mlp.fc.weight
  models.11._orig_mod.transformer.h.2.mlp.proj.weight
  models.11._orig_mod.transformer.h.2.normAtt.weight
  models.11._orig_mod.transformer.h.2.normMlp.weight
  models.11._orig_mod.transformer.h.3.blockLambdas
  models.11._orig_mod.transformer.h.3.attn.lambdas
  models.11._orig_mod.transformer.h.3.attn.projAtt.weight
  models.11._orig_mod.transformer.h.3.attn.projAtt.bias
  models.11._orig_mod.transformer.h.3.attn.projQ.weight
  models.11._orig_mod.transformer.h.3.attn.projK.weight
  models.11._orig_mod.transformer.h.3.attn.projV.weight
  models.11._orig_mod.transformer.h.3.attn.projOut.weight
  models.11._orig_mod.transformer.h.3.mlp.fc.weight
  models.11._orig_mod.transformer.h.3.mlp.proj.weight
  models.11._orig_mod.transformer.h.3.normAtt.weight
  models.11._orig_mod.transformer.h.3.normMlp.weight
  models.11._orig_mod.normWte.weight
  models.11._orig_mod.normWte.bias
Optimizer #12/20: Adam
  Param group 0: lr=0.0006, weight_decay=0.0
  models.12._orig_mod.skipWeights
  models.12._orig_mod.transformer.wte.weight
  models.12._orig_mod.transformer.h.0.blockLambdas
  models.12._orig_mod.transformer.h.0.attn.lambdas
  models.12._orig_mod.transformer.h.0.attn.projAtt.weight
  models.12._orig_mod.transformer.h.0.attn.projAtt.bias
  models.12._orig_mod.transformer.h.0.attn.projQ.weight
  models.12._orig_mod.transformer.h.0.attn.projK.weight
  models.12._orig_mod.transformer.h.0.attn.projV.weight
  models.12._orig_mod.transformer.h.0.attn.projOut.weight
  models.12._orig_mod.transformer.h.0.mlp.fc.weight
  models.12._orig_mod.transformer.h.0.mlp.proj.weight
  models.12._orig_mod.transformer.h.0.normAtt.weight
  models.12._orig_mod.transformer.h.0.normMlp.weight
  models.12._orig_mod.transformer.h.1.blockLambdas
  models.12._orig_mod.transformer.h.1.attn.lambdas
  models.12._orig_mod.transformer.h.1.attn.projAtt.weight
  models.12._orig_mod.transformer.h.1.attn.projAtt.bias
  models.12._orig_mod.transformer.h.1.attn.projQ.weight
  models.12._orig_mod.transformer.h.1.attn.projK.weight
  models.12._orig_mod.transformer.h.1.attn.projV.weight
  models.12._orig_mod.transformer.h.1.attn.projOut.weight
  models.12._orig_mod.transformer.h.1.mlp.fc.weight
  models.12._orig_mod.transformer.h.1.mlp.proj.weight
  models.12._orig_mod.transformer.h.1.normAtt.weight
  models.12._orig_mod.transformer.h.1.normMlp.weight
  models.12._orig_mod.transformer.h.2.blockLambdas
  models.12._orig_mod.transformer.h.2.attn.lambdas
  models.12._orig_mod.transformer.h.2.attn.projAtt.weight
  models.12._orig_mod.transformer.h.2.attn.projAtt.bias
  models.12._orig_mod.transformer.h.2.attn.projQ.weight
  models.12._orig_mod.transformer.h.2.attn.projK.weight
  models.12._orig_mod.transformer.h.2.attn.projV.weight
  models.12._orig_mod.transformer.h.2.attn.projOut.weight
  models.12._orig_mod.transformer.h.2.mlp.fc.weight
  models.12._orig_mod.transformer.h.2.mlp.proj.weight
  models.12._orig_mod.transformer.h.2.normAtt.weight
  models.12._orig_mod.transformer.h.2.normMlp.weight
  models.12._orig_mod.transformer.h.3.blockLambdas
  models.12._orig_mod.transformer.h.3.attn.lambdas
  models.12._orig_mod.transformer.h.3.attn.projAtt.weight
  models.12._orig_mod.transformer.h.3.attn.projAtt.bias
  models.12._orig_mod.transformer.h.3.attn.projQ.weight
  models.12._orig_mod.transformer.h.3.attn.projK.weight
  models.12._orig_mod.transformer.h.3.attn.projV.weight
  models.12._orig_mod.transformer.h.3.attn.projOut.weight
  models.12._orig_mod.transformer.h.3.mlp.fc.weight
  models.12._orig_mod.transformer.h.3.mlp.proj.weight
  models.12._orig_mod.transformer.h.3.normAtt.weight
  models.12._orig_mod.transformer.h.3.normMlp.weight
  models.12._orig_mod.normWte.weight
  models.12._orig_mod.normWte.bias
Optimizer #13/20: Adam
  Param group 0: lr=0.0006, weight_decay=0.0
  models.13._orig_mod.skipWeights
  models.13._orig_mod.transformer.wte.weight
  models.13._orig_mod.transformer.h.0.blockLambdas
  models.13._orig_mod.transformer.h.0.attn.lambdas
  models.13._orig_mod.transformer.h.0.attn.projAtt.weight
  models.13._orig_mod.transformer.h.0.attn.projAtt.bias
  models.13._orig_mod.transformer.h.0.attn.projQ.weight
  models.13._orig_mod.transformer.h.0.attn.projK.weight
  models.13._orig_mod.transformer.h.0.attn.projV.weight
  models.13._orig_mod.transformer.h.0.attn.projOut.weight
  models.13._orig_mod.transformer.h.0.mlp.fc.weight
  models.13._orig_mod.transformer.h.0.mlp.proj.weight
  models.13._orig_mod.transformer.h.0.normAtt.weight
  models.13._orig_mod.transformer.h.0.normMlp.weight
  models.13._orig_mod.transformer.h.1.blockLambdas
  models.13._orig_mod.transformer.h.1.attn.lambdas
  models.13._orig_mod.transformer.h.1.attn.projAtt.weight
  models.13._orig_mod.transformer.h.1.attn.projAtt.bias
  models.13._orig_mod.transformer.h.1.attn.projQ.weight
  models.13._orig_mod.transformer.h.1.attn.projK.weight
  models.13._orig_mod.transformer.h.1.attn.projV.weight
  models.13._orig_mod.transformer.h.1.attn.projOut.weight
  models.13._orig_mod.transformer.h.1.mlp.fc.weight
  models.13._orig_mod.transformer.h.1.mlp.proj.weight
  models.13._orig_mod.transformer.h.1.normAtt.weight
  models.13._orig_mod.transformer.h.1.normMlp.weight
  models.13._orig_mod.transformer.h.2.blockLambdas
  models.13._orig_mod.transformer.h.2.attn.lambdas
  models.13._orig_mod.transformer.h.2.attn.projAtt.weight
  models.13._orig_mod.transformer.h.2.attn.projAtt.bias
  models.13._orig_mod.transformer.h.2.attn.projQ.weight
  models.13._orig_mod.transformer.h.2.attn.projK.weight
  models.13._orig_mod.transformer.h.2.attn.projV.weight
  models.13._orig_mod.transformer.h.2.attn.projOut.weight
  models.13._orig_mod.transformer.h.2.mlp.fc.weight
  models.13._orig_mod.transformer.h.2.mlp.proj.weight
  models.13._orig_mod.transformer.h.2.normAtt.weight
  models.13._orig_mod.transformer.h.2.normMlp.weight
  models.13._orig_mod.transformer.h.3.blockLambdas
  models.13._orig_mod.transformer.h.3.attn.lambdas
  models.13._orig_mod.transformer.h.3.attn.projAtt.weight
  models.13._orig_mod.transformer.h.3.attn.projAtt.bias
  models.13._orig_mod.transformer.h.3.attn.projQ.weight
  models.13._orig_mod.transformer.h.3.attn.projK.weight
  models.13._orig_mod.transformer.h.3.attn.projV.weight
  models.13._orig_mod.transformer.h.3.attn.projOut.weight
  models.13._orig_mod.transformer.h.3.mlp.fc.weight
  models.13._orig_mod.transformer.h.3.mlp.proj.weight
  models.13._orig_mod.transformer.h.3.normAtt.weight
  models.13._orig_mod.transformer.h.3.normMlp.weight
  models.13._orig_mod.normWte.weight
  models.13._orig_mod.normWte.bias
Optimizer #14/20: Adam
  Param group 0: lr=0.0006, weight_decay=0.0
  models.14._orig_mod.skipWeights
  models.14._orig_mod.transformer.wte.weight
  models.14._orig_mod.transformer.h.0.blockLambdas
  models.14._orig_mod.transformer.h.0.attn.lambdas
  models.14._orig_mod.transformer.h.0.attn.projAtt.weight
  models.14._orig_mod.transformer.h.0.attn.projAtt.bias
  models.14._orig_mod.transformer.h.0.attn.projQ.weight
  models.14._orig_mod.transformer.h.0.attn.projK.weight
  models.14._orig_mod.transformer.h.0.attn.projV.weight
  models.14._orig_mod.transformer.h.0.attn.projOut.weight
  models.14._orig_mod.transformer.h.0.mlp.fc.weight
  models.14._orig_mod.transformer.h.0.mlp.proj.weight
  models.14._orig_mod.transformer.h.0.normAtt.weight
  models.14._orig_mod.transformer.h.0.normMlp.weight
  models.14._orig_mod.transformer.h.1.blockLambdas
  models.14._orig_mod.transformer.h.1.attn.lambdas
  models.14._orig_mod.transformer.h.1.attn.projAtt.weight
  models.14._orig_mod.transformer.h.1.attn.projAtt.bias
  models.14._orig_mod.transformer.h.1.attn.projQ.weight
  models.14._orig_mod.transformer.h.1.attn.projK.weight
  models.14._orig_mod.transformer.h.1.attn.projV.weight
  models.14._orig_mod.transformer.h.1.attn.projOut.weight
  models.14._orig_mod.transformer.h.1.mlp.fc.weight
  models.14._orig_mod.transformer.h.1.mlp.proj.weight
  models.14._orig_mod.transformer.h.1.normAtt.weight
  models.14._orig_mod.transformer.h.1.normMlp.weight
  models.14._orig_mod.transformer.h.2.blockLambdas
  models.14._orig_mod.transformer.h.2.attn.lambdas
  models.14._orig_mod.transformer.h.2.attn.projAtt.weight
  models.14._orig_mod.transformer.h.2.attn.projAtt.bias
  models.14._orig_mod.transformer.h.2.attn.projQ.weight
  models.14._orig_mod.transformer.h.2.attn.projK.weight
  models.14._orig_mod.transformer.h.2.attn.projV.weight
  models.14._orig_mod.transformer.h.2.attn.projOut.weight
  models.14._orig_mod.transformer.h.2.mlp.fc.weight
  models.14._orig_mod.transformer.h.2.mlp.proj.weight
  models.14._orig_mod.transformer.h.2.normAtt.weight
  models.14._orig_mod.transformer.h.2.normMlp.weight
  models.14._orig_mod.transformer.h.3.blockLambdas
  models.14._orig_mod.transformer.h.3.attn.lambdas
  models.14._orig_mod.transformer.h.3.attn.projAtt.weight
  models.14._orig_mod.transformer.h.3.attn.projAtt.bias
  models.14._orig_mod.transformer.h.3.attn.projQ.weight
  models.14._orig_mod.transformer.h.3.attn.projK.weight
  models.14._orig_mod.transformer.h.3.attn.projV.weight
  models.14._orig_mod.transformer.h.3.attn.projOut.weight
  models.14._orig_mod.transformer.h.3.mlp.fc.weight
  models.14._orig_mod.transformer.h.3.mlp.proj.weight
  models.14._orig_mod.transformer.h.3.normAtt.weight
  models.14._orig_mod.transformer.h.3.normMlp.weight
  models.14._orig_mod.normWte.weight
  models.14._orig_mod.normWte.bias
Optimizer #15/20: Adam
  Param group 0: lr=0.0006, weight_decay=0.0
  models.15._orig_mod.skipWeights
  models.15._orig_mod.transformer.wte.weight
  models.15._orig_mod.transformer.h.0.blockLambdas
  models.15._orig_mod.transformer.h.0.attn.lambdas
  models.15._orig_mod.transformer.h.0.attn.projAtt.weight
  models.15._orig_mod.transformer.h.0.attn.projAtt.bias
  models.15._orig_mod.transformer.h.0.attn.projQ.weight
  models.15._orig_mod.transformer.h.0.attn.projK.weight
  models.15._orig_mod.transformer.h.0.attn.projV.weight
  models.15._orig_mod.transformer.h.0.attn.projOut.weight
  models.15._orig_mod.transformer.h.0.mlp.fc.weight
  models.15._orig_mod.transformer.h.0.mlp.proj.weight
  models.15._orig_mod.transformer.h.0.normAtt.weight
  models.15._orig_mod.transformer.h.0.normMlp.weight
  models.15._orig_mod.transformer.h.1.blockLambdas
  models.15._orig_mod.transformer.h.1.attn.lambdas
  models.15._orig_mod.transformer.h.1.attn.projAtt.weight
  models.15._orig_mod.transformer.h.1.attn.projAtt.bias
  models.15._orig_mod.transformer.h.1.attn.projQ.weight
  models.15._orig_mod.transformer.h.1.attn.projK.weight
  models.15._orig_mod.transformer.h.1.attn.projV.weight
  models.15._orig_mod.transformer.h.1.attn.projOut.weight
  models.15._orig_mod.transformer.h.1.mlp.fc.weight
  models.15._orig_mod.transformer.h.1.mlp.proj.weight
  models.15._orig_mod.transformer.h.1.normAtt.weight
  models.15._orig_mod.transformer.h.1.normMlp.weight
  models.15._orig_mod.transformer.h.2.blockLambdas
  models.15._orig_mod.transformer.h.2.attn.lambdas
  models.15._orig_mod.transformer.h.2.attn.projAtt.weight
  models.15._orig_mod.transformer.h.2.attn.projAtt.bias
  models.15._orig_mod.transformer.h.2.attn.projQ.weight
  models.15._orig_mod.transformer.h.2.attn.projK.weight
  models.15._orig_mod.transformer.h.2.attn.projV.weight
  models.15._orig_mod.transformer.h.2.attn.projOut.weight
  models.15._orig_mod.transformer.h.2.mlp.fc.weight
  models.15._orig_mod.transformer.h.2.mlp.proj.weight
  models.15._orig_mod.transformer.h.2.normAtt.weight
  models.15._orig_mod.transformer.h.2.normMlp.weight
  models.15._orig_mod.transformer.h.3.blockLambdas
  models.15._orig_mod.transformer.h.3.attn.lambdas
  models.15._orig_mod.transformer.h.3.attn.projAtt.weight
  models.15._orig_mod.transformer.h.3.attn.projAtt.bias
  models.15._orig_mod.transformer.h.3.attn.projQ.weight
  models.15._orig_mod.transformer.h.3.attn.projK.weight
  models.15._orig_mod.transformer.h.3.attn.projV.weight
  models.15._orig_mod.transformer.h.3.attn.projOut.weight
  models.15._orig_mod.transformer.h.3.mlp.fc.weight
  models.15._orig_mod.transformer.h.3.mlp.proj.weight
  models.15._orig_mod.transformer.h.3.normAtt.weight
  models.15._orig_mod.transformer.h.3.normMlp.weight
  models.15._orig_mod.normWte.weight
  models.15._orig_mod.normWte.bias
Optimizer #16/20: Adam
  Param group 0: lr=0.0006, weight_decay=0.0
  models.16._orig_mod.skipWeights
  models.16._orig_mod.transformer.wte.weight
  models.16._orig_mod.transformer.h.0.blockLambdas
  models.16._orig_mod.transformer.h.0.attn.lambdas
  models.16._orig_mod.transformer.h.0.attn.projAtt.weight
  models.16._orig_mod.transformer.h.0.attn.projAtt.bias
  models.16._orig_mod.transformer.h.0.attn.projQ.weight
  models.16._orig_mod.transformer.h.0.attn.projK.weight
  models.16._orig_mod.transformer.h.0.attn.projV.weight
  models.16._orig_mod.transformer.h.0.attn.projOut.weight
  models.16._orig_mod.transformer.h.0.mlp.fc.weight
  models.16._orig_mod.transformer.h.0.mlp.proj.weight
  models.16._orig_mod.transformer.h.0.normAtt.weight
  models.16._orig_mod.transformer.h.0.normMlp.weight
  models.16._orig_mod.transformer.h.1.blockLambdas
  models.16._orig_mod.transformer.h.1.attn.lambdas
  models.16._orig_mod.transformer.h.1.attn.projAtt.weight
  models.16._orig_mod.transformer.h.1.attn.projAtt.bias
  models.16._orig_mod.transformer.h.1.attn.projQ.weight
  models.16._orig_mod.transformer.h.1.attn.projK.weight
  models.16._orig_mod.transformer.h.1.attn.projV.weight
  models.16._orig_mod.transformer.h.1.attn.projOut.weight
  models.16._orig_mod.transformer.h.1.mlp.fc.weight
  models.16._orig_mod.transformer.h.1.mlp.proj.weight
  models.16._orig_mod.transformer.h.1.normAtt.weight
  models.16._orig_mod.transformer.h.1.normMlp.weight
  models.16._orig_mod.transformer.h.2.blockLambdas
  models.16._orig_mod.transformer.h.2.attn.lambdas
  models.16._orig_mod.transformer.h.2.attn.projAtt.weight
  models.16._orig_mod.transformer.h.2.attn.projAtt.bias
  models.16._orig_mod.transformer.h.2.attn.projQ.weight
  models.16._orig_mod.transformer.h.2.attn.projK.weight
  models.16._orig_mod.transformer.h.2.attn.projV.weight
  models.16._orig_mod.transformer.h.2.attn.projOut.weight
  models.16._orig_mod.transformer.h.2.mlp.fc.weight
  models.16._orig_mod.transformer.h.2.mlp.proj.weight
  models.16._orig_mod.transformer.h.2.normAtt.weight
  models.16._orig_mod.transformer.h.2.normMlp.weight
  models.16._orig_mod.transformer.h.3.blockLambdas
  models.16._orig_mod.transformer.h.3.attn.lambdas
  models.16._orig_mod.transformer.h.3.attn.projAtt.weight
  models.16._orig_mod.transformer.h.3.attn.projAtt.bias
  models.16._orig_mod.transformer.h.3.attn.projQ.weight
  models.16._orig_mod.transformer.h.3.attn.projK.weight
  models.16._orig_mod.transformer.h.3.attn.projV.weight
  models.16._orig_mod.transformer.h.3.attn.projOut.weight
  models.16._orig_mod.transformer.h.3.mlp.fc.weight
  models.16._orig_mod.transformer.h.3.mlp.proj.weight
  models.16._orig_mod.transformer.h.3.normAtt.weight
  models.16._orig_mod.transformer.h.3.normMlp.weight
  models.16._orig_mod.normWte.weight
  models.16._orig_mod.normWte.bias
Optimizer #17/20: Adam
  Param group 0: lr=0.0006, weight_decay=0.0
  models.17._orig_mod.skipWeights
  models.17._orig_mod.transformer.wte.weight
  models.17._orig_mod.transformer.h.0.blockLambdas
  models.17._orig_mod.transformer.h.0.attn.lambdas
  models.17._orig_mod.transformer.h.0.attn.projAtt.weight
  models.17._orig_mod.transformer.h.0.attn.projAtt.bias
  models.17._orig_mod.transformer.h.0.attn.projQ.weight
  models.17._orig_mod.transformer.h.0.attn.projK.weight
  models.17._orig_mod.transformer.h.0.attn.projV.weight
  models.17._orig_mod.transformer.h.0.attn.projOut.weight
  models.17._orig_mod.transformer.h.0.mlp.fc.weight
  models.17._orig_mod.transformer.h.0.mlp.proj.weight
  models.17._orig_mod.transformer.h.0.normAtt.weight
  models.17._orig_mod.transformer.h.0.normMlp.weight
  models.17._orig_mod.transformer.h.1.blockLambdas
  models.17._orig_mod.transformer.h.1.attn.lambdas
  models.17._orig_mod.transformer.h.1.attn.projAtt.weight
  models.17._orig_mod.transformer.h.1.attn.projAtt.bias
  models.17._orig_mod.transformer.h.1.attn.projQ.weight
  models.17._orig_mod.transformer.h.1.attn.projK.weight
  models.17._orig_mod.transformer.h.1.attn.projV.weight
  models.17._orig_mod.transformer.h.1.attn.projOut.weight
  models.17._orig_mod.transformer.h.1.mlp.fc.weight
  models.17._orig_mod.transformer.h.1.mlp.proj.weight
  models.17._orig_mod.transformer.h.1.normAtt.weight
  models.17._orig_mod.transformer.h.1.normMlp.weight
  models.17._orig_mod.transformer.h.2.blockLambdas
  models.17._orig_mod.transformer.h.2.attn.lambdas
  models.17._orig_mod.transformer.h.2.attn.projAtt.weight
  models.17._orig_mod.transformer.h.2.attn.projAtt.bias
  models.17._orig_mod.transformer.h.2.attn.projQ.weight
  models.17._orig_mod.transformer.h.2.attn.projK.weight
  models.17._orig_mod.transformer.h.2.attn.projV.weight
  models.17._orig_mod.transformer.h.2.attn.projOut.weight
  models.17._orig_mod.transformer.h.2.mlp.fc.weight
  models.17._orig_mod.transformer.h.2.mlp.proj.weight
  models.17._orig_mod.transformer.h.2.normAtt.weight
  models.17._orig_mod.transformer.h.2.normMlp.weight
  models.17._orig_mod.transformer.h.3.blockLambdas
  models.17._orig_mod.transformer.h.3.attn.lambdas
  models.17._orig_mod.transformer.h.3.attn.projAtt.weight
  models.17._orig_mod.transformer.h.3.attn.projAtt.bias
  models.17._orig_mod.transformer.h.3.attn.projQ.weight
  models.17._orig_mod.transformer.h.3.attn.projK.weight
  models.17._orig_mod.transformer.h.3.attn.projV.weight
  models.17._orig_mod.transformer.h.3.attn.projOut.weight
  models.17._orig_mod.transformer.h.3.mlp.fc.weight
  models.17._orig_mod.transformer.h.3.mlp.proj.weight
  models.17._orig_mod.transformer.h.3.normAtt.weight
  models.17._orig_mod.transformer.h.3.normMlp.weight
  models.17._orig_mod.normWte.weight
  models.17._orig_mod.normWte.bias
Optimizer #18/20: Adam
  Param group 0: lr=0.0006, weight_decay=0.0
  models.18._orig_mod.skipWeights
  models.18._orig_mod.transformer.wte.weight
  models.18._orig_mod.transformer.h.0.blockLambdas
  models.18._orig_mod.transformer.h.0.attn.lambdas
  models.18._orig_mod.transformer.h.0.attn.projAtt.weight
  models.18._orig_mod.transformer.h.0.attn.projAtt.bias
  models.18._orig_mod.transformer.h.0.attn.projQ.weight
  models.18._orig_mod.transformer.h.0.attn.projK.weight
  models.18._orig_mod.transformer.h.0.attn.projV.weight
  models.18._orig_mod.transformer.h.0.attn.projOut.weight
  models.18._orig_mod.transformer.h.0.mlp.fc.weight
  models.18._orig_mod.transformer.h.0.mlp.proj.weight
  models.18._orig_mod.transformer.h.0.normAtt.weight
  models.18._orig_mod.transformer.h.0.normMlp.weight
  models.18._orig_mod.transformer.h.1.blockLambdas
  models.18._orig_mod.transformer.h.1.attn.lambdas
  models.18._orig_mod.transformer.h.1.attn.projAtt.weight
  models.18._orig_mod.transformer.h.1.attn.projAtt.bias
  models.18._orig_mod.transformer.h.1.attn.projQ.weight
  models.18._orig_mod.transformer.h.1.attn.projK.weight
  models.18._orig_mod.transformer.h.1.attn.projV.weight
  models.18._orig_mod.transformer.h.1.attn.projOut.weight
  models.18._orig_mod.transformer.h.1.mlp.fc.weight
  models.18._orig_mod.transformer.h.1.mlp.proj.weight
  models.18._orig_mod.transformer.h.1.normAtt.weight
  models.18._orig_mod.transformer.h.1.normMlp.weight
  models.18._orig_mod.transformer.h.2.blockLambdas
  models.18._orig_mod.transformer.h.2.attn.lambdas
  models.18._orig_mod.transformer.h.2.attn.projAtt.weight
  models.18._orig_mod.transformer.h.2.attn.projAtt.bias
  models.18._orig_mod.transformer.h.2.attn.projQ.weight
  models.18._orig_mod.transformer.h.2.attn.projK.weight
  models.18._orig_mod.transformer.h.2.attn.projV.weight
  models.18._orig_mod.transformer.h.2.attn.projOut.weight
  models.18._orig_mod.transformer.h.2.mlp.fc.weight
  models.18._orig_mod.transformer.h.2.mlp.proj.weight
  models.18._orig_mod.transformer.h.2.normAtt.weight
  models.18._orig_mod.transformer.h.2.normMlp.weight
  models.18._orig_mod.transformer.h.3.blockLambdas
  models.18._orig_mod.transformer.h.3.attn.lambdas
  models.18._orig_mod.transformer.h.3.attn.projAtt.weight
  models.18._orig_mod.transformer.h.3.attn.projAtt.bias
  models.18._orig_mod.transformer.h.3.attn.projQ.weight
  models.18._orig_mod.transformer.h.3.attn.projK.weight
  models.18._orig_mod.transformer.h.3.attn.projV.weight
  models.18._orig_mod.transformer.h.3.attn.projOut.weight
  models.18._orig_mod.transformer.h.3.mlp.fc.weight
  models.18._orig_mod.transformer.h.3.mlp.proj.weight
  models.18._orig_mod.transformer.h.3.normAtt.weight
  models.18._orig_mod.transformer.h.3.normMlp.weight
  models.18._orig_mod.normWte.weight
  models.18._orig_mod.normWte.bias
Optimizer #19/20: Adam
  Param group 0: lr=0.0006, weight_decay=0.0
  models.19._orig_mod.skipWeights
  models.19._orig_mod.transformer.wte.weight
  models.19._orig_mod.transformer.h.0.blockLambdas
  models.19._orig_mod.transformer.h.0.attn.lambdas
  models.19._orig_mod.transformer.h.0.attn.projAtt.weight
  models.19._orig_mod.transformer.h.0.attn.projAtt.bias
  models.19._orig_mod.transformer.h.0.attn.projQ.weight
  models.19._orig_mod.transformer.h.0.attn.projK.weight
  models.19._orig_mod.transformer.h.0.attn.projV.weight
  models.19._orig_mod.transformer.h.0.attn.projOut.weight
  models.19._orig_mod.transformer.h.0.mlp.fc.weight
  models.19._orig_mod.transformer.h.0.mlp.proj.weight
  models.19._orig_mod.transformer.h.0.normAtt.weight
  models.19._orig_mod.transformer.h.0.normMlp.weight
  models.19._orig_mod.transformer.h.1.blockLambdas
  models.19._orig_mod.transformer.h.1.attn.lambdas
  models.19._orig_mod.transformer.h.1.attn.projAtt.weight
  models.19._orig_mod.transformer.h.1.attn.projAtt.bias
  models.19._orig_mod.transformer.h.1.attn.projQ.weight
  models.19._orig_mod.transformer.h.1.attn.projK.weight
  models.19._orig_mod.transformer.h.1.attn.projV.weight
  models.19._orig_mod.transformer.h.1.attn.projOut.weight
  models.19._orig_mod.transformer.h.1.mlp.fc.weight
  models.19._orig_mod.transformer.h.1.mlp.proj.weight
  models.19._orig_mod.transformer.h.1.normAtt.weight
  models.19._orig_mod.transformer.h.1.normMlp.weight
  models.19._orig_mod.transformer.h.2.blockLambdas
  models.19._orig_mod.transformer.h.2.attn.lambdas
  models.19._orig_mod.transformer.h.2.attn.projAtt.weight
  models.19._orig_mod.transformer.h.2.attn.projAtt.bias
  models.19._orig_mod.transformer.h.2.attn.projQ.weight
  models.19._orig_mod.transformer.h.2.attn.projK.weight
  models.19._orig_mod.transformer.h.2.attn.projV.weight
  models.19._orig_mod.transformer.h.2.attn.projOut.weight
  models.19._orig_mod.transformer.h.2.mlp.fc.weight
  models.19._orig_mod.transformer.h.2.mlp.proj.weight
  models.19._orig_mod.transformer.h.2.normAtt.weight
  models.19._orig_mod.transformer.h.2.normMlp.weight
  models.19._orig_mod.transformer.h.3.blockLambdas
  models.19._orig_mod.transformer.h.3.attn.lambdas
  models.19._orig_mod.transformer.h.3.attn.projAtt.weight
  models.19._orig_mod.transformer.h.3.attn.projAtt.bias
  models.19._orig_mod.transformer.h.3.attn.projQ.weight
  models.19._orig_mod.transformer.h.3.attn.projK.weight
  models.19._orig_mod.transformer.h.3.attn.projV.weight
  models.19._orig_mod.transformer.h.3.attn.projOut.weight
  models.19._orig_mod.transformer.h.3.mlp.fc.weight
  models.19._orig_mod.transformer.h.3.mlp.proj.weight
  models.19._orig_mod.transformer.h.3.normAtt.weight
  models.19._orig_mod.transformer.h.3.normMlp.weight
  models.19._orig_mod.normWte.weight
  models.19._orig_mod.normWte.bias
Optimizer #20/20: Adam
  Param group 0: lr=0.1, weight_decay=0
  models.0._orig_mod.transformer.h.0.attn.af.afVals
  models.0._orig_mod.transformer.h.0.mlp.af.afVals
====================================================================================================
Step 1/20000   lossVa 15.1172   accVa 0.0
Step 1/20000   lossTr 15.1457   accTr 0.0   23s = 22.59s/step
Step 1/20000   Activation magnitudes (2): 0.80 0.26 0.80 0.26 0.80 0.25 0.80 0.26
Step 1/20000   Activation magnitudes (3): 4.56 1.66 4.56 1.56 4.56 1.62 4.56 1.75
Step 6/20000   Peak CUDA memory usage: 2.839 GB
Step 6/20000   lossTr 15.1354   accTr 0.0   35s = 2.48s/step
Step 6/20000   Activation magnitudes (2): 0.80 0.26 0.80 0.26 0.80 0.25 0.80 0.26
Step 6/20000   Activation magnitudes (3): 4.54 1.55 4.54 1.61 4.54 1.67 4.54 1.62
Step 11/20000   Peak CUDA memory usage: 2.797 GB
Step 201/20000   lossTr 13.0417   accTr 0.8   46s = 0.06s/step
Step 201/20000   Activation magnitudes (2): 0.80 0.30 0.80 0.25 0.82 0.23 0.84 0.19
Step 201/20000   Activation magnitudes (3): 4.79 1.76 4.97 1.56 4.64 1.64 4.17 1.66
Step 401/20000   lossTr 7.4421   accTr 9.1   58s = 0.06s/step
Step 401/20000   Activation magnitudes (2): 0.80 0.30 0.80 0.26 0.81 0.27 0.82 0.24
Step 401/20000   Activation magnitudes (3): 4.75 1.73 4.64 1.61 4.38 1.84 4.13 1.81
Step 501/20000   lossVa 6.8937   accVa 12.0
Step 601/20000   lossTr 6.8918   accTr 12.0   70s = 0.06s/step
Step 601/20000   Activation magnitudes (2): 0.81 0.30 0.81 0.27 0.81 0.29 0.81 0.27
Step 601/20000   Activation magnitudes (3): 4.61 1.80 4.60 1.90 4.45 1.91 3.97 1.80
Step 801/20000   lossTr 6.6726   accTr 13.5   81s = 0.06s/step
Step 801/20000   Activation magnitudes (2): 0.82 0.30 0.80 0.28 0.81 0.31 0.80 0.29
Step 801/20000   Activation magnitudes (3): 4.61 1.98 5.13 2.00 4.37 2.24 4.37 1.93
Step 1001/20000   lossVa 6.5489   accVa 14.7
Step 1001/20000   lossTr 6.5811   accTr 14.1   92s = 0.06s/step
Step 1001/20000   Activation magnitudes (2): 0.83 0.30 0.80 0.29 0.81 0.34 0.77 0.31
Step 1001/20000   Activation magnitudes (3): 4.71 2.19 5.28 2.09 4.23 2.47 5.03 2.05
Step 1201/20000   lossTr 6.4399   accTr 15.0   120s = 0.14s/step
Step 1201/20000   Activation magnitudes (2): 0.86 0.32 0.82 0.32 0.81 0.37 0.78 0.32
Step 1201/20000   Activation magnitudes (3): 4.87 2.36 4.72 2.27 5.76 2.54 5.64 2.41
Step 1401/20000   lossTr 6.4204   accTr 15.3   149s = 0.14s/step
Step 1401/20000   Activation magnitudes (2): 0.89 0.34 0.82 0.36 0.81 0.40 0.79 0.35
Step 1401/20000   Activation magnitudes (3): 5.08 2.56 4.82 2.45 6.25 2.92 7.06 2.53
Step 1501/20000   lossVa 6.3308   accVa 16.1
Step 1601/20000   lossTr 6.3671   accTr 15.5   177s = 0.14s/step
Step 1601/20000   Activation magnitudes (2): 0.90 0.36 0.81 0.38 0.79 0.43 0.77 0.34
Step 1601/20000   Activation magnitudes (3): 5.23 2.31 5.51 2.44 6.73 2.89 7.39 2.52
Step 1801/20000   lossTr 6.2804   accTr 16.0   205s = 0.14s/step
Step 1801/20000   Activation magnitudes (2): 0.90 0.38 0.79 0.41 0.77 0.45 0.76 0.37
Step 1801/20000   Activation magnitudes (3): 5.65 2.37 6.31 2.73 7.70 2.82 7.86 2.72
Step 2001/20000   lossVa 6.2001   accVa 16.7
Step 2001/20000   lossTr 6.2853   accTr 16.1   233s = 0.14s/step
Step 2001/20000   Activation magnitudes (2): 0.89 0.39 0.77 0.44 0.75 0.48 0.74 0.39
Step 2001/20000   Activation magnitudes (3): 5.41 2.41 6.13 2.88 7.45 2.95 8.76 2.80
Step 2201/20000   lossTr 6.1921   accTr 16.6   275s = 0.21s/step
Step 2201/20000   Activation magnitudes (2): 0.92 0.42 0.79 0.50 0.76 0.52 0.76 0.43
Step 2201/20000   Activation magnitudes (3): 5.95 2.76 6.90 3.29 7.91 3.29 9.50 3.13
Step 2401/20000   lossTr 6.1653   accTr 16.6   315s = 0.20s/step
Step 2401/20000   Activation magnitudes (2): 0.90 0.44 0.76 0.52 0.74 0.54 0.74 0.45
Step 2401/20000   Activation magnitudes (3): 6.48 2.78 6.65 3.08 8.62 3.70 9.36 3.28
Step 2501/20000   lossVa 6.0866   accVa 17.2
Step 2601/20000   lossTr 6.0641   accTr 17.2   356s = 0.20s/step
Step 2601/20000   Activation magnitudes (2): 0.89 0.44 0.74 0.54 0.71 0.56 0.73 0.46
Step 2601/20000   Activation magnitudes (3): 6.85 2.85 6.95 3.57 9.08 3.57 10.57 3.33
Step 2801/20000   lossTr 6.0105   accTr 17.6   396s = 0.20s/step
Step 2801/20000   Activation magnitudes (2): 0.87 0.44 0.72 0.54 0.70 0.56 0.71 0.46
Step 2801/20000   Activation magnitudes (3): 7.34 2.65 7.32 3.66 10.15 3.78 11.00 3.64
Step 3001/20000   lossVa 5.9124   accVa 18.2
Step 3001/20000   lossTr 5.9590   accTr 17.9   437s = 0.20s/step
Step 3001/20000   Activation magnitudes (2): 0.87 0.44 0.71 0.55 0.69 0.57 0.69 0.46
Step 3001/20000   Activation magnitudes (3): 7.71 2.82 7.80 3.90 11.14 4.35 11.93 3.54
Step 3201/20000   lossTr 5.8816   accTr 18.3   491s = 0.27s/step
Step 3201/20000   Activation magnitudes (2): 0.89 0.47 0.73 0.60 0.71 0.61 0.73 0.52
Step 3201/20000   Activation magnitudes (3): 8.23 3.04 7.31 4.49 10.37 4.38 10.86 3.72
Step 3401/20000   lossTr 5.9952   accTr 17.9   545s = 0.27s/step
Step 3401/20000   Activation magnitudes (2): 0.87 0.47 0.71 0.62 0.69 0.65 0.71 0.53
Step 3401/20000   Activation magnitudes (3): 8.26 3.05 7.25 4.03 9.35 4.54 10.94 3.83
Step 3501/20000   lossVa 5.8775   accVa 18.7
Step 3601/20000   lossTr 5.8825   accTr 18.5   598s = 0.26s/step
Step 3601/20000   Activation magnitudes (2): 0.88 0.47 0.70 0.62 0.68 0.64 0.69 0.54
Step 3601/20000   Activation magnitudes (3): 8.36 3.17 7.02 4.41 9.78 4.54 11.04 4.12
Step 3801/20000   lossTr 5.7963   accTr 19.0   650s = 0.26s/step
Step 3801/20000   Activation magnitudes (2): 0.88 0.48 0.69 0.62 0.68 0.65 0.69 0.53
Step 3801/20000   Activation magnitudes (3): 8.71 3.16 7.50 4.55 9.49 5.34 10.92 4.30
Step 4001/20000   lossVa 5.7359   accVa 19.6
Step 4001/20000   lossTr 5.7398   accTr 19.6   701s = 0.26s/step
Step 4001/20000   Activation magnitudes (2): 0.89 0.48 0.68 0.63 0.67 0.65 0.69 0.53
Step 4001/20000   Activation magnitudes (3): 8.79 3.37 7.54 4.76 9.55 5.38 10.65 4.46
Step 4201/20000   lossTr 5.7124   accTr 19.8   767s = 0.33s/step
Step 4201/20000   Activation magnitudes (2): 0.91 0.49 0.69 0.67 0.70 0.69 0.71 0.57
Step 4201/20000   Activation magnitudes (3): 9.08 3.44 7.72 5.00 9.29 5.29 10.47 4.79
Step 4401/20000   lossTr 5.7370   accTr 19.9   831s = 0.32s/step
Step 4401/20000   Activation magnitudes (2): 0.90 0.50 0.68 0.69 0.68 0.70 0.69 0.59
Step 4401/20000   Activation magnitudes (3): 9.05 3.84 7.45 5.57 9.21 5.69 9.94 5.05
Step 4501/20000   lossVa 5.6188   accVa 20.7
Step 4601/20000   lossTr 5.6519   accTr 20.4   896s = 0.32s/step
Step 4601/20000   Activation magnitudes (2): 0.91 0.50 0.67 0.68 0.68 0.71 0.69 0.60
Step 4601/20000   Activation magnitudes (3): 9.17 3.48 7.17 5.64 9.29 6.06 9.49 5.19
Step 4801/20000   lossTr 5.6212   accTr 20.8   959s = 0.32s/step
Step 4801/20000   Activation magnitudes (2): 0.91 0.50 0.66 0.68 0.67 0.71 0.69 0.59
Step 4801/20000   Activation magnitudes (3): 9.16 3.66 7.36 5.87 9.70 5.75 9.88 5.25
Step 5001/20000   lossVa 5.5549   accVa 21.3
Step 5001/20000   lossTr 5.5625   accTr 21.1   1020s = 0.31s/step
Step 5001/20000   Activation magnitudes (2): 0.92 0.51 0.65 0.69 0.66 0.70 0.69 0.59
Step 5001/20000   Activation magnitudes (3): 8.96 3.76 7.57 6.55 9.88 6.14 10.10 5.10
Step 5201/20000   lossTr 5.5187   accTr 21.4   1094s = 0.37s/step
Step 5201/20000   Activation magnitudes (2): 0.92 0.52 0.66 0.73 0.68 0.74 0.69 0.62
Step 5201/20000   Activation magnitudes (3): 9.01 4.40 6.83 6.14 9.55 5.91 9.70 4.91
Step 5401/20000   lossTr 5.5142   accTr 21.7   1163s = 0.35s/step
Step 5401/20000   Activation magnitudes (2): 0.92 0.52 0.65 0.75 0.67 0.76 0.68 0.63
Step 5401/20000   Activation magnitudes (3): 9.00 3.96 7.17 5.98 9.36 6.14 9.47 5.33
Step 5501/20000   lossVa 5.4786   accVa 21.9
Step 5601/20000   lossTr 5.5028   accTr 21.8   1233s = 0.35s/step
Step 5601/20000   Activation magnitudes (2): 0.92 0.53 0.64 0.75 0.67 0.75 0.68 0.63
Step 5601/20000   Activation magnitudes (3): 9.03 4.13 6.99 6.94 9.61 6.98 9.90 5.46
Step 5801/20000   lossTr 5.4237   accTr 22.1   1303s = 0.35s/step
Step 5801/20000   Activation magnitudes (2): 0.91 0.54 0.64 0.74 0.66 0.73 0.68 0.62
Step 5801/20000   Activation magnitudes (3): 8.95 4.11 7.03 6.95 9.15 7.58 9.66 5.52
Step 6001/20000   lossVa 5.4170   accVa 22.4
Step 6001/20000   lossTr 5.4519   accTr 22.3   1370s = 0.34s/step
Step 6001/20000   Activation magnitudes (2): 0.92 0.54 0.63 0.76 0.66 0.76 0.68 0.64
Step 6001/20000   Activation magnitudes (3): 8.90 4.24 7.03 7.01 9.16 7.03 9.57 5.56
Step 6201/20000   lossTr 5.4464   accTr 22.1   1450s = 0.40s/step
Step 6201/20000   Activation magnitudes (2): 0.91 0.55 0.64 0.77 0.68 0.76 0.69 0.63
Step 6201/20000   Activation magnitudes (3): 9.13 4.42 7.18 6.81 8.68 7.51 9.21 6.01
Step 6401/20000   lossTr 5.4540   accTr 22.1   1529s = 0.39s/step
Step 6401/20000   Activation magnitudes (2): 0.91 0.56 0.63 0.79 0.66 0.80 0.67 0.67
Step 6401/20000   Activation magnitudes (3): 8.88 4.71 7.04 6.97 9.36 7.16 9.66 5.89
Step 6501/20000   lossVa 5.3841   accVa 22.8
Step 6601/20000   lossTr 5.4015   accTr 22.5   1608s = 0.40s/step
Step 6601/20000   Activation magnitudes (2): 0.90 0.56 0.63 0.80 0.65 0.80 0.67 0.66
Step 6601/20000   Activation magnitudes (3): 8.75 4.60 7.12 8.65 8.86 7.23 9.78 6.52
Step 6801/20000   lossTr 5.3652   accTr 22.8   1687s = 0.39s/step
Step 6801/20000   Activation magnitudes (2): 0.91 0.57 0.62 0.81 0.64 0.82 0.66 0.68
Step 6801/20000   Activation magnitudes (3): 8.63 4.88 7.17 7.75 9.10 7.97 9.44 6.32
Step 7001/20000   lossVa 5.3549   accVa 22.9
Step 7001/20000   lossTr 5.3557   accTr 22.9   1763s = 0.38s/step
Step 7001/20000   Activation magnitudes (2): 0.90 0.58 0.62 0.82 0.65 0.82 0.66 0.68
Step 7001/20000   Activation magnitudes (3): 8.44 4.77 7.29 8.14 9.18 8.24 9.86 6.23
Step 7201/20000   lossTr 5.3552   accTr 22.8   1853s = 0.45s/step
Step 7201/20000   Activation magnitudes (2): 0.91 0.58 0.62 0.84 0.66 0.84 0.67 0.70
Step 7201/20000   Activation magnitudes (3): 8.32 4.81 7.37 8.00 8.68 7.24 9.34 6.20
Step 7401/20000   lossTr 5.3776   accTr 22.9   1942s = 0.45s/step
Step 7401/20000   Activation magnitudes (2): 0.90 0.59 0.62 0.83 0.65 0.84 0.67 0.71
Step 7401/20000   Activation magnitudes (3): 8.09 4.59 7.13 8.40 8.78 8.04 9.75 6.37
Step 7501/20000   lossVa 5.3451   accVa 23.1
Step 7601/20000   lossTr 5.3475   accTr 23.0   2032s = 0.45s/step
Step 7601/20000   Activation magnitudes (2): 0.90 0.60 0.61 0.85 0.64 0.84 0.65 0.70
Step 7601/20000   Activation magnitudes (3): 7.88 5.25 7.18 8.13 8.72 8.30 9.87 6.47
Step 7801/20000   lossTr 5.3150   accTr 23.2   2123s = 0.45s/step
Step 7801/20000   Activation magnitudes (2): 0.89 0.61 0.61 0.86 0.63 0.87 0.65 0.72
Step 7801/20000   Activation magnitudes (3): 7.56 5.10 7.25 8.48 9.00 8.69 10.04 6.68
Step 8001/20000   lossVa 5.2819   accVa 23.7
Step 8001/20000   lossTr 5.3330   accTr 23.2   2213s = 0.45s/step
Step 8001/20000   Activation magnitudes (2): 0.89 0.61 0.60 0.86 0.63 0.87 0.66 0.72
Step 8001/20000   Activation magnitudes (3): 7.39 5.04 6.85 8.08 8.65 8.80 10.23 6.91
Step 8201/20000   lossTr 5.3056   accTr 23.1   2317s = 0.52s/step
Step 8201/20000   Activation magnitudes (2): 0.89 0.62 0.61 0.89 0.64 0.88 0.66 0.74
Step 8201/20000   Activation magnitudes (3): 7.32 4.98 7.18 7.53 8.29 8.13 10.25 6.60
Step 8401/20000   lossTr 5.3155   accTr 23.2   2419s = 0.51s/step
Step 8401/20000   Activation magnitudes (2): 0.88 0.63 0.60 0.90 0.64 0.91 0.64 0.74
Step 8401/20000   Activation magnitudes (3): 7.30 5.26 6.79 8.19 8.26 8.69 9.82 6.82
Step 8501/20000   lossVa 5.3158   accVa 23.4
Step 8601/20000   lossTr 5.2960   accTr 23.3   2524s = 0.52s/step
Step 8601/20000   Activation magnitudes (2): 0.87 0.63 0.60 0.90 0.63 0.91 0.64 0.73
Step 8601/20000   Activation magnitudes (3): 7.37 5.67 6.61 8.43 8.40 9.44 10.02 6.94
Step 8801/20000   lossTr 5.2955   accTr 23.5   2627s = 0.51s/step
Step 8801/20000   Activation magnitudes (2): 0.87 0.64 0.59 0.91 0.63 0.91 0.64 0.75
Step 8801/20000   Activation magnitudes (3): 7.36 5.44 6.69 8.42 8.47 8.89 10.16 7.34
Step 9001/20000   lossVa 5.2531   accVa 23.8
Step 9001/20000   lossTr 5.2506   accTr 23.5   2730s = 0.52s/step
Step 9001/20000   Activation magnitudes (2): 0.86 0.64 0.59 0.91 0.63 0.90 0.64 0.76
Step 9001/20000   Activation magnitudes (3): 7.45 5.51 6.58 8.81 8.72 9.19 10.10 7.26
Step 9201/20000   lossTr 5.3009   accTr 23.4   2845s = 0.57s/step
Step 9201/20000   Activation magnitudes (2): 0.88 0.66 0.59 0.94 0.63 0.92 0.64 0.79
Step 9201/20000   Activation magnitudes (3): 7.45 5.15 6.50 8.32 8.34 9.00 10.16 7.44
Step 9401/20000   lossTr 5.2735   accTr 23.7   2959s = 0.57s/step
Step 9401/20000   Activation magnitudes (2): 0.85 0.66 0.59 0.94 0.63 0.95 0.63 0.79
Step 9401/20000   Activation magnitudes (3): 7.47 5.22 6.52 8.17 8.55 9.55 10.09 7.05
Step 9501/20000   lossVa 5.2492   accVa 23.7
Step 9601/20000   lossTr 5.2293   accTr 23.8   3073s = 0.57s/step
Step 9601/20000   Activation magnitudes (2): 0.85 0.66 0.59 0.94 0.62 0.94 0.63 0.78
Step 9601/20000   Activation magnitudes (3): 7.45 6.11 6.08 8.63 8.07 9.68 9.80 7.40
Step 9801/20000   lossTr 5.2256   accTr 24.0   3190s = 0.59s/step
Step 9801/20000   Activation magnitudes (2): 0.84 0.66 0.58 0.95 0.61 0.97 0.62 0.78
Step 9801/20000   Activation magnitudes (3): 7.49 5.67 6.12 10.40 8.42 9.93 9.81 7.25
Step 10001/20000   lossVa 5.2090   accVa 24.1
Step 10001/20000   lossTr 5.2519   accTr 23.7   3310s = 0.60s/step
Step 10001/20000   Activation magnitudes (2): 0.83 0.67 0.58 0.95 0.62 0.97 0.63 0.81
Step 10001/20000   Activation magnitudes (3): 7.52 5.91 6.09 10.25 8.33 10.90 9.73 7.52
Step 10201/20000   lossTr 5.2368   accTr 23.7   3437s = 0.64s/step
Step 10201/20000   Activation magnitudes (2): 0.83 0.68 0.58 0.97 0.62 0.98 0.63 0.80
Step 10201/20000   Activation magnitudes (3): 7.50 5.94 6.10 10.62 8.35 9.94 9.76 7.31
Step 10401/20000   lossTr 5.2143   accTr 24.0   3561s = 0.62s/step
Step 10401/20000   Activation magnitudes (2): 0.84 0.70 0.58 0.98 0.62 0.97 0.63 0.82
Step 10401/20000   Activation magnitudes (3): 7.44 5.76 6.22 8.92 8.22 10.94 9.70 7.90
Step 10501/20000   lossVa 5.2088   accVa 24.2
Step 10601/20000   lossTr 5.2199   accTr 24.1   3688s = 0.63s/step
Step 10601/20000   Activation magnitudes (2): 0.81 0.67 0.58 0.97 0.62 0.95 0.63 0.79
Step 10601/20000   Activation magnitudes (3): 7.63 5.90 6.24 10.76 8.27 10.14 9.08 7.73
Step 10801/20000   lossTr 5.2246   accTr 24.0   3814s = 0.63s/step
Step 10801/20000   Activation magnitudes (2): 0.82 0.70 0.57 0.99 0.61 1.00 0.62 0.81
Step 10801/20000   Activation magnitudes (3): 7.50 6.07 6.12 9.83 8.09 10.15 9.02 7.69
Step 11001/20000   lossVa 5.1670   accVa 24.4
Step 11001/20000   lossTr 5.2048   accTr 24.2   3945s = 0.65s/step
Step 11001/20000   Activation magnitudes (2): 0.82 0.70 0.57 0.99 0.61 1.00 0.62 0.84
Step 11001/20000   Activation magnitudes (3): 7.55 6.40 6.03 10.80 8.34 10.65 8.85 8.29
Step 11201/20000   lossTr 5.1835   accTr 24.1   4091s = 0.73s/step
Step 11201/20000   Activation magnitudes (2): 0.82 0.71 0.57 1.02 0.61 1.02 0.62 0.82
Step 11201/20000   Activation magnitudes (3): 7.51 6.08 6.17 10.08 8.53 10.50 8.71 8.16
Step 11401/20000   lossTr 5.1716   accTr 23.9   4230s = 0.70s/step
Step 11401/20000   Activation magnitudes (2): 0.82 0.71 0.57 1.01 0.61 1.03 0.62 0.83
Step 11401/20000   Activation magnitudes (3): 7.30 6.25 6.26 10.15 8.24 10.11 8.35 8.22
Step 11501/20000   lossVa 5.1765   accVa 24.4
Step 11601/20000   lossTr 5.1596   accTr 24.4   4372s = 0.71s/step
Step 11601/20000   Activation magnitudes (2): 0.82 0.71 0.57 1.01 0.61 1.04 0.62 0.83
Step 11601/20000   Activation magnitudes (3): 7.36 6.48 6.05 9.19 8.36 10.94 8.13 8.18
Step 11801/20000   lossTr 5.1527   accTr 24.7   4509s = 0.68s/step
Step 11801/20000   Activation magnitudes (2): 0.82 0.71 0.56 1.02 0.61 1.05 0.62 0.84
Step 11801/20000   Activation magnitudes (3): 7.39 6.34 6.10 9.87 8.58 10.76 8.12 9.09
Step 12001/20000   lossVa 5.1453   accVa 24.6
Step 12001/20000   lossTr 5.1336   accTr 24.9   4646s = 0.68s/step
Step 12001/20000   Activation magnitudes (2): 0.83 0.72 0.56 1.03 0.61 1.04 0.62 0.87
Step 12001/20000   Activation magnitudes (3): 7.51 6.37 5.98 12.21 8.62 11.15 7.86 9.10
Step 12201/20000   lossTr 5.1538   accTr 24.5   4837s = 0.96s/step
Step 12201/20000   Activation magnitudes (2): 0.84 0.73 0.56 1.04 0.61 1.07 0.62 0.85
Step 12201/20000   Activation magnitudes (3): 7.61 6.97 6.20 10.30 8.43 11.06 7.39 8.18
Step 12401/20000   lossTr 5.1565   accTr 24.3   5056s = 1.09s/step
Step 12401/20000   Activation magnitudes (2): 0.83 0.73 0.56 1.05 0.61 1.07 0.61 0.88
Step 12401/20000   Activation magnitudes (3): 7.58 6.36 6.32 11.30 8.50 10.66 7.40 8.06
Step 12501/20000   lossVa 5.1269   accVa 24.8
Step 12601/20000   lossTr 5.1471   accTr 24.5   5304s = 1.24s/step
Step 12601/20000   Activation magnitudes (2): 0.84 0.74 0.56 1.03 0.61 1.04 0.62 0.84
Step 12601/20000   Activation magnitudes (3): 7.69 6.50 5.99 10.05 8.62 11.12 7.15 8.31
Step 12801/20000   lossTr 5.1410   accTr 24.5   5564s = 1.30s/step
Step 12801/20000   Activation magnitudes (2): 0.82 0.74 0.55 1.04 0.61 1.01 0.62 0.88
Step 12801/20000   Activation magnitudes (3): 7.79 6.56 6.09 12.15 8.18 10.58 6.30 9.42
Step 13001/20000   lossVa 5.1053   accVa 24.9
Step 13001/20000   lossTr 5.1427   accTr 24.6   5767s = 1.01s/step
Step 13001/20000   Activation magnitudes (2): 0.84 0.74 0.56 1.06 0.61 1.08 0.62 0.91
Step 13001/20000   Activation magnitudes (3): 7.85 6.66 6.05 10.75 8.62 11.22 7.30 9.35
Step 13201/20000   lossTr 5.1487   accTr 24.6   6009s = 1.21s/step
Step 13201/20000   Activation magnitudes (2): 0.85 0.75 0.56 1.07 0.61 1.08 0.62 0.87
Step 13201/20000   Activation magnitudes (3): 7.98 7.13 6.02 11.59 8.57 10.97 7.21 8.50
Step 13401/20000   lossTr 5.1021   accTr 24.9   6226s = 1.09s/step
Step 13401/20000   Activation magnitudes (2): 0.85 0.76 0.56 1.07 0.61 1.09 0.61 0.89
Step 13401/20000   Activation magnitudes (3): 8.17 6.92 6.11 10.30 8.75 10.62 7.35 8.56
Step 13501/20000   lossVa 5.1092   accVa 24.9
Step 13601/20000   lossTr 5.1346   accTr 24.7   6435s = 1.04s/step
Step 13601/20000   Activation magnitudes (2): 0.86 0.75 0.56 1.08 0.61 1.10 0.61 0.90
Step 13601/20000   Activation magnitudes (3): 8.30 7.30 6.11 11.95 8.76 11.17 7.16 8.48
Step 13801/20000   lossTr 5.0923   accTr 25.0   6648s = 1.07s/step
Step 13801/20000   Activation magnitudes (2): 0.88 0.76 0.55 1.07 0.61 1.12 0.62 0.89
Step 13801/20000   Activation magnitudes (3): 8.42 7.27 5.94 12.40 8.62 12.33 7.13 8.78
Step 14001/20000   lossVa 5.0825   accVa 25.1
Step 14001/20000   lossTr 5.0663   accTr 25.2   6872s = 1.12s/step
Step 14001/20000   Activation magnitudes (2): 0.87 0.76 0.56 1.08 0.61 1.11 0.63 0.95
Step 14001/20000   Activation magnitudes (3): 8.49 6.56 6.01 11.49 8.57 11.59 7.24 9.34
Step 14201/20000   lossTr 5.1408   accTr 24.7   7107s = 1.17s/step
Step 14201/20000   Activation magnitudes (2): 0.88 0.76 0.56 1.09 0.61 1.10 0.63 0.90
Step 14201/20000   Activation magnitudes (3): 8.65 7.01 6.03 11.29 8.57 10.71 6.79 9.28
Step 14401/20000   lossTr 5.1102   accTr 25.1   7360s = 1.26s/step
Step 14401/20000   Activation magnitudes (2): 0.88 0.77 0.55 1.09 0.61 1.13 0.62 0.92
Step 14401/20000   Activation magnitudes (3): 8.64 6.95 5.95 13.37 8.39 11.85 7.03 9.14
Step 14501/20000   lossVa 5.1003   accVa 25.1
Step 14601/20000   lossTr 5.0775   accTr 25.2   7624s = 1.32s/step
Step 14601/20000   Activation magnitudes (2): 0.88 0.77 0.55 1.09 0.61 1.08 0.64 0.92
Step 14601/20000   Activation magnitudes (3): 8.76 8.00 5.98 12.76 8.43 11.33 7.05 8.48
Step 14801/20000   lossTr 5.0351   accTr 25.2   7884s = 1.30s/step
Step 14801/20000   Activation magnitudes (2): 0.91 0.78 0.55 1.11 0.61 1.15 0.63 0.95
Step 14801/20000   Activation magnitudes (3): 8.92 6.68 5.92 11.19 8.25 11.79 7.19 8.58
Step 15001/20000   lossVa 5.0575   accVa 25.4
Step 15001/20000   lossTr 5.1011   accTr 25.1   8108s = 1.12s/step
Step 15001/20000   Activation magnitudes (2): 0.90 0.77 0.55 1.10 0.61 1.13 0.64 0.95
Step 15001/20000   Activation magnitudes (3): 9.01 6.68 5.98 11.70 8.48 12.45 7.30 8.84
Step 15201/20000   lossTr 5.0951   accTr 25.0   8349s = 1.21s/step
Step 15201/20000   Activation magnitudes (2): 0.90 0.78 0.55 1.11 0.61 1.13 0.64 0.92
Step 15201/20000   Activation magnitudes (3): 6.45 6.75 5.94 11.65 8.57 11.34 7.28 8.53
Step 15401/20000   lossTr 5.0342   accTr 25.2   8619s = 1.35s/step
Step 15401/20000   Activation magnitudes (2): 0.91 0.77 0.56 1.11 0.61 1.15 0.64 0.95
Step 15401/20000   Activation magnitudes (3): 9.19 6.89 5.93 12.08 8.68 12.08 7.11 8.99
Step 15501/20000   lossVa 5.0667   accVa 25.4
Step 15601/20000   lossTr 5.0466   accTr 25.4   8894s = 1.38s/step
Step 15601/20000   Activation magnitudes (2): 0.92 0.78 0.55 1.13 0.61 1.17 0.64 0.98
Step 15601/20000   Activation magnitudes (3): 9.28 6.95 5.80 12.69 8.27 12.50 7.22 9.35
Step 15801/20000   lossTr 5.0546   accTr 25.2   9160s = 1.33s/step
Step 15801/20000   Activation magnitudes (2): 0.92 0.79 0.55 1.12 0.61 1.17 0.64 0.99
Step 15801/20000   Activation magnitudes (3): 9.37 7.22 5.86 11.61 8.48 11.62 7.47 9.76
Step 16001/20000   lossVa 5.0532   accVa 25.6
Step 16001/20000   lossTr 5.0191   accTr 25.6   9438s = 1.39s/step
Step 16001/20000   Activation magnitudes (2): 0.92 0.78 0.55 1.11 0.61 1.14 0.65 0.99
Step 16001/20000   Activation magnitudes (3): 9.47 7.09 5.69 11.91 8.80 12.58 7.42 9.23
Step 16201/20000   lossTr 5.0272   accTr 25.4   9718s = 1.40s/step
Step 16201/20000   Activation magnitudes (2): 0.93 0.78 0.56 1.12 0.62 1.16 0.66 0.98
Step 16201/20000   Activation magnitudes (3): 9.52 8.20 6.13 11.89 8.66 12.01 7.30 8.64
Step 16401/20000   lossTr 5.0201   accTr 25.8   9986s = 1.34s/step
Step 16401/20000   Activation magnitudes (2): 0.93 0.79 0.56 1.10 0.62 1.11 0.67 0.96
Step 16401/20000   Activation magnitudes (3): 6.52 6.66 6.02 12.13 8.78 12.64 7.54 8.89
Step 16501/20000   lossVa 5.0141   accVa 25.6
Step 16601/20000   lossTr 4.9854   accTr 25.8   10270s = 1.42s/step
Step 16601/20000   Activation magnitudes (2): 0.93 0.78 0.55 1.12 0.61 1.14 0.66 1.00
Step 16601/20000   Activation magnitudes (3): 9.68 6.83 5.84 12.59 8.68 11.76 7.47 8.90
Step 16801/20000   lossTr 4.9841   accTr 25.9   10561s = 1.45s/step
Step 16801/20000   Activation magnitudes (2): 0.95 0.79 0.56 1.13 0.61 1.18 0.65 1.00
Step 16801/20000   Activation magnitudes (3): 9.80 6.76 5.65 12.23 8.44 12.21 7.49 9.36
Step 17001/20000   lossVa 4.9996   accVa 25.7
Step 17001/20000   lossTr 4.9769   accTr 25.9   10855s = 1.47s/step
Step 17001/20000   Activation magnitudes (2): 0.94 0.79 0.55 1.13 0.62 1.17 0.66 1.04
Step 17001/20000   Activation magnitudes (3): 9.82 7.02 5.67 12.39 8.71 12.28 7.58 9.34
Step 17201/20000   lossTr 4.9741   accTr 26.0   11202s = 1.74s/step
Step 17201/20000   Activation magnitudes (2): 0.96 0.79 0.56 1.14 0.62 1.20 0.67 1.03
Step 17201/20000   Activation magnitudes (3): 9.90 6.94 5.97 12.35 8.32 12.23 7.06 9.06
Step 17401/20000   lossTr 4.9773   accTr 25.8   11527s = 1.62s/step
Step 17401/20000   Activation magnitudes (2): 0.96 0.79 0.56 1.14 0.62 1.19 0.66 1.02
Step 17401/20000   Activation magnitudes (3): 9.95 7.03 5.91 12.31 8.79 11.99 7.35 8.96
Step 17501/20000   lossVa 4.9869   accVa 26.0
Step 17601/20000   lossTr 4.9731   accTr 26.2   11851s = 1.62s/step
Step 17601/20000   Activation magnitudes (2): 0.96 0.79 0.56 1.14 0.62 1.21 0.67 1.05
Step 17601/20000   Activation magnitudes (3): 10.04 6.89 5.78 12.17 8.53 11.97 7.39 9.37
Step 17801/20000   lossTr 4.9681   accTr 25.9   12195s = 1.72s/step
Step 17801/20000   Activation magnitudes (2): 0.96 0.78 0.56 1.08 0.62 1.12 0.69 1.03
Step 17801/20000   Activation magnitudes (3): 6.67 6.77 5.63 11.84 8.32 12.39 7.27 8.95
Step 18001/20000   lossVa 4.9594   accVa 26.1
Step 18001/20000   lossTr 5.0232   accTr 25.7   12505s = 1.55s/step
Step 18001/20000   Activation magnitudes (2): 0.96 0.79 0.56 1.14 0.62 1.19 0.68 1.09
Step 18001/20000   Activation magnitudes (3): 10.12 7.03 5.70 12.21 8.71 13.15 7.74 9.46
Step 18201/20000   lossTr 4.9839   accTr 25.9   12831s = 1.63s/step
Step 18201/20000   Activation magnitudes (2): 0.96 0.79 0.56 1.13 0.62 1.19 0.67 1.07
Step 18201/20000   Activation magnitudes (3): 10.19 6.99 6.05 12.10 8.60 12.61 7.29 9.00
Step 18401/20000   lossTr 4.9453   accTr 26.5   13105s = 1.37s/step
Step 18401/20000   Activation magnitudes (2): 0.97 0.79 0.56 1.14 0.62 1.20 0.67 1.07
Step 18401/20000   Activation magnitudes (3): 10.20 7.13 6.06 11.79 8.75 12.56 7.57 9.27
Step 18501/20000   lossVa 4.9512   accVa 26.2
Step 18601/20000   lossTr 4.9688   accTr 25.8   13408s = 1.52s/step
Step 18601/20000   Activation magnitudes (2): 0.98 0.80 0.56 1.14 0.62 1.20 0.67 1.08
Step 18601/20000   Activation magnitudes (3): 10.26 7.16 5.96 12.98 8.71 11.51 7.48 9.10
Step 18801/20000   lossTr 4.9658   accTr 26.1   13708s = 1.50s/step
Step 18801/20000   Activation magnitudes (2): 0.97 0.79 0.56 1.14 0.62 1.19 0.69 1.11
Step 18801/20000   Activation magnitudes (3): 10.32 6.82 5.91 12.26 8.82 12.00 7.80 10.24
Step 19001/20000   lossVa 4.9332   accVa 26.4
Step 19001/20000   lossTr 5.0020   accTr 25.9   14044s = 1.68s/step
Step 19001/20000   Activation magnitudes (2): 0.97 0.79 0.56 1.14 0.62 1.20 0.68 1.13
Step 19001/20000   Activation magnitudes (3): 10.37 6.84 5.88 12.24 8.75 12.94 7.90 9.39
Step 19201/20000   lossTr 4.9325   accTr 26.5   14405s = 1.80s/step
Step 19201/20000   Activation magnitudes (2): 0.97 0.79 0.56 1.14 0.62 1.20 0.68 1.14
Step 19201/20000   Activation magnitudes (3): 10.40 7.07 6.03 11.96 8.87 12.52 7.36 9.58
Step 19401/20000   lossTr 4.9481   accTr 26.1   14770s = 1.83s/step
Step 19401/20000   Activation magnitudes (2): 0.98 0.79 0.56 1.14 0.62 1.20 0.68 1.12
Step 19401/20000   Activation magnitudes (3): 10.42 7.08 6.08 12.50 8.54 11.67 7.81 9.01
Step 19501/20000   lossVa 4.9193   accVa 26.5
Step 19601/20000   lossTr 4.9525   accTr 26.2   15150s = 1.90s/step
Step 19601/20000   Activation magnitudes (2): 0.99 0.80 0.56 1.14 0.62 1.23 0.68 1.13
Step 19601/20000   Activation magnitudes (3): 10.45 6.78 6.06 11.99 8.71 12.16 7.54 9.29
Step 19801/20000   lossTr 4.9161   accTr 26.4   15593s = 2.22s/step
Step 19801/20000   Activation magnitudes (2): 0.98 0.79 0.56 1.15 0.62 1.24 0.68 1.16
Step 19801/20000   Activation magnitudes (3): 10.46 6.74 5.95 12.38 8.62 12.71 7.45 9.40
Step 20000/20000   lossVa 4.9111   accVa 26.6
Step 20000/20000   Peak CUDA memory usage: 6.275 GB
Step 20000/20000   lossTr 4.9192   accTr 26.3   15977s = 1.93s/step
Step 20000/20000   Activation magnitudes (2): 0.98 0.79 0.56 1.14 0.62 1.20 0.69 1.16
Step 20000/20000   Activation magnitudes (3): 10.46 6.80 6.06 12.19 8.83 12.62 8.00 9.41
Saving AF:       ./results-fineweb10B\ptAf-optimizedMlpAtt-layerShared-phase1-step20000.pt
Saving AF (CSV): ./results-fineweb10B\ptAf-optimizedMlpAtt-layerShared-phase1-step20000.csv
Saving plot (loss/acc): ./results-fineweb10B\tr-optimizedMlpAtt-layerShared-phase1-step020000-00042616-00045100.png
Saving plot (AF): ./results-fineweb10B\af-optimizedMlpAtt-layerShared-phase1-step020000.png
Saving plot (act): ./results-fineweb10B\act-optimizedMlpAtt-layerShared-phase1-step020000.png
